[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rapid Online Assessment of Reading (ROAR)",
    "section": "",
    "text": "A bridge between the lab and the classroom\nAssessments are typically time-consuming and resource-intensive to administer: Individually administering assessments to each student in a classroom means a substantial amount of lost instruction time and requires extensive training for teachers to accurately administer and score measures that are used for high-stakes decisions (e.g., access to intervention). Researchers face these same challenges creating a bottle-neck to research at scale. While education technology companies have built products that lower the demands on teachers, many of these products are expensive, grounded in opaque, proprietary technology, and lack a strong research backing. Hence, these products rarely get used in research, creating a disconnect between educational research and practice.\nWe launched ROAR envisioning a new model: an open-source, open-access assessment platform, grounded in ongoing academic research, and co-developed in collaboration with school-district stakeholders. Rather than a one-way street from the lab to society (often with a commercial intermediary), ROAR’s goal is to inculcate a virtuous cycle between research and practice. We aim to build a suite of completely automated, lightly gamified, online assessments that are grounded in ongoing cognitive neuroscience research and validated against the current “gold standard” of standardized, individually-administered assessments. Our approach is to partner with school districts and community based organizations at each stage of research and development to ensure that our research is grounded in real-world problems and inspired by the deep knowledge of educators who work with children and youth across a diversity of contexts. Through this “Research Practice Partnership” model, we endeavor towards a new assessment methodology that is more valid, precise, efficient, and informative. We aim to design this platform around the diversity of learners in the United States (and abroad). We prioritize transparency at every stage: whenever feasible, materials and technology are made public and each measure within ROAR is published in open-access, peer-reviewed journals with the goal of building more systemic connections between the lab, classroom, and society.\n\n\nAcknowledgements\nROAR reflects the collective vision and dedication of an incredible team of collaborators. This work would not have been possible without the hundreds of teachers, reading specialists, school administrators, parents, and community based organizations that believed in the ROAR mission (1  ROAR Vision and Mission) and collaborated at each stage of ROAR research and development. Additionally, dozens of academics have made important contributions along the way including Ben Domingue, Rebecca Silverman, Joshua Lawrence, Clementine Chou, Amy Burkhardt, Jasmine Tran, Maya Brunton, Aryaman Taore, Kenny Tang, Alby Ungashe, Klint Kanopka, and others.\nROAR would not have been possible without generous funding from the Eunice Kennedy Shriver National Institute of Child Health and Human Development (R01HD095861), Advanced Educational Research and Development Fund (AERDF), Stanford-Sequoia K-12 Research Collaborative, Microsoft, Stanford Impact Labs, Neuroscience:Translate, Klingenstein Foundation, Tools Competition. Rapid Visual Processing measures (9.2.2 Rapid Visual Processing) were developed in collaboration with the UCSF Dyslexia Center with funding from the State of California.\n© Jason D. Yeatman, Stanford University",
    "crumbs": [
      "A bridge between the lab and the classroom"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  ROAR Vision and Mission",
    "section": "",
    "text": "1.1 Open-source ideology in educational assessment\nThe last decade has seen a revolution in scientific transparency. The open-science movement began as a grass roots movement to make science more transparent, accessible and reproducible through open the sharing of code and data to accompany publications in open-access journals. The success of the open-science movement can be appreciated in new public mandates for data sharing by many of the major scientific funders in the United States and Europe, as well as proliferation of organizations like the Center for Open Science, and preprint servers like bioRxiv, that all make it easier to document, share and reproduce scientific research. In fields like cognitive neuroscience, it is now standard practice for software and algorithms to be open-source, and many journals even require various open-science practices. However, in education, most widely used assessments are grounded in proprietary products, with many of the technical details guarded by pay-walls or made purposefully opaque to maintain a competitive edge in the market. There are, of course, counter examples like DIBELS that have always maintained open-access printed materials, and with projects like the Item Response Warehouse which provides open-access to many educational datasets, their is a clear desire among many educational researchers for a move toward open-science.\nWe launched ROAR with the mission to bring the open-source ideology to educational assessment. Our lab has a long track record of developing and supporting open-source software for analysis and sharing of brain imaging data, and for modeling the interplay between brain development and learning. ROAR represents the next phase of this open-science mission: to build tools that fill the needs of educators to assess reading development while, simultaneously, opening the door to research at an unprecedented scale. Not every aspect of ROAR is completely open, but we consciously prioritize open-science at every stage of development including this technical manual which is written as an open-source quarto book.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ROAR Vision and Mission</span>"
    ]
  },
  {
    "objectID": "intro.html#approach-to-validation",
    "href": "intro.html#approach-to-validation",
    "title": "1  ROAR Vision and Mission",
    "section": "1.2 Approach to validation",
    "text": "1.2 Approach to validation\nEach ROAR measure is rigorously validated both in an academic research setting (i.e., “in the lab”) as well as in a typical school setting (i.e., “in the classroom”). We take both these approaches to validation to ensure that ROAR meets the highest standards of rigor across applications in research and practice. Lab validation studies involve recruiting research participants through the typical recruitment avenues of the Brain Development & Education Lab and involve validating new ROAR measures against “gold standard” individually administered diagnostic assessments that are widely accepted by reading and dyslexia researchers. School validation studies are conducted through a Research Practice Partnership model in collaboration with school districts to ensure that ROAR is valid for the desired use cases in the school. Since the question for a school is often “how does ROAR relate to our standard of practice”, we report both a) validation of ROAR measures against the current assessments that are used in standard practice in our collaborating schools and b) validation of ROAR measures against validation measures administered by the ROAR research team to students in the district. Together these two approaches to validation have allowed us to extensively examine the accuracy and precision of ROAR relative to a) the constructs it was designed to measure and b) other related measures that are widely used across the United States.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ROAR Vision and Mission</span>"
    ]
  },
  {
    "objectID": "intro.html#the-roar-assessment-suite",
    "href": "intro.html#the-roar-assessment-suite",
    "title": "1  ROAR Vision and Mission",
    "section": "1.3 The ROAR Assessment Suite",
    "text": "1.3 The ROAR Assessment Suite\nROAR consists of a collection of measures, each designed to tap into a critical aspect of reading. Each individual measure can be run independently and returns raw scores, standard scores, and percentiles relative to national norms. Additionally, measures are also grouped into measurement suites that comprehensively evaluate different constructs in reading development, and produce composite scores and risk indices.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ROAR Vision and Mission</span>"
    ]
  },
  {
    "objectID": "intro-foundational-reading-skills.html",
    "href": "intro-foundational-reading-skills.html",
    "title": "2  ROAR Assessment Suites",
    "section": "",
    "text": "2.1 Foundational Reading Skills\nMastering the code of written language such that text can be fluently decoded into sound and meaning is the foundation of literacy. Foundational reading skills are also the bottleneck for children with dyslexia who struggle to learn letter-sound correspondences and decoding skills early in elementary school and typically have continued struggles with reading fluency. The ROAR Foundational Reading Skills Suite assesses the collection of skills that are at the foundation of reading development and also represent the major challenges for students with dyslexia.\nReading skills develop sequentially with each skill building upon the foundation that has already been established. In an alphabetic script like English where letters represent sounds that are blended together to form words, a critical foundation is Phonological Awareness. Phonological Awareness refers to the ability to identify and manipulate the sounds that make up spoken words. Phonological Awareness typically develops hand in hand with Letter Sound Knowledge. With the development of Phonological Awareness and Letter Sound Knowledge, children begin learning to crack the code of written language, decode text to sound, and read words. Decoding skills and Single Word Reading measure the complexity of words that a student can read going from simple consonant-vowel-consonant words like “cat”, to complex, multi-syllabic words like “heterogeneity” (Yeatman et al. 2021). But reading connected text is much more than decoding a sequence of words in isolation; reading sentences efficiently and fluently is critical for comprehension of more complicated texts. ROAR Foundational Reading Skills is a composite of ROAR-Phoneme (Gijbels et al. 2024), ROAR-Letter, ROAR-Word (Yeatman et al. 2021), and ROAR-Sentence (Tran et al. 2023).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ROAR Assessment Suites</span>"
    ]
  },
  {
    "objectID": "intro-foundational-reading-skills.html#dyslexia-screening-and-subtyping",
    "href": "intro-foundational-reading-skills.html#dyslexia-screening-and-subtyping",
    "title": "2  ROAR Assessment Suites",
    "section": "2.2 Dyslexia Screening and Subtyping",
    "text": "2.2 Dyslexia Screening and Subtyping\nDevelopmental dyslexia is an impairment in foundational reading skills. Whereas most children who are provided systematic and structured reading instruction are able to master phonological awareness, letter-sound correspondences, and decoding early in elementary school, children with dyslexia struggle to develop these skills and require substantially more support. For people with dyslexia reading efficiency can remain a challenge throughout life. Thus, the ROAR Foundational Reading Skills Suite (Phoneme, Letter, Word, and Sentence) serves as a reliable and accurate index of the reading challenges associated with dyslexia. Additionally, dyslexia is associated with other challenges including Rapid Automatized Naming (RAN) and various aspects of visual processing. Thus, in addition to the Foundational Reading Skills Suite, ROAR contains additional measures that have been designed to characterize other challenges that are common in people with dyslexia. These additional measures are useful for both predicting children’s struggles with reading development as well as characterizing differences that might contribute to or exacerbate reading challenges.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ROAR Assessment Suites</span>"
    ]
  },
  {
    "objectID": "intro-norms.html",
    "href": "intro-norms.html",
    "title": "3  ROAR Scores and Norms",
    "section": "",
    "text": "3.1 ROAR Scores\nROAR assessment report different types of scores that each have intended use cases. The ROAR Families and Teachers Guide provides detailed descriptions of how to interpret scores and use them to guide instruction and/or intervention.\nOne important consideration for interpreting scores on any assessment is participant effort, concentration, and engagement. A score is only an accurate representative of the participant’s ability level if the participant is engaged and tries their hardest even as the assessment gets difficult. For assessments that are individually administered (e.g., by a teacher), the administrator might get a qualitative impression of the participant’s effort and focus. If the same person is administering the assessment and interpreting the scores (e.g., classroom teacher or reading specialist), this qualitative impression can be helpful. However it can also be a source of bias and it is hard to standardize the criteria for judging engagement. For automated, online assessments like ROAR, participant disengagement might be a particular concern and needs to be considered when interpreting scores. Each ROAR measure has a defined criteria, grounded in research, for identifying disengaged participants and flagging unreliable scores (for example see Section 15.2 and Section 16.2). This criteria is defined in an algorithm that takes into account a) the response time distribution and b) pattern of responses on the assessment. Scores for any participant that are flagged for disengagement or other issues that might affect the interpretation of the score are flagged in the ROAR Score Report.\nTypes of Scores",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ROAR Scores and Norms</span>"
    ]
  },
  {
    "objectID": "intro-norms.html#sec-intro-scores",
    "href": "intro-norms.html#sec-intro-scores",
    "title": "3  ROAR Scores and Norms",
    "section": "",
    "text": "Figure 3.1: Understanding ROAR scores using the normal curve (Gaussian distribution).\n\n\n\n\n\nRaw Scores: A raw score is the basic measure of a student’s performance on the test. Each assessment reports a raw score and the scoring rules for that raw score are detailed in the introduction to that assessment (for example, see Section 5.2 for the scoring rules for ROAR-Word). Most of the assessment use item response theory (IRT; see Section 4.2) and computer adaptive testing (CAT; see Chapter 4) for scoring though some timed assessments like ROAR-Sentence (see Section 12.3) use other types of scoring models. The raw score is comparable across grade levels and over time. Raw Scores are useful for tracking growth in reading skills over time.\nPercentile Scores: The percentile refers to a student’s rank within their grade level on the given skill. The percentile is the number of students out of 100 who have lower scores. Percentile scores are computed by comparing raw scores to a norming table. A norming table captures the distribution of scores for each age bin in a lookup table providing the percentiles associated with each raw score. Percentile Scores are useful for identifying students who are struggling relative to their peers (or relative to national norms). The norming table for percentile scores are computed in 2 ways:\n\nBased on ROAR Norms. Section 3.2 shows shows the participating ROAR schools, Section 3.3 presents school characteristics and Section 3.4 show student characteristics.\nBased on linking ROAR scores to criterion measures like the Woodcock Johnson Basic Reading Skills (WJ BRS) Standard Scores. This linking allows ROAR-Word scores to be interpreted with direct reference to the criterion measure that is often used to define dyslexia risk (for example see Section 22.1).\n\nStandard Scores: A standard score is a way of showing how performance compares to other kids of the same age or grade. The standard score is comparable within a grade level, but not across grade levels or over time. Age standardized scores for ROAR-Word put scores for each age bin on a standard scale (normal distribution, \\(\\mu=100\\), \\(\\sigma=15\\), see Figure 3.1) and are computed in 2 ways:\n\nBased on ROAR Norms\nBased on linking ROAR scores to WJ BRS Standard Scores.\n\nSupport Categories: For each measure, ROAR recommends students who are in need of extra support. Support categories can also be interpreted as indicating risk of reading difficulties such as dyslexia (for more information see Chapter 27 and (intro-dys-screening?)). Dyslexia refers to the lower end of a continuum of reading skills and there is no agreed upon cutoff. The 25th percentile based on national norms is a common cutpoint that is used to indicate students who are in need of additional support and that is the cut point that is implemented in ROAR Support Categories. Students below the 25th percentile are recommended for additional support. Students between the 25th and 50th percentile are indicated that the skill is still developing. Students above the 50th percentile are indicated as being at grade level (achieved skill). For example, Figure 3.2 shows the distribution of support categories from a hypothetical school district in the ROAR Score Report.\n\n\n\n\n\n\n\nFigure 3.2: Figure from ROAR Score Reports showing the distribution of students for a hypothetical school district that Need Extra Support (red; below 25th percentile), Developing Skill (yellow; 25th - 50th percentile), Achieved Skill at grade level (green; above 50th percentile)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ROAR Scores and Norms</span>"
    ]
  },
  {
    "objectID": "intro-norms.html#sec-school-map",
    "href": "intro-norms.html#sec-school-map",
    "title": "3  ROAR Scores and Norms",
    "section": "3.2 Map of ROAR schools",
    "text": "3.2 Map of ROAR schools\n\n\n\n\n\n\n\n\nFigure 3.3: State Map of ROAR collaborators",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ROAR Scores and Norms</span>"
    ]
  },
  {
    "objectID": "intro-norms.html#sec-schools",
    "href": "intro-norms.html#sec-schools",
    "title": "3  ROAR Scores and Norms",
    "section": "3.3 Table of school characteristics",
    "text": "3.3 Table of school characteristics\n\n\n\n\n\n\nN\n\n\n\n\nMedian Students\n411\n\n\nMedian Free or Reduced Lunch\n269\n\n\nRace/Ethnicity\n\n\nMedian Hispanic Ethnicity\n155\n\n\nMedian White\n31\n\n\nMedian Black or African American\n12\n\n\nMedian Asian\n14\n\n\nMedian American Indian or Alaska Native\n0\n\n\nMedian Hawaiian or Other Pacific Islander\n0\n\n\nMedian Multiracial\n2402\n\n\nOrganization Type\n\n\nPublic School\n58\n\n\nCharter School\n40\n\n\nPrivate School\n11\n\n\nSummer School/Tutor Program/Other\n10",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ROAR Scores and Norms</span>"
    ]
  },
  {
    "objectID": "intro-norms.html#sec-participants",
    "href": "intro-norms.html#sec-participants",
    "title": "3  ROAR Scores and Norms",
    "section": "3.4 Table of student demographics",
    "text": "3.4 Table of student demographics\n\n\n\n\n\n\nN\n%\n% Missing\n\n\n\n\nFemale\n4617\n42.91\n11.92\n\n\nFree or Reduced Lunch\n1143\n10.62\n48.87\n\n\nRace/Ethnicity\n\n\nHispanic Ethnicity\n3194\n29.69\n0.00\n\n\nWhite\n3713\n34.51\n0.00\n\n\nBlack or African American\n693\n6.44\n0.00\n\n\nAsian\n1593\n14.81\n0.00\n\n\nAmerican Indian or Alaska Native\n117\n1.09\n0.00\n\n\nHawaiian or Other Pacific Islander\n51\n0.47\n0.00\n\n\nMultiracial\n976\n9.07\n0.00\n\n\nEnglish Language Learner\n1792\n16.66\n40.86\n\n\nSpecial Education\n381\n3.54\n43.94\n\n\nTotal\n10759",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ROAR Scores and Norms</span>"
    ]
  },
  {
    "objectID": "intro-irt-cat.html",
    "href": "intro-irt-cat.html",
    "title": "4  Computer Adaptive Testing (CAT)",
    "section": "",
    "text": "4.1 CAT parameters and item selection\nUnless otherwise specified, ROAR assessments use a Rasch model and items are selected based on Fisher information (Linden 2000; Ma et al. 2023).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Computer Adaptive Testing (CAT)</span>"
    ]
  },
  {
    "objectID": "intro-irt-cat.html#sec-intro-irt",
    "href": "intro-irt-cat.html#sec-intro-irt",
    "title": "4  Computer Adaptive Testing (CAT)",
    "section": "4.2 IRT models and item validation",
    "text": "4.2 IRT models and item validation\nUnless otherwise specified, ROAR uses a Rasch model (one parameter logistic), to put items on a vertical scale. Infit and outfit statistics are used to ensure that each item fits the measurement scale well. Any items with with infit/outfit statistics outside the range of 0.7 - 1.3 are removed (Wu and Adams 2013). Ensuring that each item fits the measurement scale validates that the item taps into the same latent construct as the other items in the assessment. Finally, to ensure that the measurement scale does not have bias against any demographic group we take two approaches:\n\nWe run validation studies to ensure that the reliability and criterion validity are equivalent across race, ethnicity, and socio-economic status, school district, and level of English proficiency. We take particular care to validate ROAR for English language learners (ELLs).\nWe run studies of parameter invariance (see (Ma et al. 2023)) to ensure that the difficulty of each item is consistent across samples spanning different school districsts with different demographics. Parameter invariance ensures that the assessments function equivalently across diverse groups of participants.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Computer Adaptive Testing (CAT)</span>"
    ]
  },
  {
    "objectID": "intro-swr.html",
    "href": "intro-swr.html",
    "title": "5  Single Word Reading (ROAR-Word)",
    "section": "",
    "text": "5.1 Structure of the task\nFigure 5.1 shows the structure of the ROAR-Word task. ROAR-Word is a computer adative test (CAT) with 84 items that are sampled from a large item bank (&gt;600 items). ROAR-Word begins with instructions that are narrated by characters that introduce the task as a fun adventure. Instructions are followed by practice trials with feedback to ensure that the participant understands the goal of the lexical decision task and how to respond to real and nonsense words. Finally, after practice trials, the participant is presented with 84 CAT items divided across three blocks with breaks in between each block. Throughout the task, participants are rewarded by gold coins and new charachters along their journey which helps with focus and engagement.\nFor each task trial a real or pseudo word is presented for 350ms and the participant is cued to indicate with a keypress, touchscreen or swipe whether the word was real or made up. The participant can take as long as needed to respond.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single Word Reading (ROAR-Word)</span>"
    ]
  },
  {
    "objectID": "intro-swr.html#sec-swr-structure",
    "href": "intro-swr.html#sec-swr-structure",
    "title": "5  Single Word Reading (ROAR-Word)",
    "section": "",
    "text": "Figure 5.1: ROAR-Word task.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single Word Reading (ROAR-Word)</span>"
    ]
  },
  {
    "objectID": "intro-swr.html#sec-swr-scoring",
    "href": "intro-swr.html#sec-swr-scoring",
    "title": "5  Single Word Reading (ROAR-Word)",
    "section": "5.2 Scoring",
    "text": "5.2 Scoring\nROAR-Word is a two alternative forced choice (2AFC) task and items are scored as correct or incorrect (dichotomous scoring) by comparing the participant’s response (keypress/touchscreen/swipe indicating real word vs. pseudo word), to the true classification (real/pseudo) of the item. Each response is scored in real time. The participant’s ability or raw score (\\(\\theta\\)) on ROAR-Word is computed based on an item response theory (IRT) model. IRT puts ability (\\(\\theta\\)) on an interval scale meaning that scores can be compared over time and across grades.\nTypes of Scores\n\nRaw Scores: ROAR-Word raw scores are computed based on an IRT model which puts ability (\\(\\theta\\)) on an interval scale. Raw scores are then put on a scale spanning 100-900 by applying a linear transform to \\(\\theta\\) estimates. \\(\\text{RAOR Score} = \\text{round}\\left( \\left( \\frac{\\theta + 6}{3} \\times 200 \\right) + 100 \\right)\\)\nPercentile Scores: Percentile scores are computed in 2 ways (see Chapter 3):\n\nBased on ROAR Norms\nBased on linking ROAR scores to Woodcock Johnson Basic Reading Skills (WJ BRS) Standard Scores. This linking allows ROAR-Word scores to be interpreted with direct reference to the criterion measure that is often used to define dyslexia risk.\n\nStandard Scores: Age standardized scores for ROAR-Word put scores for each age bin on a standard scale (normal distribution, \\(\\mu=100\\), \\(\\sigma=15\\)) and are computed in 2 ways:\n\nBased on ROAR Norms\nBased on linking ROAR scores to WJ BRS Standard Scores.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single Word Reading (ROAR-Word)</span>"
    ]
  },
  {
    "objectID": "intro-swr.html#design-and-validation-of-items",
    "href": "intro-swr.html#design-and-validation-of-items",
    "title": "5  Single Word Reading (ROAR-Word)",
    "section": "5.3 Design and validation of items",
    "text": "5.3 Design and validation of items\nThe ROAR-Word measurement scale has already been validated in previous publications (Yeatman et al. 2021; Ma et al. 2023; Barrington et al. 2023). Each individual item is validated based on its fit to the IRT measurement scale. ROAR-Word uses a Rasch (one parameter logistic) model (with a guess rate of 0.5) and item fit is assessed based on infit and outfit statistics using standard criteria (Wu and Adams 2013). Items are included in the assessment if infit and outfit are betwen 0.7 and 1.3.\nAdditionally, item parameters are validated to ensure that items function similarly across diverse demographic groups. Ma et al. (2023) report an analysis of parameter invariance across 4 groups of school districts that differ dramatically in terms of:\n\nRace/Ethnicity\nPercentage of English language learners\nMedian income\nPercentage of students who qualify for free and reduced price lunch\n\nMa et al. (2023) demonstrated that after removing a small number of items, item difficulty was consistent across all the school districts included in the sample.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Single Word Reading (ROAR-Word)</span>"
    ]
  },
  {
    "objectID": "intro-sre.html",
    "href": "intro-sre.html",
    "title": "6  Sentence Reading Efficiency (ROAR-Sentence)",
    "section": "",
    "text": "6.1 Defining the construct of Sentence Reading Efficiency\nTimed reading measures go under a variety of names (e.g., reading fluency, reading efficiency, etc) and involve different levels of demands on comprehension and articulation, making it hard to interpret the extent to which scores reflect differences in reading efficiency versus separate constructs. We designed ROAR-Sentence to isolate reading efficiency by minimizing comprehension demands while maintaining checks for understanding, and we used a silent reading task to a) avoid the confounds of articulation that are inherent to oral reading tasks and b), measure the most ecologically valid form of reading (silent reading). This stands in contrast to other reading fluency measures that confound articulation, comprehension and efficiency leading to a less interpretable score.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sentence Reading Efficiency (ROAR-Sentence)</span>"
    ]
  },
  {
    "objectID": "intro-sre.html#other-measures-of-oral-and-silent-reading-efficiency-and-fluency",
    "href": "intro-sre.html#other-measures-of-oral-and-silent-reading-efficiency-and-fluency",
    "title": "6  Sentence Reading Efficiency (ROAR-Sentence)",
    "section": "6.2 Other measures of oral and silent reading efficiency and fluency",
    "text": "6.2 Other measures of oral and silent reading efficiency and fluency\nTraditional measures that are most similar to ROAR-SRE are sometimes referred to as sentence reading fluency tasks, and while they are not administered online, they do elicit silent responses from students. For example, the Woodcock Johnson (WJ) Tests of Achievement “Sentence Reading Fluency” subtest (Schrank et al. 2014), and Test Of Silent Reading Efficiency and Comprehension (TOSREC) (Wagner, R. K., Torgesen, J. K., Rashotte, C. A., & Pearson, N. A. 2010), rely on an established design: A student reads a set of sentences and endorses whether each sentence is true or false. For example, the sentence, Fire is hot, would be endorsed as True. A student endorses as many sentences as they can within a fixed time limit (usually three minutes). The final score is the total number of correctly endorsed sentences minus the total number of incorrectly endorsed sentences. Both the WJ and TOSREC are standardized to be administered in a one-on-one setting (though TOSREC can also be group administered) and the stimuli consist of printed lists of sentences which students read silently and mark True/False with a pencil. Even though the criteria for item development on these assessments is not specified in detail, there is a growing literature showing the utility of this general approach. First of all, this quick, 3 minute assessment is straightforward to administer and score and has exceptional reliability, generally between 0.85 and 0.90 for alternate form reliability (Wagner, R. K., Torgesen, J. K., Rashotte, C. A., & Pearson, N. A. 2010; Johnson, Pool, and Carter 2011; Wagner 2011). Moreover, this measure has been shown to be useful for predicting performance on state reading assessments: For example, Johnson and colleagues demonstrated that TOSREC scores could accurately predict students who did not achieve grade-level performance benchmarks on end-of-the-year state testing of reading proficiency (Johnson, Pool, and Carter 2011).\nFurther evidence for validity comes from the strong correspondence between silent sentence reading measures such as the TOSREC and Oral Reading Fluency (ORF) measures (Denton et al. 2011; Wagner 2011; Johnson, Pool, and Carter 2011; Kang and Shin 2019; Y.-S. Kim, Wagner, and Lopez 2012; Y.-S. G. Kim, Park, and Wagner 2014; Price et al. 2016). ORF is one of the most widely used measures of reading development in research and practice, and some have even argued for ORF as an indicator of overall reading competence (Fuchs et al. 2001). ORF is widely used to chart reading progress in the classroom, providing scores with units of words per minute that can be examined longitudinally (e.g., for progress monitoring (Cummings, Park, and Bauer Schaper 2013; Good, Gruba, and Kaminski 2002; Hoffman, Jenkins, and Dunlap 2009)), compared across classrooms and districts, and can inform policy decisions such as how to confront learning loss from the Covid-19 pandemic Domingue et al. (2022). Even though silent reading and ORF are highly correlated, the measures also have unique variance (Hudson et al. 2008; kim2012developmental?; Wagner2011?) and, theoretically, have different strengths and weaknesses. For example, even though there are strong empirical connections between ORF and reading comprehension (Y.-S. G. Kim, Park, and Wagner 2014), ORF does not require any understanding of the text and has been labeled by some as “barking at print” (Samuels 2007). Silent reading, on the other hand, is the most common form of reading, particularly as children advance in reading instruction. In line with this theoretical perspective, Kim and colleagues found that silent sentence reading fluency was a better predictor of reading comprehension than ORF starting in second grade (Y.-S. Kim, Wagner, and Lopez 2012). Thus, given the practical benefits of silent reading measures (easy to administer and score at scale), along with the strong empirical evidence of reliability, concurrent, and predictive validity, and face validity of the measure, an online measure of silent sentence reading efficiency would be useful for both research and practice.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sentence Reading Efficiency (ROAR-Sentence)</span>"
    ]
  },
  {
    "objectID": "intro-sre.html#structure-of-the-task-and-design-of-the-items",
    "href": "intro-sre.html#structure-of-the-task-and-design-of-the-items",
    "title": "6  Sentence Reading Efficiency (ROAR-Sentence)",
    "section": "6.3 Structure of the task and design of the items",
    "text": "6.3 Structure of the task and design of the items\nROAR-Sentence uses a similar task design as the Test of Silent Reading Efficiency and Comprehension (TOSREC) and WJ Sentence Reading Fluency sub-test with the major differences being: a) ROAR-Sentence is a gamified online task rather than pencil and paper and b) the ROAR-Sentence items are designed specifically to tap into silent sentence reading efficiency. Figure 6.1 shows the ROAR-Sentence task.\n\n\n\n\n\n\nFigure 6.1: ROAR-Sentence task. In the ROAR-Sentence task, participants first choose the character they want to read with and then they are instructed to read the sentences as quickly as possible and indicate with a button-press whether each sentence is true or false. Their score is computed as the number of correct responses minus the number of incorrect responses in either 180 second or 90 seconds depending on the version of the task\n\n\n\nThe strength of silent reading fluency/efficiency tasks is also their weakness: On the one hand, these tasks include comprehension, which bolsters the argument for the face validity of silent reading measures. On the other hand, what is meant by comprehension in these sentence reading tasks is often ill-defined and, thus, a low score lacks clarity on whether the student is struggling due to difficulties with “comprehension” or “efficiency”. As a concrete example, sentences in the TOSREC incorporate low frequency vocabulary words (e.g., porpoise, bagpipes, locomotive, greyhounds, buzzards) meaning that vocabulary knowledge as well as specific content knowledge (e.g., knowledge about porpoises, bagpipes and locomotives) will affect scores. While this design decision might be a strength in some scenarios (e.g., generalizability to more complex reading measures such as state testing), it presents a challenge for interpretability. An interpretable construct is critical if scores are used to individualize instruction. For example, does a fourth grade student with a low TOSREC score need targeted instruction and practice focused on a) building greater automaticity and efficiency in reading or b) vocabulary, syntax and background knowledge. Our goal in designing a new silent sentence reading efficiency measure was to more directly target reading efficiency by designing simple sentences that are unambiguously true or false and have minimal requirements in terms of vocabulary, syntax and background knowledge. Ideally, this measure could be used to track reading rate in units of words per minute, akin to a silent reading version of the ORF task, but with a check to ensure reading for understanding.\nTo consider the ideal characteristics of these sentences, it may be helpful to begin by considering the ORF task which is used to compute an oral reading rate (words per minute) for connected text. In an ORF task, the test administrator can simply count the number of words read correctly to assess each student’s reading rate. Translating this task to a silent task that can be administered at scale online poses an issue because an administrator is unable to monitor the number of sentences read by the student. A student could be instructed to press a button on the keyboard after the completion of a sentence in order to proceed to the next one. However, the validity of this method depends on the student’s ability to exhibit restraint and wait until the completion of each sentence before proceeding to the next sentence.\nIn the interest of preserving the validity of the interpretations of the scores, we retain the True/False endorsement of the TOSREC and WJ, but reframe its use. That is, for the ROAR-Sentence task, the endorsement of True/False should be interpreted as an indication that the student has read the sentence, rather than as an evaluation of comprehension per se. In this context, if the student has difficulty comprehending a sentence, or if the student takes a long time to consider the correct answer because the sentence is confusing, syntactically complex, or depends on background knowledge and high-level reasoning, we lose confidence in the inferences that we can make about a student’s reading efficiency. As such, it is important that sentences designed for this task are simple assertions that are unambiguously true or false. However, creating sentences to adhere to these basic standards may not always be straightforward. For example, the statement “the sky is blue” may be true for a student in the high-plain desert in Colorado but may be a controversial statement for a student in Seattle. Thus, careful consideration must be given to crafting sentences that do not depend on specific background knowledge and are aligned with the goal of measuring reading efficiency. (Tran et al. 2023) provides a detailed description of the iterative research and design process that went into defining and validating this construct and the reader is referred to that publication for more details on the item bank.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sentence Reading Efficiency (ROAR-Sentence)</span>"
    ]
  },
  {
    "objectID": "intro-sre.html#sec-sre-scoring",
    "href": "intro-sre.html#sec-sre-scoring",
    "title": "6  Sentence Reading Efficiency (ROAR-Sentence)",
    "section": "6.4 Scoring",
    "text": "6.4 Scoring\nROAR-Sentence is a two alternative forced choice (2AFC) task and is scored as the total number of correct responses minus the total number of incorrect responses in the alloted (3 minute) time window. This scoring method controls for guessing by controlling for the number of incorrect responses in the calculation of the scores.\nTypes of Scores\n\nRaw Scores: The student’s raw score will range between 0-130.\nPercentile Scores: Percentile scores are computed in 2 ways (see Chapter 3):\n\nBased on ROAR Norms\nBased on linking ROAR scores to Test of Silent Reading Efficiency and Comprehension (TOSREC) and Woodcock Johnson Sentence Reading Fluency Standard Scores. This linking allows ROAR-Sentence scores to be interpreted with direct reference to the criterion measure that is often used to define dyslexia risk.\n\nStandard Scores: Age standardized scores for ROAR-Sentence put scores for each age bin on a standard scale (normal distribution, \\(\\mu=100\\), \\(\\sigma=15\\)) and are computed in 2 ways:\n\nBased on ROAR Norms\nBased on linking ROAR scores to TOSREC and WJ Standard Scores.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sentence Reading Efficiency (ROAR-Sentence)</span>"
    ]
  },
  {
    "objectID": "intro-phoneme.html",
    "href": "intro-phoneme.html",
    "title": "7  Phonological Awareness (ROAR-Phoneme)",
    "section": "",
    "text": "7.1 Structure of the task\nROAR-Phoneme has 3 sub-tests that measure different dimensions of phonological awareness:\nAnd two optional sub-test that were deemed unnecessary for obtaining a reliable and valid measure of PA but still have utility in specific use cases (for more information see (Gijbels et al. 2024)):\nROAR-Phoneme employs a one-interval, three-alternative forced choice task. Instructions are narrated by a character, and the participant selects the appropriate response with a touchscreen or mouse click. ROAR-Phoneme consists of 3 subtests (19 items per subtest), with each subtest consisting of 2 or 3 blocks (divided by difficulty level). Each subtest starts off with 3 training items with feedback. Training items have to be completed correctly before the task will continue to the test items. To engage children from pre-k through 5th grade, the task is embedded in a story where the child has to help a monkey and his friends (rabbit, bear, and otter) collect their favorite foods. At the end of every trial, images of food are displayed. At the end of every block, a visualization of the collected food is presented, and the character provides encouragement (e.g., “Great job! So many bananas! Let’s get a few more!”). Every character provides the instructions of their own subtest, guides throughout the task, motivates the participants to take short breaks between blocks, and introduces their next friend.\nParticipants are instructed to work on a computer or tablet sitting at a desk. Sound has to be turned on and set to a comfortable level, based on the instructions of one of the characters. All game instructions are provided in both text and audio. For all trials of all subtests, one image, accompanied by an audio fragment, recorded by a native English-speaker, provides the specific instruction of that trial (e.g., subtest FSM: “Which picture starts with the same sound as dog?”). This screen shown in Figure 7.1 is followed by the instruction image (top) plus three answer options (left, middle, right). All images are verbalized (e.g., “dam” (target), “goat” (foil 1), “mop” (foil 2)) and the position of the images is randomized for each trial. For the DEL subtest, the images are not verbalized as this could give away the correct answer. In contrast, participants are allowed to listen to the instruction phrase two times for this subtest. We did not implement the ability to listen to the instructions more than once throughout the entire task to stay as consistent as possible with standardized PA tasks like the CTOPP-2 in which instructions are not repeated. Participants pick a response by clicking the image, which is followed by a visualization of the response with a random number of food images as motivation (Figure 7.1).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Phonological Awareness (ROAR-Phoneme)</span>"
    ]
  },
  {
    "objectID": "intro-phoneme.html#structure-of-the-task",
    "href": "intro-phoneme.html#structure-of-the-task",
    "title": "7  Phonological Awareness (ROAR-Phoneme)",
    "section": "",
    "text": "First-Sound Matching (FSM)\nLast-Sound Matching (LSM)\nDeletion (DEL)\n\n\n\nBlending (BLE)\nRhyming (RHY)\n\n\n\n\n\n\n\n\n\nFigure 7.1: ROAR-Phoneme task. Every subtest starts with visual+auditory instructions, followed by some practice items with feedback. After the practice items, 19 items are presented in a one-interval three-alternative forced choice (1I-3AFC) task with random feedback (independent of answer, as motivation). The items are presented in semi-random order (within every block) and there are 2–3 sections per subtest.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Phonological Awareness (ROAR-Phoneme)</span>"
    ]
  },
  {
    "objectID": "intro-letter.html",
    "href": "intro-letter.html",
    "title": "8  Letter Sound Knowledge (ROAR-Letter)",
    "section": "",
    "text": "8.1 Structure of the task\nROAR-Letter is a four alternative fourced choice (4AFC) task divided into 3 blocks: - Upper-case letter names (26 items) - Lower-case letter names (26 items) - Letter-sound correspondences (36 items) Like all ROAR measures, ROAR-Letter is lightly gamified. The task begins with instructions and practice trials with feedback until the student understands the game. Then, in each block, the student is presented with the name or sound of a letter and asked to select the correct letter from the four choices (see Figure 8.1).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Letter Sound Knowledge (ROAR-Letter)</span>"
    ]
  },
  {
    "objectID": "intro-letter.html#structure-of-the-task",
    "href": "intro-letter.html#structure-of-the-task",
    "title": "8  Letter Sound Knowledge (ROAR-Letter)",
    "section": "",
    "text": "Figure 8.1: ROAR-Letter task. In the ROAR-Letter task, participants are first directed to choose the spoken letter name and then the letter sound in separate blocks. The task is divided into several blocks with encouragement throughout. Students have the opportunity to replay any letter name or sound audio they miss. There is no time limit for responses.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Letter Sound Knowledge (ROAR-Letter)</span>"
    ]
  },
  {
    "objectID": "intro-letter.html#design-and-implementation-of-computer-adaptive-letter-sound-assessment",
    "href": "intro-letter.html#design-and-implementation-of-computer-adaptive-letter-sound-assessment",
    "title": "8  Letter Sound Knowledge (ROAR-Letter)",
    "section": "8.2 Design and implementation of computer-adaptive letter-sound assessment",
    "text": "8.2 Design and implementation of computer-adaptive letter-sound assessment\nTo determine the optimal ROAR-Letter CAT we first collected data in 4,041 students in kindergarten and first grade. Table 8.1 shows the demographics of the participants and Table 8.2 shows the characteristics of the schools that participated. 2,840 of these students were administered all 88 items (26 lower case letter names; 26 upper case letter names; 36 letter sound correspondences). Based on these data, we ran a CAT simulation to a) determine how these students would have responded under different item-selection criteria and b) choose the number of items and optimal CAT parameters to achieve reliable estimates of letter-sound knowledge in the fewest number of trials. Figure 18.1 shows the upper and lower bounds on reliability as a function of the number of items that a participant completes. Based on these data we were able to develop an extremely efficient and precise computer adaptive test of letter sound knowledge (see Chapter 18).\n\n\n\n\n\n\n\n\n\nN\n%\n% Missing\n\n\n\n\nFemale\n673\n23.43\n51.18\n\n\nFree or Reduced Lunch\n547\n19.05\n56.86\n\n\nRace/Ethnicity\n\n\nHispanic Ethnicity\n700\n24.37\n48.33\n\n\nWhite\n362\n12.60\n48.33\n\n\nBlack or African American\n78\n2.72\n48.33\n\n\nAsian\n96\n3.34\n48.33\n\n\nAmerican Indian or Alaska Native\n3\n0.10\n48.33\n\n\nHawaiian or Other Pacific Islander\n12\n0.42\n48.33\n\n\nMultiracial\n26\n0.91\n48.33\n\n\nTotal\n2872\n\n\n\n\n\n\n\n\n\n\nTable 8.1: Demographics of ROAR-Letter calibration sample.\n\n\n\n\n\n\n\n\n\n\n\n\n\nN\n\n\n\n\nMedian Students\n441\n\n\nMedian Free or Reduced Lunch\n247\n\n\nRace/Ethnicity\n\n\nMedian Hispanic Ethnicity\n69\n\n\nMedian White\n129\n\n\nMedian Black or African American\n3\n\n\nMedian Asian\n25\n\n\nMedian American Indian or Alaska Native\n0\n\n\nMedian Hawaiian or Other Pacific Islander\n1\n\n\nMedian Multiracial\n794\n\n\nOrganization Type\n\n\nPublic School\n21\n\n\nCharter School\n2\n\n\nPrivate School\n0\n\n\nSummer School/Tutor Program/Other\n2\n\n\n\n\n\n\n\n\nTable 8.2: Characteristics of participating schools in the calibration sample.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Letter Sound Knowledge (ROAR-Letter)</span>"
    ]
  },
  {
    "objectID": "intro-letter.html#sec-letter-scoring",
    "href": "intro-letter.html#sec-letter-scoring",
    "title": "8  Letter Sound Knowledge (ROAR-Letter)",
    "section": "8.3 Scoring",
    "text": "8.3 Scoring\nROAR-Letter is a four alternative forced choice (4AFC) task and items are scored as correct or incorrect (dichotomous scoring) by comparing the participant’s response (mouse click/touchscreen indicating the chosen letter), to the correct answer. Each response is scored in real time. The participant’s ability or raw score (\\(\\theta\\)) on ROAR-Letter is computed based on an item response theory (IRT) model. IRT puts ability (\\(\\theta\\)) on an interval scale meaning that scores can be compared over time and across grades. A CAT algorithm is used to optimize the precision and efficiency of ROAR-Letter (see Chapter 18)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Letter Sound Knowledge (ROAR-Letter)</span>"
    ]
  },
  {
    "objectID": "intro-dyslexia-screening-subtyping.html",
    "href": "intro-dyslexia-screening-subtyping.html",
    "title": "9  ROAR Dyslexia Screening and Subtyping",
    "section": "",
    "text": "9.1 Dyslexia screening based on foundational reading skills\nPeople with dyslexia struggle learning how to read. The most direct way to screen (or diagnose) dyslexia is based on measures of foundational reading skills. Children with dyslexia are delayed relative to their peers in the development of: (1) Phonological Awareness (see Chapter 7), (2) Letter Sound Knowledge (see Chapter 8), (3) real word and pseudo word reading (see Chapter 5), and (4) reading speed, efficiency or fluency (see Chapter 6). Difficulties with all these skills can persist through adulthood without proper identification and intervention. The goal of screening based on foundational reading skills is to a) identify challenges early in in elementary school and b) intervene while the developing brain is optimally plastic and interventions are most effective (Gaab and Petscher 2022; Blachman 2013; Torgesen 2004, 1998; Lovett et al. 2017). For example, Lovett et al. (2017) has shown that intervening early is more efficient (larger effects per hour of intervention) compared to waiting until later in elementary school. To quote Torgesen (1998), the goal of early screening is to “catch them before they fall”.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ROAR Dyslexia Screening and Subtyping</span>"
    ]
  },
  {
    "objectID": "intro-dyslexia-screening-subtyping.html#sec-intro-subtype",
    "href": "intro-dyslexia-screening-subtyping.html#sec-intro-subtype",
    "title": "9  ROAR Dyslexia Screening and Subtyping",
    "section": "9.2 Dyslexia prediction and subtyping",
    "text": "9.2 Dyslexia prediction and subtyping\nWolf and Bowers (1999) and colleagues first introduced the “double deficit hypothesis” as a response to the “core phonological deficit hypothesis” and demonstrated that a) some children with typical PA skills still struggle learning to read and b) many of these struggling readers have early challenges with Rapid Automatized Naming (RAN) (Denckla and Cutting 1999; Denckla and Rudel 1976; Wolf and Bowers 1999; Compton, DeFries, and Olson 2001; Wolf et al. 2002). Additionally, children who struggle with PA and RAN tend to have even larger challenges with reading than those who only struggle on one skill. RAN is now required by most dyslexia screening legislation. However, RAN is not a useful intervention target per se: whereas a child who struggles with PA will benefit from training targeting PA (Bradley L and Bryant P E 1983), a child struggling with RAN does not simply need to practice RAN. Each of the measures in the ROAR Foundational Reading Skills battery is a core component of reading development and a useful intervention target. Measures in the Dyslexia Prediction and Subtyping battery are predictive of reading development and often help to understand the mechanisms underlying a student’s struggles, but are not proven intervention targets. For example, RAN (see Section 9.2.1) is highly predictive of future reading development, and also indicates a different type of struggle than PA (i.e., automatization or connectivity between visual and verbal processing), but is not a skill that needs direct instruction. Rapid Visual Processing (see Section 9.2.2)is another measure that has mounting evidence of a causal relationship to reading development and has utility as a screener but is not a skill that should be directly taught (Ramamurthy, White, and Yeatman 2023a; Ramamurthy et al. 2024; Lobier and Valdois 2015; Marie Line Bosse, Tainturier, and Valdois 2007).\n\n9.2.1 Rapid Automatized Naming (ROAR-RAN)\n\n9.2.1.1 Structure of the task, administration and scoring\nROAR-RAN is the only ROAR measure that requires verbal responses. Thus, ROAR-RAN has unique considerations for administration and scoring. Whereas all other ROAR measures are specifically designed and validated to produce accurate and reliable results in a large group setting (e.g., a classroom) where dozens (or hundreds) of students are silently completing ROAR assessments at the same time, ROAR-RAN requires students to be assessed in a quiet and private space. RAN is specifically designed to measure naming speed. Since speed is the fundamental unit of the measure, and since naming must be done out loud, it is important that participants are in a space where they can speak rapidly and without distraction.\nSimilar to other ROAR measures, RAN is scored automatically and instructions are narrated by characters in a gamelike setting. RAN does not require a test administrator to administer or score the assessment though young students might need some monitoring. Students should be in a private space where they can speak out loud without distractions and, akin to other ROAR measures, they log in to the dashboard and click to launch the ROAR-RAN assessment. Upon launch, a character will narrate the instructions. The participant will first be cued to name each individual item - letters, numbers or colors. This both serves as a check to ensure that the participant knows the name of each item and also to calibrate the automated scoring algorithm. After the quick calibration phase, the participant is the instructed to name all the items in sequence as quickly as possible. A countdown indicates the beginning of the measure, the stimuli are presented, and the participant responses are recorded through the webcam microphone. The webcam is also used to track the participants gaze so that the speech data can be co-registered to the item the participant is fixating, allowing for more precise scoring of individual items.\nScoring is performed automatically by the ROAR-RAN automated scoring algorithm. The algorithm processes the speech data, records the timestamp and duration for each spoken item, measuring both the speed and accuracy of the participant’s responses. If a participant incorrectly identifies more than three items, the test is invalidated. For valid tests, the system calculates the total time taken to complete the task by recording the timestamp of the participant’s response to the first symbol and the timestamp of their response to the last symbol. This total duration, measured in seconds, is then reported as the participant’s score on the ROAR-RAN assessment (see Section 19.1 for validation of the scoring algorithm).\n\n\n\n\n\n\nA Webcam is Required for ROAR-RAN\n\n\n\nSince RAN is fundamentally about naming, it is the only ROAR assessment that requires a webcam. Responses are recorded, securely stored, scored with an algorithm, and scores are displayed in the ROAR score report.\n\n\n\n\n9.2.1.2 RAN-Letters\nRAN is intended to measure naming that is “automatized”. To select the ideal upper case letters for RAN-Letters we assessed knowledge of upper case letter names in 4,022 kindergarten and first grade students and calculated the proportion of students that knew the name of each upper case letter name. For the RAN-Letter stimuli, we chose the 6 monosyllabic letter names with a) the highest accuracy and b) phonetically distinct: X, A, O, Z, F, M\n\n\n\n\n\n\n\n\nTable 9.1: Letter naming accuracy for upper case letters in kindergarten and 1st grade.\n\n\n\n\n\n9.2.1.3 RAN-Colors\nFor young children who have not yet established sufficient letter knowledge for RAN-Letters, RAN-Colors is more appropriate. In RAN-Colors, rather than upper case letters, the stimuli are an array of color patches. The task is the same: the participant sequentially names all the colors as quickly and accurately as possible. RAN-Colors uses the following colors which have distinct and unambiguous names: Red, Blue, Pink, Black, Yellow. RAN-Colors does not use Green due to the high occurance of Red/Green color blindness.\n\n\n9.2.1.4 RAN-Numbers\nAnother option for young children who have not yet established knowledge of letter names is RAN-Numbers. The following numbers are used as stimuli in RAN-Numbers: 2, 3, 4, 5, 7, 8.\n\n\n\n9.2.2 Rapid Visual Processing\n\n9.2.2.1 Theoretical background\nThere is a long-standing controversy about the visual factors associated with dyslexia. While phonological difficulties have been widely recognized as a core feature of dyslexia, debate has persisted over visual processing theories, suggesting a more complex, multifaceted etiology (R. Pennington and Pennington 2011; Vellutino et al. 2004; O’Brien and Yeatman 2021). The field has yet to reach consensus because existing theory on visual processing differences in dyslexia is often built upon small samples, using experimental measures without established psychometric properties, that are deployed across different age ranges often without prior work demonstrating the validity of the measure in each developmental window. Validated measures of visual processing that predict future reading development could contribute in overcoming the challenges faced by other screening measures since visual development is language-agnostic and not directly taught in preschool.\nRapid Visual Processing (RVP) refers to the ability to rapidly encode and recall multiple visual elements simultaneously in a brief glimpse (Sperling 1960, 1983). In this task, a string of letters or symbols is briefly flashed at the center of the screen and then the participant is cued to report the identity of a single randomly chosen element. Of all the measures of sensory processing that have been studied in relation to word reading difficulties, the rapid visual processing task has the strongest evidence for identifying a subgroup of struggling readers who are not captured by conventional measures of phonological awareness. Previous studies have demonstrated that this task:\n\nConsistently correlates with reading ability (Marie-Line Bosse and Valdois 2009; Ramamurthy, White, and Yeatman 2023b).\nDiffers in children with dyslexia (Lobier, Zoubrinetzky, and Valdois 2012; Ramamurthy, White, and Yeatman 2023b).\nCannot be explained as a consequence of dyslexia (Lobier and Valdois 2015).\nMight be a useful intervention target (Valdois et al. 2014; Zhao et al. 2019; Zoubrinetzky et al. 2019).\nIdentifies a subset of poor readers that have high phonological awareness despite their reading difficulties (Saksida et al. 2016; Valdois et al. 2020).\nRecent evidence suggests that the difficulties with Rapid Visual Processing in children with dyslexia are consistent across languages including French, English, Dutch and Chinese (Huang, Liu, and Zhao 2021; Lobier and Valdois 2015) making this an ecologically relevant measure related to reading across different languages.\n\nBy including both letter and symbol stimuli, the task allows for the assessment of rapid visual processing skills both within and independent of language experience. The measure’s language-agnostic nature makes it particularly valuable for early screening, as visual processing issues precede reading difficulties. This approach, of including visual measures that are linked to reading, aligns with emerging multifactorial models of dyslexia, contributing to a more comprehensive understanding of the various cognitive factors influencing reading development (O’Brien and Yeatman 2021; Catts et al. 2024; B. F. Pennington 2006; Compton 2021; Zuk et al. 2021; Bergen, Leij, and Jong 2014; Wolf and Bowers 1999).\n\n\n9.2.2.2 Structure of the task\nThe Rapid Visual Processing Task was developed in close collaboration with the UCSF Multitudes project. Task design, data colletion and analysis was shared across the two projects.\nROAR-RVP has 2 versions that measure the same construct of rapid visual processing: Rapid Visual Processing with Letters (RVPL) and Rapid Visual Processing with Symbols (RVPS). RVPL measures the ability to rapidly locate and identify letters in 2-, 4-, and 6-letter strings. RVPS assesses the ability to rapidly locate and identify non-namable visual symbols, making it language-agnostic. These tasks are considered promising tools for early identifying of struggling readers not captured by conventional phonological awareness measures.\n\nLetters (RVPL)\nSymbols (RVPS)\n\nROAR-RVP employs a six-alternative forced choice task presented as an engaging underwater adventure game. There are two versions, each with two difficulty blocks (2- and 4- element strings. 6-element string can be added for older students). Participants help a lost dolphin (RVPL) or whale (RVPS) find friends and treasures. The task sequence involves fixating on a central point where 2-4 elements (letters or pseudo-letters) briefly appear (240ms). A post-cue then indicates the target position, and participants select the correct element from six choices. Each version begins with 6 longer-duration practice items. Narrated instructions, encouragement animations, and feedback sounds guide participants throughout. The game is designed for K-2 children but can be extended to older students and adults.\nFigure 9.1 shows the rapid visual processing task with letters (RVPL) a string of letters is briefly flashed at the center of the screen (240ms) and then the participant is cued to report the identity of a single randomly chosen letter. The participant’s task is to report the identity of the target element that was at the prompted single location, by tapping on one letter from a set of 6 letter choices provided. This task was optimized to provide highly reliable metrics of rapid visual processing in children as young as five years of age (see Section 19.2)\n\n\n\n\n\n\nFigure 9.1: Schematic of the ROAR Rapid Visual Processing (ROAR-RVP) Task\n\n\n\n\n\n9.2.2.3 Are visual measures biased to social factors like one’s socio-economic status and primary language?\nWhile different cognitive and language processes have been increasingly reported to be influenced by socioeconomic status and childhood experience Schwab and Lew-Williams (2016), in theory, visual processing measures should not be influenced by language, socioeconomic status or early childhood experience. This is because most sensory processes have a developmental trajectory that precedes Stein (2022) formal reading instructions, making them valuable screening measures for identifying potential future reading challenges.\nWe first examined whether the visual measures we developed exhibit bias related to students’ primary language or socio-economic status as indexed by eligibility for free and reduced-price meals (see Figure 9.2). This investigation addresses a fundamental challenge in early screening: many measures show bias across various social factors. Consequently, even when measures demonstrate good predictive power for reading challenges, it often remains unclear whether this predictive ability stems from the measure assessing a construct fundamental to reading skill or simply from capturing the same differences in social factors that are known to influence reading outcomes. Our findings show that ROAR-RVP were not affected by English language proficiency or socioeconomic status, which shows promise for addressing a major challenge in the field.\n\n\n\n\n\n\nFigure 9.2: ROAR Rapid Visual Processing is not affected by spoken language or socioeconomic status",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ROAR Dyslexia Screening and Subtyping</span>"
    ]
  },
  {
    "objectID": "intro-multilingualism.html",
    "href": "intro-multilingualism.html",
    "title": "10  Multilingualism",
    "section": "",
    "text": "10.1 Prevalence\nBased on responses to the mandatory Home Language Survey, about 39.5 % (2,310,311) of students enrolled in the Californian public school system speak a language other than English at home (California Department of Education, 2022). About 19.1 % (1.3 million) of Californian students are classified as English Learners (ELs), meaning that they did not meet the criteria, based on the English Language Proficiency Assessment of California (ELPAC), to be (re)classified as English-proficient. While these students speak more than 100 different languages, 81.9 % are Spanish-speakers, highlighting the need for universal screening instruments in Spanish, such as ROAR-Español.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multilingualism</span>"
    ]
  },
  {
    "objectID": "intro-multilingualism.html#choosing-the-languages-of-assessment",
    "href": "intro-multilingualism.html#choosing-the-languages-of-assessment",
    "title": "10  Multilingualism",
    "section": "10.2 Choosing the Language(s) of Assessment",
    "text": "10.2 Choosing the Language(s) of Assessment\nDetermining the appropriate language(s) of assessment for a multilingual student is challenging. Factors to consider are the languages reported to be spoken at home, languages of previous and current instruction, as well as the outcome of interest. Currently, we offer ROAR-English and ROAR-Español as two standalone suites of tests (Italian, Portuguese, German and French are in development). However, we are continuously exploring ways of combining multilingual students’ scores in multiple languages so that we will be able to provide a unified reading risk estimation for multilingual students.\nIn the meantime, it is best practice to assess students in all their languages. For Spanish-speaking ELs that means administering both ROAR-English and ROAR-Español and qualitatively interpreting a student’s results in both languages in conjunction with other available information on, for example, their home language environment and prior languages of instruction. Multilingual students meeting or exceeding standards derived from monolingual populations may generally be considered as meeting those standards. However, in cases where a multilingual student performs below proficiency thresholds, this may be due to a number of reasons (the ongoing acquisition of the language of assessment, different expected developmental trajectories, etc.) and one should not conclude that this result indicates a screening flag. For example, consider a 2nd grade student who grew up in a home that primarily spoke Spanish, began learning to speak English when they entered Kindergarten, and was primarily taught to read in English. Their scores on ROAR-English and ROAR-Spanish would both be useful for gauging their reading development and making planning instruction, but we would not expect their ROAR-Español scores to be at the same level as a monolingual Spanish speaker who is being taught to read in Spanish. ROAR scores provide detailed information about reading skills in each language of assessment but must be interpreted in context alongside other sources of information.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multilingualism</span>"
    ]
  },
  {
    "objectID": "intro-multilingualism.html#sec-roar-esp-scores",
    "href": "intro-multilingualism.html#sec-roar-esp-scores",
    "title": "10  Multilingualism",
    "section": "10.3 ROAR-Español Scores",
    "text": "10.3 ROAR-Español Scores\nROAR-Español has been validated in a sample of multilingual learners in California and a sample of (primarily) monolingual Spanish speakers in Colombia (see Bhat et al. (2024) for an initial publication based on ROAR-Español). Care has been taken in the design and validation of each ROAR-Español measure to design the items around the linguistic diversity of Spanish speakers. ROAR-Español returns the same types of scores as English ROAR measures (see Section 3.1 and ROAR Families and Teachers Guide for detailed information) and, additionally, ROAR-Español returns grade level equivalent scores based on the normative data from monolingual Spanish speakers in Colombia. Every country will, of course, have different normative trajectories of reading development. Literacy levels among school aged children in Colombia is similar to Mexico, but below average compared to other countries in Latin America. Figure 10.1 shows literacy achievement data across different countries in Latin America (reproduced from US AID A SUMMARY ANALYSIS OF EDUCATION TRENDS IN LATIN AMERICA AND THE CARIBBEAN 2022 UPDATE). The large, urban, school districts we partnered with in Colombia perform above that national average and could be considered reasonably representative of a typical learning trajectory for a (primarily) monolingual Spanish speaker in an urban area of Latin America. Moreover, the representative data we collected in California schools places Spanish speaking students from California within the typical range of the Colombian norms. Thus, ROAR-Español Grade Level Equivalent (GLE) Scores can provide useful information to teachers in the United States about a student’s Spanish reading proficiency relative to monolingual learners in Latin America. These data can be useful for interpreting scores on an English screener. For example, a new second grade student who has just moved from a monolingual Spanish environment to the US who has low scores on ROAR measures in English but with GLE Scores on ROAR-Español that are in the typical range for a 2nd grader still needs additional reading instruction, but is not at high risk for dyslexia.\n\n\n\n\n\n\nFigure 10.1: Literacy achievement levels in Latin America",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multilingualism</span>"
    ]
  },
  {
    "objectID": "intro-multilingualism.html#vignettes-interpretation-and-use-of-multilingual-students-scores",
    "href": "intro-multilingualism.html#vignettes-interpretation-and-use-of-multilingual-students-scores",
    "title": "10  Multilingualism",
    "section": "10.4 Vignettes: Interpretation and Use of Multilingual Students’ Scores",
    "text": "10.4 Vignettes: Interpretation and Use of Multilingual Students’ Scores\n\n10.4.1 Vignette 1\nA student who grew up and attended (pre-)school in Mexico arrives in the US with his family in the summer before starting second grade. The family reports speaking Spanish at home and that their child had attended two years of prior instruction in Spanish. In this case, knowing how this student fares in relation to his monolingual Spanish peers is of value, as this is a fair comparison–the student has, hitherto, lived in a linguistic environment and had a scholastic experience that is represented by the largely monolingual Colombian norming sample (Colombia and Mexico also have comparable literacy achievement levels. See Figure 10.1). This student will not have had many opportunities to learn English. In this case, even if the student is deemed to have met the minimal requirements to be assessed in English, their performance on any English assessment is not particularly meaningful, as it will be a function of their lacking exposure to the English language, rather than of any underlying reading difficulty.\nThus, if this students’ English score suggest that they are in need of additional support, this result should not be interpreted in isolation. Rather, one ought to obtain the student’s ROAR-Español scores. If the student’s ROAR-Español grade level equivalent score classifies them as performing at grade-level/on-track, then one can conclude that the student is not at risk of developing a reading difficulty/dyslexia, but instead is in need of exposure to and structured support with learning English. They will likely also need support in translating their Spanish reading ability to English since English is a highly irregular writing system (i.e., an opaque orthography). But needing instruction is qualitatively different than being at risk for long-term reading difficulties like dyslexia. If the student’s ROAR-Español grade level equivalent score results in a ‘needing support’/not-on-track classification, further assessment is recommended.\n\n\n10.4.2 Vignette 2\nAn incoming first-grade student grows up in a predominantly Spanish-speaking family in California. While the parents were born in Mexico and had moved to the US in their adult life, the student was born in the US, lived in a linguistically diverse neighborhood where both English and Spanish are spoken, and attended pre-school and kindergarten classes where English was the language of instruction. In this case, though the student only hears and speaks Spanish at home, their instructional experience and formal language instruction was exclusively English.\nHere, we cannot reasonably expect the student to perform similarly to monolingual Spanish speakers on the ROAR-Español because their linguistic and educational experiences were spread out across multiple languages. Therefore, their grade level equivalent score is much less meaningful and no “risk” classification should be made on the basis of their Spanish performance, alone. At the same time, the students cannot be fairly compared to monolingual English speakers on English assessments. Ultimately, testing in both languages is strongly recommended and, due to multilingual individuals heterogeneity and distinct developmental trajectories, the student’s performance is best judged in conjunction with other qualitative information available. This vignette serves to highlight the challenges in assessment of multilingual learners. Even though the goal of screeners is often to produce a simple and interpretable “risk metric”, interpreting assessment data in multilingual learners requires nuance and expertise and should always take into account the learning context.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multilingualism</span>"
    ]
  },
  {
    "objectID": "intro-palabra.html",
    "href": "intro-palabra.html",
    "title": "11  Spanish Single Word Reading (ROAR-Palabra)",
    "section": "",
    "text": "11.1 Task Development\nROAR-Palabra is explicitly not a translation of the ROAR-Word— simple translations usually fail to produce equivalent versions of a test (Solano-Flores, Backhoff, and Contreras-Niño 2009). In contrast to many other non-English measures, we started the development process from a Spanish perspective: We created an initial list of stimuli by prompting ChatGPT to produce a list of frequent Spanish words, known to pre- and middle-schoolers across the Americas and occurring in all the varieties of Spanish spoken there. We then then used the Wuggy algorithm (Keuleers and Brysbaert 2010) to create matching, word-like pseudowords—stimuli conforming to Spanish orthographic rules and matching the real word list in terms of word length, letter-transition frequencies, and orthographic neighbourhood size. Candidate items were then inspected by experts in reading development who speak various regional varieties of Spanish. The goal was to create an assessment that would be equitable across the wide variety of Spanish language varieties that are spoken in the United States. Spanish speakers from Chile, Colombia, Mexico, and Spain independently reviewed both the real and pseudowords. Items flagged as problematic due to, for example, low frequency of occurrence or inappropriate slang meanings in any one of the Spanishes were removed.\nThis process resulted in an initial item bank with 378 item pairs (that is 189 real words and 189 matched pseudowords). To keep administration time reasonable, we randomly selected 70 core items (35 real and 35 pseudowords), which form the basis of the current version. Every test-taker responded to these core items, as well as 30 additional items (15 real and 15 pseudowords) randomly selected from the broader item pool. In future versions, these additional items will be calibrated, too, so that the task can be made computer-adaptive.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spanish Single Word Reading (ROAR-Palabra)</span>"
    ]
  },
  {
    "objectID": "intro-palabra.html#task-development",
    "href": "intro-palabra.html#task-development",
    "title": "11  Spanish Single Word Reading (ROAR-Palabra)",
    "section": "",
    "text": "11.1.1 Californian Calibration Sub-sample\nTo account for regional variations in the Spanish language, as well as for the different linguistic profiles of monolingual and multilingual speakers of Spanish (see Chapter 10), we developed the ROAR-Palabra using data from both a native Spanish-speaking sample in Chile, as well as a multilingual sample from a school district in California. This allowed us to ensure that the task is appropriate for use in Spanish-speaking populations with different linguistic and cultural backgrounds. Table 11.1 shows the demographic characteristics of our California sub-sample.\n\n\n\n\nTable 11.1: ROAR-Palabra Calibration Sub-sample from California\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nKindergarten N = 5\nGrade 1 N = 201\nGrade 2 N = 220\n\n\n\n\nFemale\n3 (60%)\n96 (48%)\n81 (38%)\n\n\nUnknown\n0\n3\n4\n\n\nFree/Reduced-price Lunch Eligibility\n3 (60%)\n140 (70%)\n155 (70%)\n\n\nSpecial Educational Needs\n1 (20%)\n21 (11%)\n26 (12%)\n\n\nUnknown\n0\n3\n3\n\n\nEnglish Proficiency Status\n\n\n\n\n\nEL\n2 (40%)\n121 (61%)\n145 (67%)\n\n\nEO\n2 (40%)\n52 (26%)\n49 (23%)\n\n\nIFEP\n1 (20%)\n14 (7.1%)\n12 (5.5%)\n\n\nRFEP\n0 (0%)\n11 (5.6%)\n10 (4.6%)\n\n\nTBD\n0 (0%)\n0 (0%)\n1 (0.5%)\n\n\nUnknown\n0\n3\n3\n\n\nPrimary Language\n\n\n\n\n\nEnglish\n2 (40%)\n74 (41%)\n93 (46%)\n\n\nSpanish\n3 (60%)\n107 (59%)\n111 (54%)\n\n\nUnknown\n0\n20\n16\n\n\nRace\n\n\n\n\n\nAsian\n0 (0%)\n3 (1.5%)\n0 (0%)\n\n\nBlack or African American\n0 (0%)\n3 (1.5%)\n0 (0%)\n\n\nFilipino\n0 (0%)\n1 (0.5%)\n0 (0%)\n\n\nHawaiian or Other Pacific Islander\n0 (0%)\n1 (0.5%)\n0 (0%)\n\n\nHispanic\n3 (60%)\n178 (90%)\n195 (100%)\n\n\nWhite\n2 (40%)\n12 (6.1%)\n0 (0%)\n\n\nUnknown\n0\n3\n25\n\n\n\n\n\n\n\n\n\n\n11.1.2 Response Time\nFigure 11.1 shows the distribution of students’ median response times on the ROARA-Palabra. Following the rationale outlined in Chapter 15), participants with a median response time &lt;450ms were excluded from further analyses. This results in 374 of 6035 students (6.2 %) being excluded from further analyses due to guessing or other unreliable testing behaviours.\n\n\n\n\n\n\n\n\nFigure 11.1: Distribution of median response times (both locations) with indication of cut-off (450ms).\n\n\n\n\n\n\n\n11.1.3 Item Properties\nOverall, items tended to be easier for students in Colombia; this is expected, as these students are native Spanish speakers instructed in their first language, while the Californian sub-sample is more linguistically diverse in terms of the Spanish abilities and their instructional programmes. Also, the Colombian sub-sample drawn on here consists of students from grades 1-11. While this disallows for a meaningful direct comparison between Californian and Colombian students,\n\n\n\n\n\n\n\n\nFigure 11.2: Distribution of item difficulty parameters, computed separately by location (California vs. Colombia) drawing only on students in grades K-2.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.3: Correlation between item difficulty parameters, computed separately by location (California vs. Colombia) drawing only on students in grades K-2.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Spanish Single Word Reading (ROAR-Palabra)</span>"
    ]
  },
  {
    "objectID": "intro-frase.html",
    "href": "intro-frase.html",
    "title": "12  Eficiencia de lectura de frases (ROAR-Frase)",
    "section": "",
    "text": "12.1 Other measures of silent reading efficiency in Spanish\nThe Woodcock-Johnson Manufacturers, who created the Woodcock-Johnson Tests of Achievement Sentence Reading Fluency sub-test (F. A. Schrank et al. 2014) that is used as a metric of comparison to English ROAR-Sentence (see Section 23.1.1), also adapted a version for use in Spanish. The Spanish measure, Woodcock-Muñoz Batería de aprochevamiento fluidez en lecutra de frases (Fredrick A. Schrank et al. 2005), follows the same setup as the Sentence Reading Fluency measure in English. The student is given a booklet and a pencil and are instructed to read sentences silently and endorse as many as they can as true or false within three minutes. A proctor administers each assessment to a student individually, one on one, providing instructions for sample items and then guiding them through practice responses. The final score here is the total number endorsed correctly minus the total number endorsed incorrectly. In this assessment, skips do not count negatively.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Eficiencia de lectura de frases (ROAR-Frase)</span>"
    ]
  },
  {
    "objectID": "intro-frase.html#structure-of-the-task-and-design-of-the-items",
    "href": "intro-frase.html#structure-of-the-task-and-design-of-the-items",
    "title": "12  Eficiencia de lectura de frases (ROAR-Frase)",
    "section": "12.2 Structure of the task and design of the items",
    "text": "12.2 Structure of the task and design of the items\nAs with ROAR-Sentence, ROAR-Frase uses a similar task design as the Woodcock-Muñoz fluidez en lecutra de frases. ROAR-Frase, however, is delivered online rather than through through a paper booklet, features gamification, items are designed specifically to tap into silent sentence reading efficiency, and items are delivered in a way that students are forced to endorse each item they see rather than being able to skip items. Importantly, items are designed and validated to ensure cultural relevance across different Spanish language variations spoken in the US and abroad.\nAs is discussed in the ROAR-Sentence section, the comprehension component that is often built into sentence reading tasks make for interpreting low scores difficult as disaggregating comprehension from efficiency for a struggling reader becomes very difficult. In line with this, there is yet another facet of comprehension to take into account for Spanish speakers as their experience with the language itself may vary so drastically from one student to another (Rosario Basterra, Trumbull, and Solano-Flores 2011; Hambleton and Kanjee 1995). By designing items that are unambiguously true or false, removing content knowledge as well specific experience based knowledge and maintaining simple sentence structures, we aim to target reading efficiency more directly.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Eficiencia de lectura de frases (ROAR-Frase)</span>"
    ]
  },
  {
    "objectID": "intro-frase.html#sec-sre-scoring",
    "href": "intro-frase.html#sec-sre-scoring",
    "title": "12  Eficiencia de lectura de frases (ROAR-Frase)",
    "section": "12.3 Scoring",
    "text": "12.3 Scoring\nROAR-Frase, like ROAR-Sentence, is a two alternative forced choice (2AFC) task and is scored as the total number of correct responses minus the total number of incorrect responses in the alloted time window. This method of scoring controls for students who were engaging in guessing behavior by accounting for the number of incorrect responses.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Eficiencia de lectura de frases (ROAR-Frase)</span>"
    ]
  },
  {
    "objectID": "intro-frase.html#roar-frase-norms-for-monolingual-spanish-speakers",
    "href": "intro-frase.html#roar-frase-norms-for-monolingual-spanish-speakers",
    "title": "12  Eficiencia de lectura de frases (ROAR-Frase)",
    "section": "12.4 ROAR-Frase Norms for Monolingual Spanish Speakers",
    "text": "12.4 ROAR-Frase Norms for Monolingual Spanish Speakers\nAs described in Section 10.3, an initial norming study was completed in a primarily monolingual spanish speaking sample in Colombia. Partiipants completed two independent 90 second blocks of ROAR-Frase. ROAR-Frase was delivered to all students above 2nd grade in Colombia. We then ran another group of bilingual students in California in grades 1 and 2. Figure 12.1 displays the norms by grades and Figure 12.2 shows the norms by age. Scores for a represenative sample of multilingual learners in California are shown relative to the monolingual norms.\n\n\n\n\n\n\n\n\nFigure 12.1: ROAR-Frase Colombia score distrubtion by grade. Frase score is the combination of correct minus incorrect across 2 90-second blocks. United States median is included as a red dot on the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.2: ROAR-Frase Colombia score distrubtion by age groups Frase score is the combination of correct minus incorrect across 2 90-second blocks. United States median is included as a red dot on the plot.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Eficiencia de lectura de frases (ROAR-Frase)</span>"
    ]
  },
  {
    "objectID": "intro-fonema.html",
    "href": "intro-fonema.html",
    "title": "13  Conciencia fonológica (ROAR-Fonema)",
    "section": "",
    "text": "13.1 Structure of the task\nROAR-Fonema has 2 sub-tests that measure different dimensions of phonological awareness:\nROAR-Fonema, like ROAR-Phoneme, employs a one-interval, three-alternative forced choice task. As with the entire ROAR suite, instructions are narrated by a character provided with light gamification. The student first interacts with practice items before each subtask where they are given feedback if they respond incorrectly. Practice items must be completed correctly for the student to begin the block. The student hears positive feedback from the characters along the way and is given breaks throughout each block where they can pause if they would like to. Responses are not timed and a student can take as long as they would like to respond to any given item, but cannot have the item repeated as this is consistent with other tests of phonological awareness. As with ROAR-Phoneme, ROAR-Fonema follows the same stimuli presentation - the student hears a speaker give the prompt, sees the instruction image at the top, is read each of the three answer options at the bottom, and then selects their answer once the speaker finishes labeling each of the photos.\nAdaptation of this task to Spanish took into account several important factors. Careful attention was given to ensuring that both items and images were representative of real students’ experiences. Regionally specific words or images were avoided for confusion, for example there are several ways to say the word “car” in Spanish and some are more commonly heard in specific areas while others are more common broadly. We chose to use the words that were most commonly used across diverse sets of Spanish speakers. Additionally, great emphasis was placed on assuring that a wide range of phonemes with varying difficulty levels as well as frequencies were chosen. Similar to ROAR-Letra, attention was also paid to phonemes that would be confusing for bilingual versus monolingual Spanish speakers.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conciencia fonológica (ROAR-Fonema)</span>"
    ]
  },
  {
    "objectID": "intro-fonema.html#structure-of-the-task",
    "href": "intro-fonema.html#structure-of-the-task",
    "title": "13  Conciencia fonológica (ROAR-Fonema)",
    "section": "",
    "text": "First-Sound Matching (FSM)\nLast-Sound Matching (LSM)",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conciencia fonológica (ROAR-Fonema)</span>"
    ]
  },
  {
    "objectID": "intro-letra.html",
    "href": "intro-letra.html",
    "title": "14  Conocimiento de Sonidos de Letras (ROAR-Letra)",
    "section": "",
    "text": "14.1 Adaptation of the Task to Spanish\nROAR-Letra underwent rigous testing through an iterative research and development process to be universally suitable for Spanish speaking students from diverse linguistic backgrounds. Careful attention was paid to ensure the sounds and letters were understandable across different Spanish dialects by accounting for variations in pronunciation. For example, sounds like “ll” which may be pronounced differently in various dialects were chosen very carefully to ensure broad usability.\nAdditionally, important considerations for bilingual students were also kept in mind. For example, “x”, “h”, and “j” were not included as distractors for one another as bilingual students, who are accustomed to both English and Spanish, may have a harder time differentiating these sounds. This decision was influenced by research on bilingual children’s spelling strategies and common mistakes that bilingual learners make when spelling. Helman (2004) highlights how Spanish-speaking students leverage their understanding of the Spanish sound system when learning to spell in English, which can lead to confusion with similar-sounding letters like “h” and “j” or “v” and “b”. Zutell and Allen (1988) discuss the spelling strategies used by Spanish-speaking bilingual children and the challenges they face in distinguishing between English and Spanish phonemes that sound similar across languages. By avoiding such distractors, we aim to create an assessment that is fair to English language learners.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conocimiento de Sonidos de Letras (ROAR-Letra)</span>"
    ]
  },
  {
    "objectID": "intro-letra.html#structure-of-the-task",
    "href": "intro-letra.html#structure-of-the-task",
    "title": "14  Conocimiento de Sonidos de Letras (ROAR-Letra)",
    "section": "14.2 Structure of the task",
    "text": "14.2 Structure of the task\nROAR-Letra, like ROAR-Letter, is a four alternative fourced choice (4AFC) task divided into 3 blocks: - Upper-case letter names (27 items) - Lower-case letter names (27 items) - Letter-sound correspondences (62 items) Like all ROAR measures, ROAR-Letra is lightly gamified. The task begins with instructions and practice trials with feedback until the student understands the game. Then, in each block, the student is presented with the name or sound of a letter and asked to select the correct letter from the four choices. The student can replay letter names or sound if they need to. Figure 14.1 depicts ROAR-Letra.\n\n\n\n\n\n\nFigure 14.1: ROAR-Letra task. In the ROAR-Letra task, participants are first directed to choose the spoken letter name and then the letter sound in separate blocks. The task is divided into several blocks with encouragement throughout. Students have the opportunity to replay any letter name or sound audio they miss. There is no time limit for responses.",
    "crumbs": [
      "Introduction to ROAR-Español",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conocimiento de Sonidos de Letras (ROAR-Letra)</span>"
    ]
  },
  {
    "objectID": "reliability-swr.html",
    "href": "reliability-swr.html",
    "title": "15  Reliability of ROAR-Word",
    "section": "",
    "text": "15.1 Background: Published studies\nThe first published version of ROAR-Word achieved exceptional alternate form reliability (r=0.95) using fixed forms that were equated based on item response theory (Yeatman et al. 2021). To improve efficiency of ROAR-Word, Ma et al. (2023) built the first, open-source, computer adaptive testing (CAT) algorithm in Javascript, and then ran a series of experiments to study how reliability and efficiency of ROAR-Word could be improved with CAT. Figure 15.1 reproduces a figure from Ma et al. (2023) showing an experiment comparing ROAR-CAT to a standard, non-adaptive testing approach. In this experiment, participant were randomly assigned to complete ROAR-Word with the trial order controlled by either a) jsCAT (solid line) versus b) random item sampling (dotted line). ROAR-CAT achieved the same reliability in roughly 40% fewer trials.\nThis innovation has now been incorporated into all the ROAR measures to create quick and efficient, adaptive assessments that span broad age ranges.",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reliability of ROAR-Word</span>"
    ]
  },
  {
    "objectID": "reliability-swr.html#background-published-studies",
    "href": "reliability-swr.html#background-published-studies",
    "title": "15  Reliability of ROAR-Word",
    "section": "",
    "text": "Figure 15.1: ROAR-Word is 40% more efficient when using computer adaptive testing.",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reliability of ROAR-Word</span>"
    ]
  },
  {
    "objectID": "reliability-swr.html#sec-swr-flag",
    "href": "reliability-swr.html#sec-swr-flag",
    "title": "15  Reliability of ROAR-Word",
    "section": "15.2 Criteria for identifying disengaged participants and flagging unreliable scores",
    "text": "15.2 Criteria for identifying disengaged participants and flagging unreliable scores\nROAR-Word is designed to be totally automated: words are read silently, responses are non-verbal, instructions and practice trials are narrated by characters, and scoring is done in real time after each response. This makes it possible to efficiently assess a whole school district simultaneously. However, a concern about automated assessments is that without a teacher to individually administer items, monitor, and score responses, some students might disengage and provide data that is not representative of their true ability. One benefit of a lexical decision task is that there is an extensive literature on the expected response time distribution (Balota, Yap, and Cortese 2006; Keuleers et al. 2012; Balota et al. 2007). Based on the amount of time it takes signals from the eye to reach the brain, for the visual features to be processed, the word to be recognized, and a motor response to be initiated, extremely fast response times are most likely due to rapid guessing behavior indicative of disengagement from the assessment (Ratcliff, McKoon, and Gomez 2004; Balota, Yap, and Cortese 2006). Our previous publications have validated fast response time as an indicator of participant disengagement (Ma et al. 2023; Yeatman et al. 2021). This effect can be seen in Figure 15.2 which shows a plot of median response time (RT) versus proportion correct for each participant. None of the participants with a median response time less than 450ms (horizontal black line in Figure 15.2) are accurate on ROAR-Word. Since ROAR-Word is run as a computer adaptive test (CAT), All participants should be around 75% correct: item difficulty changes adapatively based on participant responses. Participants that respond very quickly and inaccurately are disengaged and not providing data that is representative of their true ability.\n\n\n\n\n\n\nCriteria for flagging unreliable scores\n\n\n\nParticipants with low accuracy (&lt;65% correct) and a median response time &lt;450ms are flagged in ROAR-Score reports and their data is excluded from analyses. Teachers can choose whether to re-administer ROAR or interpret data cautiously in relation to other data sources and contextual factors.\n\n\n\n\n\n\n\n\n\n\nFigure 15.2: Criteria for identifying disengaged participants and flagging unreliable scores on ROAR-Word. Participants displaying extremely rapid responses performed near chance indicative of disengagement and/or rapid guessing behavior. Black lines indicate the cut off for flagging disengaged participants with unreliable scores.",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reliability of ROAR-Word</span>"
    ]
  },
  {
    "objectID": "reliability-swr.html#reliability-of-computer-adativer-roar-word",
    "href": "reliability-swr.html#reliability-of-computer-adativer-roar-word",
    "title": "15  Reliability of ROAR-Word",
    "section": "15.3 Reliability of computer adativer ROAR-Word",
    "text": "15.3 Reliability of computer adativer ROAR-Word\nROAR-Word runs as computer adaptive test based on a Rasch model. The current, default version of ROAR-Word takes about 4 minutes (84 items). More items can be administered for a more precise measure or fewer items can be administered as a quick screener. Table 15.1 reports marginal reliability computed based on data from 10294 students under the IRT model for the standard, 84 item version of ROAR-Word. Reliability (\\(\\rho_{xx^\\prime}\\)) is computed based on the estimated variance of \\(\\hat{\\theta}\\) relative to the estimated standard error (\\(\\widehat{SE}(\\hat{\\theta})^2\\)) using Equation 20.1:\n\\[\n\\hat{\\rho}_{xx^\\prime} = \\frac{\\widehat{VAR}(\\hat{\\theta})}{\\widehat{VAR}(\\hat{\\theta}) + \\widehat{SE}(\\hat{\\theta})^2},\n\\tag{15.1}\\]\n\n\n\n\n\n\n\n\nGrade\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9415742\n10294\n\n\nK\n0.8708076\n131\n\n\n1\n0.9174180\n1050\n\n\n2\n0.9295753\n1123\n\n\n3\n0.9418479\n572\n\n\n4\n0.9361429\n320\n\n\n5\n0.9397313\n315\n\n\n6\n0.9203889\n1000\n\n\n7\n0.9114732\n846\n\n\n8\n0.9195974\n716\n\n\n9\n0.9093690\n1347\n\n\n10\n0.9071286\n1243\n\n\n11\n0.9082764\n932\n\n\n12\n0.9081830\n699\n\n\n\n\n\n\n\n\nTable 15.1: Reliability of ROAR-Word by Grade\n\n\n\n\nTo ensure that ROAR-Word is a fair and equitable assessment across different demographic groups we also report reliability separately by gender (Table 15.2), eligibility for free and reduced price lunch (Table 15.3), English learner status as designated by the school district (Table 15.4), primary language (Table 15.5), special education (Table 15.6), ethnicity (Table 15.7), and race (Table 15.8)\n\n\n\n\n\n\n\n\nGender\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9415742\n3733\n\n\nF\n0.9442770\n1800\n\n\nM\n0.9433320\n1933\n\n\n\n\n\n\n\n\nTable 15.2: Reliability of ROAR-Word by Gender (F=female, M=male)\n\n\n\n\n\n\n\n\n\n\n\n\nFree/Reduced Lunch Status\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9415742\n1949\n\n\nF\n0.9275092\n409\n\n\nP\n0.9434327\n1390\n\n\nR\n0.9271505\n150\n\n\n\n\n\n\n\n\nTable 15.3: Reliability of ROAR-Word by FRL (F=Free, P=Paid, R=Reduced)\n\n\n\n\n\n\n\n\n\n\n\n\nEnglish Learner Status\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9415742\n2368\n\n\nEL\n0.9474195\n897\n\n\nEO\n0.9432996\n1180\n\n\nIFEP\n0.9356226\n213\n\n\nRFEP\n0.9344000\n76\n\n\nTBD\n0.5528642\n2\n\n\n\n\n\n\n\n\nTable 15.4: Reliability of ROAR-Word by EL Status (EL=English Learner, EO=English Only, IFEP=Initial Fluent English Proficient, RFEP=Reclassified Fluent English Proficient)\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary Language\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9415742\n1916\n\n\nEnglish\n0.9447252\n1396\n\n\nOther\n0.9154780\n188\n\n\nSpanish\n0.9175260\n332\n\n\n\n\n\n\n\n\nTable 15.5: Reliability of ROAR-Word by Primary Language\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial Education Status\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9415742\n2046\n\n\n0\n0.9456441\n1874\n\n\n1\n0.9470536\n172\n\n\n\n\n\n\n\n\nTable 15.6: Reliability of ROAR-Word by Special Education Status\n\n\n\n\n\n\n\n\n\n\n\n\nHispanic Ethnicity\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9415742\n4246\n\n\n0\n0.9422047\n2580\n\n\n1\n0.9436946\n1666\n\n\n\n\n\n\n\n\nTable 15.7: Reliability of ROAR-Word by Hispanic Ethnicity\n\n\n\n\n\n\n\n\n\n\n\n\nRace\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9415742\n3187\n\n\nAmerican Indian or Alaska Native\n0.9338252\n3\n\n\nAsian\n0.9400807\n439\n\n\nBlack or African American\n0.9310394\n37\n\n\nFilipino\n0.9404038\n9\n\n\nHawaiian or Other Pacific Islander\n0.9042327\n8\n\n\nHispanic\n0.9436946\n1666\n\n\nMultiracial\n0.9392057\n249\n\n\nWhite\n0.9488497\n776\n\n\n\n\n\n\n\n\nTable 15.8: Reliability of ROAR-Word by Race",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Reliability of ROAR-Word</span>"
    ]
  },
  {
    "objectID": "reliability-sre.html",
    "href": "reliability-sre.html",
    "title": "16  Reliability of ROAR-Sentence",
    "section": "",
    "text": "16.1 Equating ROAR-Sentence test forms",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reliability of ROAR-Sentence</span>"
    ]
  },
  {
    "objectID": "reliability-sre.html#sec-flag-sre",
    "href": "reliability-sre.html#sec-flag-sre",
    "title": "16  Reliability of ROAR-Sentence",
    "section": "16.2 Criteria for identifying disengaged participants and flagging unreliable scores",
    "text": "16.2 Criteria for identifying disengaged participants and flagging unreliable scores\nROAR-Sentence is designed to be totally automated: reading is done silently, responses are non-verbal, instructions and practice trials are narrated by characters, and scoring is done automatically in real time. This makes it possible to efficiently assess a whole district simultaneously. A concern about automated assessments is that without a teacher to individually administer items, monitor, and score responses, some students might disengage and provide data that is not representative of their true ability. For a measure like ROAR-Sentence where items are designed and validated to have an unambiguous and clear answer, disengaged participants can be detected based on fast and innacurate responses. Our approach to identifying and flagging disegnaged participants with unreliable scores was published in (Tran et al. 2023). Figure 16.1 shows a plot of median response time (RT) versus proportion correct for each participant. Most participants were very accurate (&gt;90% correct responses). However there was a bimodal distribution indicating a small group of participants who were performing around chance. These participants also had extremely fast response times.\n\n\n\n\n\n\nCriteria for flagging unreliable scores\n\n\n\nParticipants with a median response time &lt;1,000ms AND low accuracy (&lt;65% correct) are flagged as unreliable scores in ROAR score reports and are excluded from analyses since scores do not accurately represent the participant’s ability. Teachers can choose whether to re-administer ROAR or interpret data cautiously in relation to other data sources and contextual factors.\n\n\n\n\n\n\n\n\n\n\nFigure 16.1: Criteria for identifying disengaged participants and flagging unreliable scores on ROAR-Sentence. Participants displaying extremely rapid responses performed near chance on ROAR-Sentence. This criteria is consistent across multiple studies (Tran et al. 2023). Black lines indicate the cut off for flagging disengaged participants with unreliable scores.",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reliability of ROAR-Sentence</span>"
    ]
  },
  {
    "objectID": "reliability-sre.html#sec-sre-alt-form-rel",
    "href": "reliability-sre.html#sec-sre-alt-form-rel",
    "title": "16  Reliability of ROAR-Sentence",
    "section": "16.3 Alternate form reliability",
    "text": "16.3 Alternate form reliability\nAlternate form reliability is computed as the Pearson correlation between scores on equated test forms that were administered during the same testing session. Figure 16.2 (a) shows a plot of student scores on alternate test forms combining grades and Figure 16.2 (b) shows separate plots for each grade. Table 16.1 reports alternate form reliability for the full sample and separately by grade.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Alternate form reliability across grades\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Alternate form reliability separately by grade\n\n\n\n\n\n\n\nFigure 16.2: Alternate form reliability for ROAR-Sentence\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nAlternate Form Reliability\nN\n\n\n\n\nAll\n0.8116780\n3633\n\n\n1\n0.7810830\n69\n\n\n2\n0.8890698\n163\n\n\n3\n0.8943240\n185\n\n\n4\n0.8320945\n44\n\n\n5\n0.7115194\n45\n\n\n6\n0.8184703\n271\n\n\n7\n0.7958987\n209\n\n\n8\n0.7963777\n272\n\n\n9\n0.7307424\n745\n\n\n10\n0.7894435\n643\n\n\n11\n0.7669829\n542\n\n\n12\n0.7175356\n445\n\n\n\n\n\n\n\n\nTable 16.1: Alternate form reliability for ROAR-Sentence",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reliability of ROAR-Sentence</span>"
    ]
  },
  {
    "objectID": "reliability-pa.html",
    "href": "reliability-pa.html",
    "title": "17  Reliability of ROAR-Phoneme",
    "section": "",
    "text": "17.1 Background: Published studies\nGijbels et al. (2024) reported high correlations between ROAR-Phoneme and standardized measures of PA (CTOPP-2, r=.80) for children from Pre-K through fourth grade and exceptional reliability for the ROAR-Phoneme composite score (\\(\\alpha=0.96\\)). Each individual subtest was also highly reliable:",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reliability of ROAR-Phoneme</span>"
    ]
  },
  {
    "objectID": "reliability-pa.html#background-published-studies",
    "href": "reliability-pa.html#background-published-studies",
    "title": "17  Reliability of ROAR-Phoneme",
    "section": "",
    "text": "First Sound Matching (FSM) \\(\\alpha=0.89\\)\nLast Sound Matching (LSM) \\(\\alpha=0.92\\)\nDeletion (DEL) \\(\\alpha=0.85\\)",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reliability of ROAR-Phoneme</span>"
    ]
  },
  {
    "objectID": "reliability-letter.html",
    "href": "reliability-letter.html",
    "title": "18  Reliability of ROAR-Letter",
    "section": "",
    "text": "18.1 Design and implimentation of computer-adaptive letter-sound assessment\nA Rasch model was fit to the ROAR-Letter calibration sample (see Table 8.1 and Table 8.2). All ROAR-Letter items fit the model well (see Chapter 4 for fit criteria). Based on this IRT model, Figure 18.1 shows the upper and lower bounds on reliability as a function of the number of items that a participant completes. We then ran a CAT simulation as in (Ma et al. 2023) to determine the final item selection criteria that would maximize reliability in the fewest number of trials.\nFigure 18.1: Letter-CAT simulation based on item-response data in 4,041 kindergarten and first grade students. Items were sampled in 3 different ways and marginal reliability was calculated as a function of the number of items that each participant completed. The simulation shows that the choice of items has a major impact on the reliability of the measure. For Optimal sampling (green) the N items with difficulty closest to the participant’s $ heta$ estimate were used. For Random sampling (orange) a random sample of N items were taken for each participant. For Worst sampling (purple) the N items with difficulty furthest from the participant’s $ heta$ estimate were used. This simulation highlights the massive efficiency gain that would be possible from an optimized CAT.",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reliability of ROAR-Letter</span>"
    ]
  },
  {
    "objectID": "reliability-letter.html#design-and-implimentation-of-computer-adaptive-letter-sound-assessment",
    "href": "reliability-letter.html#design-and-implimentation-of-computer-adaptive-letter-sound-assessment",
    "title": "18  Reliability of ROAR-Letter",
    "section": "",
    "text": "ROAR Letter CAT Parameters and Reliability\n\n\n\n25 Trials\n\n5 Upper case letter names\n5 Lower case letter names\n15 Letter-sound correspondences\n\nItem selection: Fisher information\nMarginal reliability = 0.85",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reliability of ROAR-Letter</span>"
    ]
  },
  {
    "objectID": "reliability-dyslexia.html",
    "href": "reliability-dyslexia.html",
    "title": "19  Reliability of Dyslexia Prediction and Subtyping",
    "section": "",
    "text": "19.1 Reliability of ROAR Rapid Automatized Naming (ROAR-RAN)\nReliability and evidence of construct validity was assessed based on correlations among scores across RAN-Letters, RAN-Colors and RAN-Numbers. The assessment was conducted in two stages: first, measuring the reliability of the automated scoring system relative to the manual scoring method, and second, analyzing the correlation between the three RAN measures using the automated scores.",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reliability of Dyslexia Prediction and Subtyping</span>"
    ]
  },
  {
    "objectID": "reliability-dyslexia.html#sec-ran-reliability",
    "href": "reliability-dyslexia.html#sec-ran-reliability",
    "title": "19  Reliability of Dyslexia Prediction and Subtyping",
    "section": "",
    "text": "19.1.1 Automated Scoring Reliability\nWe assessed the reliability of our automated scoring system by comparing it to manual scoring in a sample of 100 participants. In the manual process, the duration of each task was measured by manually timing from the start of the first spoken word and the end of the last spoken word. In the automated process, the duration was determined using start and end timestamps generated by our automatic speech recognition model.\nThe correlation between manual and automated scoring was calculated for each RAN task, with each task achieving a Pearson correlation coefficient greater than 0.95 (see Figure 19.1). These strong correlations suggest that the automated scoring system reliably produces scores that are nearly identical to manual scoring.\n\n\n\n\n\n\nFigure 19.1: ROAR-RAN automated scoring algorithm is precise and reliable\n\n\n\n\n\n19.1.2 Correlation Among ROAR-RAN Measures\nNext, we examined the correlation between the three RAN measures—RAN-Letters, RAN-Colors, and RAN-Numbers—using the automated scores. This analysis aimed to confirm the construct validity of the ROAR-RAN tasks by evaluating the relationships among these measures. All three measures are designed to tap into the same latent construct, though color naming is not as automatized as letter naming and, thus, we expect a lower correlation for RAN-Colors.\nWe calculated Pearson correlations between the automated scores for all participants across the following pairs of tasks: RAN-Letters and RAN-Numbers, RAN-Letters and RAN-Colors, and RAN-Colors and RAN-Numbers. Figure 19.2 shows these correlations providing evidence that each measure is reliably tapping in to a similar latent construct.\n\n\n\n\n\n\nFigure 19.2: Evidence for reliability and construct validity of ROAR Rapid Automized Naming",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reliability of Dyslexia Prediction and Subtyping</span>"
    ]
  },
  {
    "objectID": "reliability-dyslexia.html#sec-rvp-reliability",
    "href": "reliability-dyslexia.html#sec-rvp-reliability",
    "title": "19  Reliability of Dyslexia Prediction and Subtyping",
    "section": "19.2 Reliability of ROAR Rapid Visual Processing (ROAR-RVP)",
    "text": "19.2 Reliability of ROAR Rapid Visual Processing (ROAR-RVP)\n\n19.2.1 Background: Published studies\nThe rapid visual processing paradigm that was administered to older children between 7-17 years (Ramamurthy, White, and Yeatman 2023) was translated for a younger population and a computer adaptive testing algorithm was implemented see 4 in order to improve reliability and ensure the task spans a broad age range. We translated the task for a younger population by iteratively changing the task and the design while administering it to small groups of kindergarten and first graders within the Multitudes study sample population. This work was done through collaboration between the ROAR team at Stanford University and the Multitudes Team at UCSF. The final versions are tailored to Kindergarten and first grade children but spans through adulthood. Details of the design and validation process are published in Ramamurthy et al. (2024).\n\n\n19.2.2 Data informed design changes to achieve high reliability in young children\nFor the RVP measure item difficulty depends on a variety of task parameters. There are two important task parameters that have the potential to influence performance: 1) encoding time and 2) string length.\n\n19.2.2.1 Study 1 (N = 56)\nWe first administered the task with similar parameters as were used in our previous study of older children and adults (Ramamurthy, White, and Yeatman 2023) (encoding time of 120ms and string length of 6 letters). We observed that K/1 children’s task performance was significantly lower with an encoding time of 120ms (16.813% +/- 7.654) with very low reliability (Spearman Brown corrected split half reliability: 0.075) compared to the older cross-sectional population reported in our previous work (37.148% +/- 1.191; reliability: 0.8). However, performance increased with an encoding time of 240 (21.125%+/- 8.371) and 480 ms (24.107% +/- 9.851; d’: 0.269). Notably, even at 480ms many participants still performed at chance and reliability was still low (Spearman Brown corrected split half reliability was 0.306).\n\n\n19.2.2.2 Study 2 (N = 86)\nWe tested how performance changes in trials with four elements (2 on either side of fixation) and six elements (3 on either side of fixation) with encoding times of 240 ms and 480 ms. We observed that there was an overall improvement in task performance in Study 2 [Mean accuracy: 30.025 +/- 1.344] compared to the overall task performance from Study 1 [Mean accuracy: 20.685 +/-0.777; Mean d’: 0.159+/-0.032 ].\n\n\n19.2.2.3 Study 3 (N= 175)\nIn the next iteration, we added a 2-element string in addition to 4- and 6- element strings. We further reduced redundancy by removing an encoding time of 480 ms that did not increase accuracy. An encoding period 240 ms ensures that encoding occurs without making a saccadic eye movement (Li, Hanning, and Carrasco 2021). Overall task performance increased significantly compared to Studies 1 and 2. Performance in Study 3 (40.429% +/- 1.0368) is comparable to the overall performance reported in a previous study with cross-sectional data (n=185) (Ramamurthy, White, and Yeatman 2023), where overall task performance for 6 to 17 yr olds in the MEP task with an encoding time of 120ms and a string length of 6 elements was 37.148% +/- 1.191. Overall task reliability was comparable between Study 3 (r = 0.8) and Study 2 (r = 0.802) see Figure Figure 19.3 below.\n\n\n\n\n\n\nFigure 19.3: Studies to optimize reliability of ROAR Rapid Visual Processing\n\n\n\n\n\n19.2.2.4 IRT to model item difficulty levels and to optimize task parameters\nData from Study 3 (N = 175) was used to calibrate an Item Response Theory (IRT) model. Trials with different string lengths were blocked (twelve 2-letter trials, twenty four 4-letter trials and thirty six 6-letter trials) respectively. The goal of IRT is to place item difficulty (blocks of different string lengths) on an interval scale. The Rasch model (1 parameter logistic with a guess rate fixed at 0.167) was fit to the response data for the 3 item types (constraining difficulty for repeated trials with the same string lengths) for all 175 participants using the MIRT package in R (Philip Chalmers 2012). Figure 19.4 shows item difficulty for each block (a) and item response functions for all three blocks of different string lengths (b).\n\n\n\n\n\n\nFigure 19.4: Calibration of item response theory (IRT) model for ROAR Rapid Visual Processing\n\n\n\n\n\n19.2.2.5 Final Optimized version\nAs a first step towards reducing redundancy, we can shorten the task by eliminating the thirty six 6-element trials completely and used 2- and 4- element trials with an encoding time to 240ms. Further, for efficient task administration we built a simple transition rule and a termination rule. At the end of the 2-element block if children get 4 or more trials correct (&gt;=4/24) then they transition to the next item difficulty and complete eight 4-letter trials. If children perform at or below chance then the task terminates after twenty four 2-letter trials.\n\n\n\n19.2.3 Construct validity: performance on RVP-Letters and RVP-Symbols is highly correlated\nWe used the optimized version of the RVP task described above and created two versions 1) letters and 2) symbols (pseudo-letters). These measures were administered to 1457 children in K/1 across the state of California by the UCSF Dyslexia Center as part of an initiative to develop a universal dyslexia screener, Multitudes. As an initial proof-of-concept study we compared ability in the RVPL task and RVPS task and found a high correlation (r = 0.73; dis-attenuated r = 0.9125 given task reliability of 0.80, Figure 19.5). This provides evidence that both RVPL and RVPS reliably tap into the same latent construct.\n\n\n\n\n\n\nFigure 19.5: Evidence for reliability and construct validity of ROAR Rapid Visual Processing",
    "crumbs": [
      "Reliability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reliability of Dyslexia Prediction and Subtyping</span>"
    ]
  },
  {
    "objectID": "reliability-palabra.html",
    "href": "reliability-palabra.html",
    "title": "20  Reliability of ROAR-Palabra",
    "section": "",
    "text": "20.1 Background: Published studies\nBhat et al. (2024) reported a large study (N=1,337) examining the relationship between reading skills, math skills, and phonological awareness in Spanish speaking students in Colombia. In this sample, the reported marginal reliability of ROAR-Palabra was 0.92, reliability of ROAR-Frase was 0.82, and ROAR-Fonema was 0.85.",
    "crumbs": [
      "Reliability of ROAR-Español",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reliability of ROAR-Palabra</span>"
    ]
  },
  {
    "objectID": "reliability-palabra.html#criteria-for-identifying-disengaged-participants-and-flagging-unreliable-scores",
    "href": "reliability-palabra.html#criteria-for-identifying-disengaged-participants-and-flagging-unreliable-scores",
    "title": "20  Reliability of ROAR-Palabra",
    "section": "20.2 Criteria for identifying disengaged participants and flagging unreliable scores",
    "text": "20.2 Criteria for identifying disengaged participants and flagging unreliable scores\nTo account for unreliable scores and disengaged participation (as discussed in Chapter 15 and shown in Chapter 11), participants with a median response time &lt;450ms are flagged in ROAR-Score reports and their data is excluded from analyses.",
    "crumbs": [
      "Reliability of ROAR-Español",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reliability of ROAR-Palabra</span>"
    ]
  },
  {
    "objectID": "reliability-palabra.html#reliability-of-fixed-length-roar-palabra",
    "href": "reliability-palabra.html#reliability-of-fixed-length-roar-palabra",
    "title": "20  Reliability of ROAR-Palabra",
    "section": "20.3 Reliability of fixed-length ROAR-Palabra",
    "text": "20.3 Reliability of fixed-length ROAR-Palabra\nROAR-Palabra runs as fixed-length test and scores are computed based on a Rasch model. The current version of ROAR-Palabra takes about 5 minutes (70 items). In the near future, a computer-adaptive version will become available leveraging the full item bank. Then, more items can be administered for a more precise measure or fewer items can be administered as a quick screener. Table 20.1 reports marginal reliability computed based on data from 5408 students under the IRT model for the standard, 70 item version of ROAR-Palabra. Reliability (\\(\\rho_{xx^\\prime}\\)) is computed based on the estimated variance of \\(\\hat{\\theta}\\) relative to the estimated standard error (\\(\\widehat{SE}(\\hat{\\theta})^2\\)) using Equation 20.1:\n\\[\n\\hat{\\rho}_{xx^\\prime} = \\frac{\\widehat{VAR}(\\hat{\\theta})}{\\widehat{VAR}(\\hat{\\theta}) + \\widehat{SE}(\\hat{\\theta})^2},\n\\tag{20.1}\\]\n\n\n\n\nTable 20.1: Reliability of ROAR-Word by Grade\n\n\n\n\n\n\nGrade\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9352133\n5408\n\n\nK\nNA\n1\n\n\n1\n0.7495653\n412\n\n\n2\n0.8847775\n638\n\n\n3\n0.9147536\n515\n\n\n4\n0.9126525\n510\n\n\n5\n0.9133743\n580\n\n\n6\n0.8850746\n589\n\n\n7\n0.8439554\n509\n\n\n8\n0.8345813\n423\n\n\n9\n0.8145449\n412\n\n\n10\n0.7855567\n421\n\n\n11\n0.7853985\n398\n\n\n\n\n\n\n\n\n\n\nTo ensure that ROAR-Palabra is fair and equitable for different demographic groups, we also report reliability by gender (Table 20.2), eligibility for free and reduced price lunch (Table 20.3), English learner status based on state of California designations (Table 20.4), primary langauge spoken (Table 20.5), special education (Table 20.6), ethnicity (Table 20.7), and race (@Table 20.8)\n\n\n\n\nTable 20.2: Reliability of ROAR-Word by Gender\n\n\n\n\n\n\nGender\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9352133\n4349\n\n\nF\n0.9387689\n2189\n\n\nM\n0.9326198\n2160\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 20.3: Reliability of ROAR-Word by FRL (California Sub-sample Only)\n\n\n\n\n\n\nFree/Reduced Lunch Status\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9352133\n250\n\n\nF\n0.8552586\n120\n\n\nP\n0.8620204\n84\n\n\nR\n0.8713555\n46\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 20.4: Reliability of ROAR-Word by EL Status (California Sub-sample Only)\n\n\n\n\n\n\nEnglish Learner Status\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9352133\n250\n\n\nEL\n0.8535509\n142\n\n\nEO\n0.8599942\n70\n\n\nIFEP\n0.8590431\n22\n\n\nRFEP\n0.9009931\n15\n\n\nTBD\nNA\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 20.5: Reliability of ROAR-Word by Primary Language (California Sub-sample Only)\n\n\n\n\n\n\nPrimary Language\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9352133\n239\n\n\nEnglish\n0.8741980\n123\n\n\nSpanish\n0.8439935\n116\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 20.6: Reliability of ROAR-Word by Special Education Status (California Sub-sample Only)\n\n\n\n\n\n\nSpecial Education Status\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9352133\n250\n\n\n0\n0.8596218\n234\n\n\n1\n0.8255854\n16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 20.7: Reliability of ROAR-Word by Hispanic Ethnicity (California Sub-sample Only)\n\n\n\n\n\n\nHispanic Ethnicity\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9352133\n232\n\n\n0\n0.7928431\n13\n\n\n1\n0.8529172\n219\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 20.8: Reliability of ROAR-Word by Race (California Sub-sample Only)\n\n\n\n\n\n\nRace\nEmpirical Reliability\nN\n\n\n\n\nAll\n0.9352133\n232\n\n\nAsian\n0.8859822\n2\n\n\nBlack or African American\n0.0016945\n2\n\n\nFilipino\nNaN\nNaN\n\n\nHawaiian or Other Pacific Islander\nNA\n1\n\n\nHispanic\n0.8529172\n219\n\n\nWhite\n0.8274541\n8",
    "crumbs": [
      "Reliability of ROAR-Español",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reliability of ROAR-Palabra</span>"
    ]
  },
  {
    "objectID": "reliability-frase.html",
    "href": "reliability-frase.html",
    "title": "21  Reliability of ROAR-Frase",
    "section": "",
    "text": "21.1 Criteria for flagging unreliable scores\nROAR-Frase is designed to be totally automated where the student can complete the assessment independent of any assistance from an educator or adult. Instructions are delivered through headphones with engaging an story-line. Additionally, students complete practice trials with feedback to ensure the task instructions are clear. Sentences are presented onscreen and reading is done silently. Students respond with their keyboards. Items are designed in a way that does not require background information to discern if a sentence is true or false.\nA potential concern with automated assessments is that, in the absence of a teacher to administer items individually, monitor responses, and score them, some students may disengage from the task, leading to data that does not accurately reflect their actual abilities. ROAR-Frase, having items that are unambiguous and clear, can detect students who were not engaged during the assessment. Our approach to identifying and highlighting disengaged participants with scores that are thought to be unreliable can be seen below. Figure 21.1 shows a plot of median response time (RT) for each participant against the proportion correct on the assessment, collapsed across both 90-second blocks. It is clear that there is a bimodal distrubition that indicates a group of paritipants who were performing at chance and responding very quickly. Participants with a median response time &lt;1,000ms and proportion correct &lt;0.65 are flagged as unreiliable scores in the ROAR score report and removed from the following analyses as it is believed that these scores do no represent a participant’s true ability.\nFigure 21.1: Criteria for identifying disengaged participants and flagging unreliable scores on ROAR-Frase. Participants displaying extremely rapid responses performed near chance on ROAR-Frase.",
    "crumbs": [
      "Reliability of ROAR-Español",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Reliability of ROAR-Frase</span>"
    ]
  },
  {
    "objectID": "reliability-frase.html#alternate-form-reliability---colombia",
    "href": "reliability-frase.html#alternate-form-reliability---colombia",
    "title": "21  Reliability of ROAR-Frase",
    "section": "21.2 Alternate form reliability - Colombia",
    "text": "21.2 Alternate form reliability - Colombia\nAlternate form reliability for Frase is computed as the Pearson correlation adjusted with the Spearman-Brown formula between scores on the two 90-second blocks that were completed during the same testing session. Figure 21.2 shows a plot of student scores on alternate test forms combining grades and Figure 21.3 shows separate plots for each grade. Table 21.1 reports alternate form reliability for the full Colombian sample and separately by grade. Table 21.2 depicts alternate form reliability for the full Colombian sample separated by gender.\n\n\n\n\n\n\n\n\nFigure 21.2: ROAR-Frase Colombia alternate form reliability across grades. Alternate form reliability is calculated as the Pearson correlation between scores on the two 90-second blocks that were completed in one sitting and adjusted by the Spearman-Brown formula.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 21.3: ROAR-Frase Colombia alternate form reliability within grades. Alternate form reliability is calculated as the Pearson correlation between scores on the two 90-second blocks that were completed in one sitting and adjusted by the Spearman-Brown formula.\n\n\n\n\n\n\n\n\n\nTable 21.1: Alternate form reliability for ROAR-Frase in Colombia split by student grade\n\n\n\n\n\n\nGrade\nAlternate Form Reliability\nN\n\n\n\n\nAll\n0.9039315\n4512\n\n\n2\n0.6908008\n413\n\n\n3\n0.7360957\n490\n\n\n4\n0.7867389\n475\n\n\n5\n0.8000892\n561\n\n\n6\n0.7886965\n569\n\n\n7\n0.7663706\n496\n\n\n8\n0.8227222\n417\n\n\n9\n0.8198782\n408\n\n\n10\n0.8203578\n346\n\n\n11\n0.8363014\n337\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 21.2: Alternate form reliability for ROAR-Frase in Colombia split by student gender\n\n\n\n\n\n\nGender\nAlternate Form Reliability\nN\n\n\n\n\nAll\n0.9039315\n4512\n\n\nFemale\n0.9035495\n1888\n\n\nMale\n0.9055784\n1849\n\n\nNA\n0.9009216\n775",
    "crumbs": [
      "Reliability of ROAR-Español",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Reliability of ROAR-Frase</span>"
    ]
  },
  {
    "objectID": "reliability-frase.html#alternate-form-reliability---united-states",
    "href": "reliability-frase.html#alternate-form-reliability---united-states",
    "title": "21  Reliability of ROAR-Frase",
    "section": "21.3 Alternate form reliability - United States",
    "text": "21.3 Alternate form reliability - United States\nHere we show reliability between blocks 1 and 2 for all United States data (California). As with Colombia data, alternate form reliability for Frase is computed as the Pearson correlation adjusted with the Spearman-Brown formula between scores on the two 90-second blocks that were completed during the same testing session. Figure 21.4 shows a plot of student scores on alternate test forms combining grades and Figure 21.3 shows separate plots for each grade.\nTable 21.3 reports alternate form reliability for the California sample and separately by grade, Table 21.4 shows the breakdown of alternate form reliability by gender, Table 21.5 depicts alternate form reliability for the full California sample separated by English Learner Status, Table 21.6 shows alternate form reliability separated by primary language, Table 21.7 shows breakdown by special education status, and finally, Table 21.8 shows breakdown of reliability by free and reduced lunch status.\n\n\n\n\n\n\n\n\nFigure 21.4: ROAR-Frase U.S. alternate form reliability across grades. Alternate form reliability is calculated as the pearson correlation between scores on the two 90-second blocks that were completed in one sitting and adjusted by the Spearman-Brown formula.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 21.5: ROAR-Frase U.S. alternate form reliability within grade. Alternate form reliability is calculated as the pearson correlation between scores on the two 90-second blocks that were completed in one sitting and adjusted by the Spearman-Brown formula.\n\n\n\n\n\n\n\n\n\nTable 21.3: Alternate form reliability for ROAR-Frase in U.S. by student grade.\n\n\n\n\n\n\nGrade\nAlternate Form Reliability\nN\n\n\n\n\nAll\n0.7567431\n256\n\n\n1\n0.6922893\n106\n\n\n2\n0.7316300\n150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 21.4: Alternate form reliability for ROAR-Frase in U.S. by student gender\n\n\n\n\n\n\nGender\nAlternate Form Reliability\nN\n\n\n\n\nAll\n0.7567431\n256\n\n\nFemale\n0.7847464\n118\n\n\nMale\n0.7271188\n134\n\n\nNA\n0.9835240\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 21.5: Alternate Form Reliability reliability for ROAR-Frase in U.S. by English Learner Status\n\n\n\n\n\n\nEnglish Learner Status\nAlternate Form Reliability\nN\n\n\n\n\nAll\n0.7567431\n256\n\n\nEnglish Learner\n0.7096593\n144\n\n\nEnglish Only\n0.7932300\n73\n\n\nInitial Fluent English Proficiency\n0.7421303\n22\n\n\nReclassified Fluent English Proficiency\n0.7493917\n13\n\n\nNA\n0.9835240\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 21.6: Alternate Form Reliability reliability for ROAR-Frase in U.S. by Primary Language\n\n\n\n\n\n\nPrimary Language\nAlternate Form Reliability\nN\n\n\n\n\nAll\n0.7567431\n256\n\n\nEnglish\n0.7817044\n117\n\n\nSpanish\n0.7292383\n125\n\n\nNA\n0.8662868\n14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 21.7: Alternate Form Reliability reliability for ROAR-Frase in U.S. by Special Education Status\n\n\n\n\n\n\nSpecial Education Status\nAlternate Form Reliability\nN\n\n\n\n\nAll\n0.7567431\n256\n\n\nYes\n0.7913782\n15\n\n\nNo\n0.7498295\n237\n\n\nNA\n0.9835240\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 21.8: Alternate Form Reliability reliability for ROAR-Frase in U.S. by Free and Reduced Lunch Status\n\n\n\n\n\n\nFree and Reduced Lunch\nAlternate Form Reliability\nN\n\n\n\n\nAll\n0.7567431\n256\n\n\nPays\n0.8066802\n82\n\n\nReduced\n0.8148188\n45\n\n\nFree\n0.6528948\n125\n\n\nNA\n0.9835240\n4",
    "crumbs": [
      "Reliability of ROAR-Español",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Reliability of ROAR-Frase</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-swr.html",
    "href": "concurrent-validity-swr.html",
    "title": "22  Single Word Recognition (ROAR-Word) Concurrent Validity",
    "section": "",
    "text": "22.1 Convergent validity with oral measures of single word reading",
    "crumbs": [
      "Construct Validity: Evidence that ROAR subtests reliably measure the intended constructs",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Single Word Recognition (ROAR-Word) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-swr.html#sec-convergent-validity-swr",
    "href": "concurrent-validity-swr.html#sec-convergent-validity-swr",
    "title": "22  Single Word Recognition (ROAR-Word) Concurrent Validity",
    "section": "",
    "text": "22.1.1 Woodock Johnson Basic Reading Skills\n\n22.1.1.1 Background: Published studies\nIn an initial proof-of-concept study we compared proportion correct on a pilot version of ROAR-Word to individually administered Woodcock-Johnson Letter Word Identification (WJ-Word-ID) scores and found an exceptionally high correlation (r = 0.91, disattenuated r = 0.94; (Yeatman et al. 2021); Figure 22.1). Moderation analysis confirmed that ROAR-Word is equally valid for children with dyslexia and typical readers (6-18 years of age). Different measures of single word reading, such as the Woodcock-Johnson (WJ) and Test of Word Reading Efficiency (TOWRE), are highly correlated, and a variety of standardized measures largely tap into the same latent construct. For example, it is common to use a threshold on WJ or TOWRE to group research participants into dyslexic versus control groups (Fletcher et al. 2006). The correlation between timed, untimed, real word, and pseudoword reading measures across these assessments ranged from r = 0.72 to 0.93; ROAR-Word is similarly correlated with each measure (Figure 22.1). Thus, in terms of convergent validity, ROAR-Word is highly correlated with myriad measures that are often used interchangeably in reading and dyslexia research.\n\n\n\n\n\n\nFigure 22.1: Strong correlation between pilot version of ROAR-Word and Woodcock Johnson Letter Word ID\n\n\n\nIn a second proof-of-concept study we optimized the measurement scale with IRT, added additional items tailored to younger participants, and deployed a new version of ROAR-Word that was half the length. Data from kindergarten, first, and second grade students revealed an exceptionally high correlation of r=0.97 between ROAR-Word and WJ-Word-ID (Figure 22.2) (Yeatman et al. 2021).\n\n\n\n\n\n\nFigure 22.2: Strong correlation between pilot version of ROAR-Word and Woodcock Johnson Letter Word ID in grades K-2\n\n\n\n\n\n22.1.1.2 Additional validation against Woodcock Johnson Basic Reading Skills\nThe initial validation studies published in (Yeatman et al. 2021) provided strong initial evidence that ROAR-Word accurately tapped into the construct of single word reading ability. However, the sample published in (Yeatman et al. 2021) was recruited to participate in research studies in the Yeatman Lab and was not, therefore, representative of the diversity of students in the United States. Hence we undertook a series of additional validation studies in collaboration with school districts that had adopted ROAR. Figure 22.3 shows the age distribution and Table 22.1 shows the demographics of the students that participated in these validation studies.\n\n\n\n\n\n\n\n\n\nN\n%\n% Missing\n\n\n\n\nFemale\n185\n45.45\n7.13\n\n\nFree or Reduced Lunch\n51\n12.53\n84.77\n\n\nRace/Ethnicity\n\n\nHispanic Ethnicity\n42\n10.32\n0.00\n\n\nWhite\n193\n47.42\n0.00\n\n\nBlack or African American\n22\n5.41\n0.00\n\n\nAsian\n89\n21.87\n0.00\n\n\nAmerican Indian or Alaska Native\n5\n1.23\n0.00\n\n\nHawaiian or Other Pacific Islander\n2\n0.49\n0.00\n\n\nMultiracial\n51\n12.53\n0.00\n\n\nTotal\n407\n\n\n\n\n\n\n\n\n\n\nTable 22.1: Demographics of participants in concurrent validity study of Woodcock Johnson Basic Reading Skills (WJ BRS) and ROAR-Word\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.3: Age distribution of concurrent validity study of Woodcock Johnson Basic Reading Skills (WJ BRS) and ROAR-Word\n\n\n\n\n\nFigure 22.4 shows the relationship between ROAR-Word raw scores and Woodock Johnson Letter Word Identification (WJ LWID) assessed at the same time point and Figure 22.5 shows this relationship broken down by grade. The strongest relationships are in early elementary school but that is because single word reading is at ceiling by late elementary school for most students. Table 22.2 reports the correlations separately for each grade.\n\n\n\n\n\n\n\n\nFigure 22.4: ROAR-Word is highly correlated with Woodcock Johnson Letter Word Identification (WJ LWID)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.5: Correlations between ROAR-Word and Woodcock Johnson Letter Word Identification across different grade bands.\n\n\n\n\n\nWhen comparing the correlation between ROAR-Word scores and individually administered Woodcock Johnson (WJ) assessments, the correlation is strongest in early elementary school and decreases in middle school and high-school (Table 22.2). But this decrease in correlation is likely driven by the fact that most middle school and high school students are at ceiling in single word reading restricting the range of WJ scores. We used the correlation between the two WJ subtests (Word Identification and Word Attack) as a reliability estimate for WJ subtest scores in each age range and found, as expected, that a decrease in reliability of the WJ explained the lower correlation between ROAR and WJ in the older grades. Table 22.3 shows disattenuated correlations that correct for differences in reliability in each sample.\n\n\n\n\n\n\n\n\nGrade\nWord ID\nWord Attack\nBasic Reading Skills\nWJ Reliability\nN\n\n\n\n\n1\n0.81\n0.81\n0.81\n0.84\n43\n\n\n2\n0.87\n0.87\n0.88\n0.82\n48\n\n\n3\n0.77\n0.77\n0.77\n0.82\n226\n\n\n4\n0.72\n0.72\n0.71\n0.88\n68\n\n\n5\n0.69\n0.69\n0.62\n0.85\n48\n\n\n6-8\n0.64\n0.64\n0.63\n0.78\n83\n\n\n9-12\n0.53\n0.53\n0.59\n0.65\n76\n\n\n\n\n\n\n\n\nTable 22.2: Pearson correlations (\\(\\rho\\)) between ROAR-Word and Woodcock Johnson Scores by grade. Basic Reading Skills is calculated by summing Word ID and Word Attack subtests. WJ Reliability is calculated as the Pearson correlation between Word ID and Word Attack subtests. This reliability metric gives an upper bound on the correlation for ROAR-Word with either subtest\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nWord ID\nWord Attack\nN\n\n\n\n\n1\n0.92\n0.92\n43\n\n\n2\n0.99\n0.99\n48\n\n\n3\n0.88\n0.88\n226\n\n\n4\n0.79\n0.79\n68\n\n\n5\n0.77\n0.77\n48\n\n\n6-8\n0.75\n0.75\n83\n\n\n9-12\n0.69\n0.69\n76\n\n\n\n\n\n\n\n\nTable 22.3: Disattenuated correlations between ROAR-Word and Woodcock Johnson Scores by grade. The correlation between WJ Word ID and WJ Word Attack was used as an estimate of WJ reliability in each sample and ROAR-Word reliability was taken from Chapter 15.\n\n\n\n\nFigure 22.6 shows concurrent validity data comparing ROAR-Word and WJ split by demographic groups. The relationship between ROAR-Word and WJ is similar across different demographics.\n\n\n\n\n\n\n\n\nFigure 22.6: Correlations between ROAR-Word and Woodcock Johnson Letter Word Identification split by demographics\n\n\n\n\n\n\n\n\n22.1.2 Fastbridge\n\n22.1.2.1 Background: Published studies\nThe Formative Assessment System for Teachers (FAST) from FastBridge Learning, is a screener and curriculum based measure widely used across many schools in the United States. In a published validation study of the new, shortened computer adaptive version of ROAR-Word which is now the current standard of practice (ROAR-CAT), we compared the \\(\\theta\\) estimates from ROAR-Word against the individually-admininstered FAST™ earlyReading measure and found a correlation of r=0.89 in 1st grade and r=0.73 in 2nd grade. This initial published study was a small sample size but provided strong evidence of construct validity for the new computer adaptive measure. Figure 22.7 reproduces Figure 10 from Ma et al. (2023) which shows convergent validity of the shortened CAT version of ROAR-Word with FastBridge and Fountas & Pinnell. The following sections undertake similar analyses with a much larger sample including multiple school districts\n\n\n\n\n\n\nFigure 22.7: Strong correlation between Computer Adaptive ROAR-Word (ROAR-CAT) and FastBrdige in grades 1-2.\n\n\n\n\n\n22.1.2.2 Additional validation against Fastbridge\nIn collaboration with two large and diverse school districts in the State of California, we ran a study of concurrent validity to compare ROAR against FastBridge. Table 22.4 shows the demographics of the sample.\n\n\n\n\n\n\n\n\nTable 22.4: Demographics of concurrent validity study comparing ROAR-Word and FastBridge\n\n\n\n\n\n\n\n\n\nN\n%\n% Missing\n\n\n\n\nFemale\n1692\n50.24\n0.65\n\n\nFree or Reduced Lunch\n520\n15.44\n11.13\n\n\nEnglish Learner\n494\n14.67\n11.13\n\n\nSpecial Education Status\n113\n3.36\n11.07\n\n\nRace/Ethnicity\n\n\nHispanic Ethnicity\n703\n20.87\n0.03\n\n\nWhite\n1210\n35.93\n0.03\n\n\nBlack or African American\n76\n2.26\n0.03\n\n\nAsian\n927\n27.52\n0.03\n\n\nAmerican Indian or Alaska Native\n20\n0.59\n0.03\n\n\nHawaiian or Other Pacific Islander\n18\n0.53\n0.03\n\n\nMultiracial\n540\n16.03\n0.03\n\n\nTotal\n3368\n\n\n\n\n\n\n\n\n\nWe compared ROAR-Word scores against following FastBridge measures administered within a month (concurrent validity):\n\nFastBridge Curriculum Based Measurement for Reading (FAST™ CBMreading) is an Oral Reading Fluency (ORF) measure where students read a leveled passage out loud for one minute. The FastBridge technical manual states “CBMreading is a simple, efficient, evidence-based assessment used for universal screening in grades 1 through 8, and progress monitoring for grades 1-12”(Christ and Colleagues 2018, 14). Words Read Correct, or WRC, “is the primary metric used in reporting student performance on FAST™ CBMreading” (Christ and Colleagues 2018, 19). This measure includes scores on three separate passages as well as a composite score.\nFAST™ earlyReading is designed to measure component skills of reading in kindergarten and first grade (Christ and Colleagues 2018, 30). It includes Sight Word (real word list) and Nonsense Word (pseudoword list) decoding measures.\n\nFigure 22.8 shows the relationship between FAST™ CBMreading, FAST™ earlyReading and ROAR-Word scores. Table 22.5 reports the Pearson correlations between each measure. Correlations between all the measures were exceptionally high and the correlation between ROAR-Word and FastBridge was almost as high as the internal consistency of FastBridge measures.\n\n\n\n\n\n\n\n\n\n\n\n(a) ROAR-Word is correlated with FAST™ CBMreading Oral Reading Fluency\n\n\n\n\n\n\n\n\n\n\n\n(b) ROAR-Word is correlated with FAST™ earlyReading Composite\n\n\n\n\n\n\n\nFigure 22.8: ROAR-Word is highly correlated with FAST™ CBMreading and FAST™ earlyReading\n\n\n\n\n\n\n\n\n\n\n\n\nROAR-Word\nORF Passage 1\nORF Passage 2\nORF Passage 3\nORF Composite\nNonsense Words\nSight Words\nearlyReading Composite\n\n\n\n\nROAR-Word\n1.00\n0.82\n0.82\n0.82\n0.83\n0.75\n0.78\n0.76\n\n\nORF Passage 1\n0.82\n1.00\n0.96\n0.96\n0.98\n0.86\n0.87\n0.90\n\n\nORF Passage 2\n0.82\n0.96\n1.00\n0.96\n0.99\n0.84\n0.87\n0.90\n\n\nORF Passage 3\n0.82\n0.96\n0.96\n1.00\n0.98\n0.84\n0.87\n0.91\n\n\nORF Composite\n0.83\n0.98\n0.99\n0.98\n1.00\n0.85\n0.87\n0.91\n\n\nNonsense Words\n0.75\n0.86\n0.84\n0.84\n0.85\n1.00\n0.81\n0.86\n\n\nSight Words\n0.78\n0.87\n0.87\n0.87\n0.87\n0.81\n1.00\n0.85\n\n\nearlyReading Composite\n0.76\n0.90\n0.90\n0.91\n0.91\n0.86\n0.85\n1.00\n\n\n\n\n\n\n\n\nTable 22.5: Convergent validity of ROAR-Word: Comparision to FastBridge\n\n\n\n\nTable 22.6 Shows the correlation between FAST™ earlyReading ROAR-Word in kindergarten (ORF is not typically administered until first grade). Table 22.7 shows the correlations for first grade and, Table 22.8 shows the correlations for second grade.\n\n\n\n\n\n\n\n\n\nROAR-Word\nNonsense Words\nSight Words\nearlyReading Composite\n\n\n\n\nROAR-Word\n1.00\n0.60\n0.63\n0.58\n\n\nNonsense Words\n0.60\n1.00\n0.81\n0.90\n\n\nSight Words\n0.63\n0.81\n1.00\n0.90\n\n\nearlyReading Composite\n0.58\n0.90\n0.90\n1.00\n\n\n\n\n\n\n\n\nTable 22.6: Convergent validity of ROAR-Word: Comparision to FAST™ earlyReading in kindergarten\n\n\n\n\n\n\n\n\n\n\n\n\n\nROAR-Word\nORF Passage 1\nORF Passage 2\nORF Passage 3\nORF Composite\nNonsense Words\nSight Words\nearlyReading Composite\n\n\n\n\nROAR-Word\n1.00\n0.81\n0.83\n0.82\n0.83\n0.73\n0.77\n0.79\n\n\nORF Passage 1\n0.81\n1.00\n0.96\n0.96\n0.98\n0.86\n0.87\n0.90\n\n\nORF Passage 2\n0.83\n0.96\n1.00\n0.97\n0.99\n0.84\n0.87\n0.90\n\n\nORF Passage 3\n0.82\n0.96\n0.97\n1.00\n0.99\n0.84\n0.87\n0.91\n\n\nORF Composite\n0.83\n0.98\n0.99\n0.99\n1.00\n0.85\n0.87\n0.91\n\n\nNonsense Words\n0.73\n0.86\n0.84\n0.84\n0.85\n1.00\n0.79\n0.87\n\n\nSight Words\n0.77\n0.87\n0.87\n0.87\n0.87\n0.79\n1.00\n0.87\n\n\nearlyReading Composite\n0.79\n0.90\n0.90\n0.91\n0.91\n0.87\n0.87\n1.00\n\n\n\n\n\n\n\n\nTable 22.7: Convergent validity of ROAR-Word: Comparision to FAST™ earlyReading in first grade\n\n\n\n\n\n\n\n\n\n\n\n\n\nROAR-Word\nORF Passage 1\nORF Passage 2\nORF Passage 3\nORF Composite\nearlyReading Composite\n\n\n\n\nROAR-Word\n1.00\n0.80\n0.79\n0.78\n0.80\nNA\n\n\nORF Passage 1\n0.80\n1.00\n0.95\n0.95\n0.98\nNA\n\n\nORF Passage 2\n0.79\n0.95\n1.00\n0.95\n0.98\nNA\n\n\nORF Passage 3\n0.78\n0.95\n0.95\n1.00\n0.98\nNA\n\n\nORF Composite\n0.80\n0.98\n0.98\n0.98\n1.00\nNA\n\n\nearlyReading Composite\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nTable 22.8: Convergent validity of ROAR-Word: Comparision to FAST™ earlyReading in second grade",
    "crumbs": [
      "Construct Validity: Evidence that ROAR subtests reliably measure the intended constructs",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Single Word Recognition (ROAR-Word) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-sre.html",
    "href": "concurrent-validity-sre.html",
    "title": "23  Sentence Reading Efficiency (ROAR-Sentence) Concurrent Validity",
    "section": "",
    "text": "23.1 Convergent validity with silent sentence reading fluency",
    "crumbs": [
      "Construct Validity: Evidence that ROAR subtests reliably measure the intended constructs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Sentence Reading Efficiency (ROAR-Sentence) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-sre.html#convergent-validity-with-silent-sentence-reading-fluency",
    "href": "concurrent-validity-sre.html#convergent-validity-with-silent-sentence-reading-fluency",
    "title": "23  Sentence Reading Efficiency (ROAR-Sentence) Concurrent Validity",
    "section": "",
    "text": "23.1.1 ROAR-Sentence (ROAR-SRE) correlation with Woodcock-Johnson Sentence Reading Fluency (WJ-SRF)\nROAR-Sentence is designed to measure the latent construct of silent sentence reading efficiency, which represents the speed or efficiency with which a student can read simple sentences for understanding. The goal of the ROAR-Sentence task is to isolate reading efficiency by minimizing comprehension demands while maintaining checks for understanding.\nWe establish concurrent validity for ROAR-Sentence through a large-scale validation study that compares student performance on ROAR-Sentence to performance on the Woodcock-Johnson Sentence Reading Fluency (WJ-SRF) subtest, (Schrank et al. 2014). The development of ROAR-Sentence and the results of the validation study are detailed in study 3 of (Tran et al. 2023).\n\n23.1.1.1 Background\nOur goal in designing a new silent sentence reading efficiency measure was to more directly target reading efficiency by designing simple sentences that are unambiguously true or false and have minimal requirements in terms of vocabulary, syntax and background knowledge.\nTraditional measures that are most similar to ROAR-Sentence are sometimes referred to as sentence reading fluency tasks, and while they are not administered online, they do elicit silent responses from students. The Woodcock Johnson (WJ) Tests of Achievement “Sentence Reading Fluency” subtest (Schrank et al. 2014), relies on an established design: A student reads a set of sentences and endorses whether each sentence is true or false. A student endorses as many sentences as they can within a fixed time limit (usually three minutes). The final score is the total number of correctly endorsed sentences minus the total number of incorrectly endorsed sentences.\nThe WJ-SRF is standardized to be administered in a one-on-one setting and the stimuli consist of printed lists of sentences which students read silently and mark Yes/No with a pencil to endorse the sentences as true or false (Schrank et al. 2014). Even though the criteria for item development on these assessments is not specified in detail, there is a growing literature showing the utility of this general approach. A similar paper-based silent reading assessment, the Test of Silent Reading Efficiency and Comprehension (TOSREC), also involves endorsing sentences as TRUE/FALSE during a 3 minute time period. It is straightforward to administer and score and has exceptional reliability (Johnson, Pool, and Carter 2011; Wagner, R. K., Torgesen, J. K., Rashotte, C. A., & Pearson, N. A. 2010; Wagner 2011).\n\n\n23.1.1.2 Participants\nParticipants for the validation study were recruited through two methods. The first validation sample was obtained from a longitudinal study of children with dyslexia (ages 8-14; grades 2-8; and adults ages 19-34) and a study of visual attention (children ages 7-17; grades 1-11). In these studies trained researcher coordinators individually administered standardized assessments and participants then completed ROAR-Sentence (Tran et al. 2023).\nThe second validation sample comprised 3rd grade students from a local school district that agreed to participate in the validation study (see Table S1 in (Tran et al. 2023) for school demographics). 3rd grade was selected for validation because it is the most common age for a dyslexia diagnosis. To conduct in-person validations in schools, a team of 7 researcher coordinators administered assessments to the students. All research coordinators completed human subjects research training, practiced extensively, and shadowed senior administrators before conducting assessments on students. Each research coordinator completed training with feedback until they were able to reliably administer each assessment. The selection of students was based on the interest of parents and teachers. Prior to the research, parents and guardians were given the opportunity to opt their students out of the research. Teachers were also informed, and their interest in the research was conveyed to the district superintendent, who then notified the research team.\nResearch into ROAR-Sentence is ongoing, and a third validation sample was collected after (Tran et al. 2023) was submitted for preprint. Sample 3 includes students in grades 1-8. It was collected from a private school in a low-income urban neigborhood in California by the same team of research coordinators who collected Sample 2.\nDemographics for the sample are shown in #tbl-sre-wj-demographics. The distribution of participants by age is shown in #fig-sentence-age-histogram.\n\n\n\n\n\n\n\n\n\nN\n%\n% Missing\n\n\n\n\nFemale\n125\n49.21\n4.72\n\n\nFree or Reduced Lunch\n14\n5.51\n94.49\n\n\nRace/Ethnicity\n\n\nHispanic Ethnicity\n20\n7.87\n4.72\n\n\nWhite\n97\n38.19\n4.72\n\n\nBlack or African American\n10\n3.94\n4.72\n\n\nAsian\n79\n31.10\n4.72\n\n\nAmerican Indian or Alaska Native\n2\n0.79\n4.72\n\n\nHawaiian or Other Pacific Islander\n0\n0.00\n4.72\n\n\nMultiracial\n37\n14.57\n4.72\n\n\nTotal\n254\n\n\n\n\n\n\n\n\n\n\nTable 23.1: Demographics for ROAR-Sentence WJ-SRF Validation\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 23.1: Distribution of ROAR-Sentence validation data by age\n\n\n\n\n\n\n\n23.1.1.3 Measures\nStudents in Sample 1 completed individually-administered reading assessments in the course of intake screening for a longitudinal study of children with dyslexia and a separate study of visual attention. Students in Sample 2 and Sample 3 were pulled out of their classrooms to complete individually-administered reading assessments. Testing for both samples included (1) Woodcock Johnson IV Tests of Achievement Sentence Reading Fluency (WJ-SRF), in which participants silently read sentences on paper in a one-on-one setting as quickly as possible and endorse them as true or false; (2) Letter Word Identification (WJ-LWID) in which participants read words out loud and are scored for accuracy; (3) Word Attack (WJ-WA) in which participants read pseudowords out loud and are scored for accuracy (Schrank et al. 2014); (4) Test of Word Reading Efficiency Sight Word Efficiency (TOWRE-SWE) in which participants read lists of real words as quickly and accurately as possible; (5) Phonemic Decoding Efficiency (TOWRE-PDE) in which participants read lists of pseudowords as quickly and accurately as possible (Torgesen, Wagner, and Rashotte 2011). Each student completed ROAR-Sentence as part of their regular school day without the presence of researchers within 2 months prior to the in-person validation.\n\n\n23.1.1.4 Results\nWe found a strong correlation between ROAR-SRE and WJ-SRF in Samples 1 and 2 (r=0.92, r=0.91), and across all samples (r=0.88) (#fig-sample-1-3).\n\n\n\n\n\n\n\n\n\n\n\n(a) Sample 1 (Figure 6B, (Tran et al. 2023))\n\n\n\n\n\n\n\n\n\n\n\n(b) Sample 2 (Figure 6A, (Tran et al. 2023))\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) All Samples\n\n\n\n\n\n\n\nFigure 23.2: ROAR-SRE is highly correlated to standardized, individually-administered, in-person assessments of reading fluency.\n\n\n\nStrong correlation between ROAR-SRE and WJ-SRF was observed across all demographic groups (#fig-sre-srf-corr-by-demo).\n\n\n\n\n\n\n\n\nFigure 23.3: Distribution of ROAR-Sentence validation data by demographic\n\n\n\n\n\nAcross all the samples, ROAR-SRE was moderately correlated with untimed single word reading accuracy (WJ-LWID, r=0.68), untimed pseudoword reading accuracy (WJ-WA, r=0.56), real word list reading speed (TOWRE-SWE, r=0.66), and pseudoword list reading speed (TOWRE-PDE, r=0.57) (#fig-sre-corr-matrix). This pattern of correlations supports the notion that sentence reading efficiency is a separable, yet highly related construct, to single word reading speed and accuracy.\n\n\n\n\n\n\n\n\nFigure 23.4: ROAR-SRE is moderately correlated with untimed single word reading accuracy (WJ-LWID), untimed pseudoword reading accuracy (WJ-WA), real word list reading speed (TOWRE-SWE), and pseudoword list reading speed (TOWRE-PDE),\n\n\n\n\n\nIn addition to examining the correlation between SRE and WJ-SRF, the study used precise timing data to investigate the optimal length for the assessment. Many assessments of sentence reading fluency/efficiency are 3 minutes by convention but previous work has not systematically analyzed the relationship between assessment length and reliability. Precise timing information collected by the application was used to calculate each participants’ ROAR-SRE score at 10 second time intervals which was then correlated against the full 3 minute WJ-SRF scores. The correlation between ROAR-SRE and WJ-SRF increased as a function of assessment length. However, the correspondence between the two measures hit a peak between 60 and 90 seconds (#fig-timing) indicating that the remaining assessment time did not further contribute to the reliability of the measure.\n\n\n\n\n\n\n\n\nFigure 23.5: Correlation between ROAR-SRE and WJ-SRF as a function of assessment time ((Tran et al. 2023), Figure 6C)\n\n\n\n\n\n\n\n23.1.1.5 Discussion\nThe study demonstrated that the unproctored, online ROAR-Sentence (ROAR-SRE) assessment was highly correlated with a similar, standardized measure delivered one-on-one in person (WJ-SRF). This provides strong evidence for the concurrent validity of an online measure. Moreover, the stronger correspondence between sentence reading (WJ-SRF) versus single word decoding (WJ-LWID and WJ-WA) and single word reading efficiency (TOWRE-SWE and TOWRE-PDE) measures demonstrated that sentence and word reading are related but dissociable constructs as highlighted in other work (Silverman et al. 2013). Finally, the analysis of assessment length demonstrated that a one minute sentence reading efficiency measure achieves high reliability. This finding opens the possibility of more regular progress monitoring with a quick and automated one minute assessment.",
    "crumbs": [
      "Construct Validity: Evidence that ROAR subtests reliably measure the intended constructs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Sentence Reading Efficiency (ROAR-Sentence) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-sre.html#convergent-validity-with-oral-reading-fluency-orf",
    "href": "concurrent-validity-sre.html#convergent-validity-with-oral-reading-fluency-orf",
    "title": "23  Sentence Reading Efficiency (ROAR-Sentence) Concurrent Validity",
    "section": "23.2 Convergent validity with oral reading fluency (ORF)",
    "text": "23.2 Convergent validity with oral reading fluency (ORF)\nIn collaboration with two large and diverse school districts in the State of California, we ran a study of concurrent validity to compare ROAR against FastBridge earlyReading. The Formative Assessment System for Teachers (FAST) from FastBridge Learning, is a screener and curriculum based measure widely used across many schools in the United States.\n\n23.2.1 ROAR-Sentence (ROAR-SRE) validation against FastBridge earlyReading\nWe compared the raw from ROAR-Sentence against the individually-admininstered FAST™ earlyReading measure and found a correlation for the Oral Reading Fluency Composite of 0.85 across grades 1-3.\n\n\n23.2.2 Participants\nThe demographics of the sample that completed both ROAR-SRE and FastBridge are displayed in Table 23.2\n\n\n\n\n\n\n\n\n\nN\n%\n% Missing\n\n\n\n\nFemale\n902\n46.78\n1.24\n\n\nFree or Reduced Lunch\n513\n26.61\n20.59\n\n\nEnglish Learner\n403\n20.90\n20.59\n\n\nSpecial Education Status\n131\n6.79\n20.59\n\n\nRace/Ethnicity\n\n\nHispanic Ethnicity\n626\n32.47\n0.10\n\n\nWhite\n432\n22.41\n0.10\n\n\nBlack or African American\n8\n0.41\n0.10\n\n\nAsian\n360\n18.67\n0.10\n\n\nAmerican Indian or Alaska Native\n1\n0.05\n0.10\n\n\nHawaiian or Other Pacific Islander\n6\n0.31\n0.10\n\n\nMultiracial\n178\n9.23\n0.10\n\n\nTotal\n1928\n\n\n\n\n\n\n\n\n\n\nTable 23.2: Demographics for ROAR-Sentence Fastbridge Validation\n\n\n\n\n\n23.2.2.1 Results\n\n\n\n\n\n\n\n\nFigure 23.6: ROAR-SRE is strongly correlated with Fastbridge ORF Composite.",
    "crumbs": [
      "Construct Validity: Evidence that ROAR subtests reliably measure the intended constructs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Sentence Reading Efficiency (ROAR-Sentence) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-pa.html",
    "href": "concurrent-validity-pa.html",
    "title": "24  Phonological Awareness (ROAR-Phoneme) Concurrent Validity",
    "section": "",
    "text": "24.1 Background: Published studies",
    "crumbs": [
      "Construct Validity: Evidence that ROAR subtests reliably measure the intended constructs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Phonological Awareness (ROAR-Phoneme) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-pa.html#background-published-studies",
    "href": "concurrent-validity-pa.html#background-published-studies",
    "title": "24  Phonological Awareness (ROAR-Phoneme) Concurrent Validity",
    "section": "",
    "text": "24.1.1 Evolution of the Design of ROAR-Phoneme items and subtests\nThe original ROAR-Phoneme subtests were selected based on well-known standardized in-person PA tasks (e.g., Wagner, Torgesen, and Rashotte (1999)):\n\nFirst Sound Matching (FSM) the participant has to find a word with the same first sound as the target word\nLast Sound Matching (LSM) the participant has to find a word with the same last sound as the target word\nRhyming (RHY) the participant has to find the word that rhymes with the target word\nBlending (BLE) the participant has to merge parts of a word together and select the appropriate target word\nDeletion (DEL) the participant has to determine what is left after a section of the word is omitted.\n\nItems for ROAR-Phoneme were designed such that selecting the correct answer would require the same cognitive operation as a traditional PA assessment with verbal responses. To achieve this, each item requires the participant to perform the same operation in their mind (e.g., determining if the first/last sound of two words matches; removing phonemes from a word), but the answer is selected from a set of alternatives rather than verbalized.\nIn the original design, FSM, LSM and RHY each consisted of 25 trials, divided into 2 blocks (16 and 9 items). The difference between blocks of these 3 subtests was finding the first sound (FSM), last sound (LSM), or word that rhymed (RHY) of a CVC word (difficulty level 1, 16 items) or a (C)CVC(C) word (difficulty level 2, 9 items). Thus, for the easier items (i.e., difficulty level 1) children had to identify a single phoneme (e.g., of FSM: Q: “Which picture starts with the same sound as pin?” A: “pup”), whereas for the more difficult items (i.e., difficulty level 2), children had to identify a consonant sound within a phoneme cluster (e.g., of FSM: Q: “Which picture starts with the same sound as clown?” A: “crab”). For FSM the three answer options were either the target (i.e., same first sound), a foil that started with the last sound of the provided word (Foil 1), or a foil with the same vowel (Foil 2). For LSM the same reasoning was made, but for the last sound of the word. For RHY the target word would rhyme, whereas Foil 1 would have the same vowel but would not rhyme and Foil 2 would have the same first sound. BLE and DEL each consisted of 24 items, divided into 3 difficulty levels (i.e., syllable level, onset or rime level, phoneme level) with each 8 items. These difficulty levels were based on a suggested hierarchy within PA skills (Stanovich 2017; Treiman and Zukowski 1991; Anthony and Lonigan 2004) . For example, for the subtest DEL an item of difficulty level 1 could be: Q: “What is lipstick without stick?” A: “lip”, for difficulty level 2: Q: “What is farm without ‘f’?” A: “arm”, and for difficulty level 3: Q: “What is snail without ‘n’?” A: “sail”. For both the BLE and DEL subtests, all additions and omissions led to lexical changes rather than morphological changes of the word structure. An item was either scored as correct (i.e., target selected) or as incorrect (i.e., foil selected). No distinction was made in the scores based on which foil was selected.\n\n\n24.1.2 Proof-of-concept: Validation of items and composite scores\nTo validate the feasibility of a web-browser based PA task (containing 5 subtests: FSM, LSM, RHY, BLE, and DEL) that only required clicks/touchscreen responses, we tested 143 participants (Age: 3.87–13.00, \\(\\mu\\)=7.13, \\(\\sigma\\)=1.89; Sex: 67 F, 76 M) and performed a correlation analysis between each ROAR-PA subtest and the well-established standardized CTOPP-2. The results (Fig. 1, left panel) revealed strong correlations between the CTOPP-2 and all ROAR-Phoneme subtests: LSM (\\(r\\)=0.65), DEL (\\(r\\)=0.62), FSM (\\(r\\)=0.61), RHY (\\(r\\)=0.60), and BLE (\\(r\\)=0.55). Each subtest, except for BLE, showed high internal consistency based on Cronbach’s \\(\\alpha\\) (LSM: \\(\\alpha\\)=0.92, CI95=[0.89; 0.93], FSM: \\(\\alpha\\)=0.90, CI95=[0.87; 0.93], RHY: \\(\\alpha\\)=0.86, CI95=[0.81; 0.89], DEL: \\(\\alpha\\)=0.84, CI95=[0.77; 0.88], BLE: \\(\\alpha\\)=0.70, CI95=[0.57; 0.78]) and the composite scores of both CTOPP-2 (\\(\\alpha\\)=0.88, CI95=[0.85 ; 0.91]) and ROAR-PA (\\(\\alpha\\)=0.85, CI95=[0.80; 0.89]) had good (0.8\\(\\leq\\alpha\\)&lt;0.9) internal consistency.\n\n\n\n\n\n\nFigure 24.1: Initial validation of ROAR-Phoneme items and composite scores. : A Pearson correlation matrix between the 5 ROAR-PA subtests (% correct), CTOPP-2 subtests (raw scores), and overall (raw) scores on both tasks, of the original cohort that completed all 5 subtests (N=143). Orange box: correlation coefficients between ROAR-PA and overall CTOPP-2; Blue Box: correlation coefficients between ROAR-PA and subtests of CTOPP-2; Black boxes: correlation coefficients within subtests of ROAR-PA and subtests of CTOPP-2. : Item Correlation Analysis: For each stimulus, we plot the point-biserial correlation between performance on the item and ROAR-PA accuracy (x-axis) as well as the correlation between performance on the item and CTOPP-2 raw score (y-axis). Items with low correlations with overall test performance or CTOPP-2 performance (threshold r≤.10; dotted red line) were removed from the test.\n\n\n\n\n\n24.1.3 Optimization of ROAR-PA as a screening tool\nTo optimize ROAR-PA as a valid screening tool we sought to create a composite score that best approximated the CTOPP-2 composite index. To do so, we created a linear model with the CTOPP-2 scores as the dependent variable and the scores of each individual subtest as predictor variables. This model (CTOPP-2~FSM+LSM+RHY+BLE+DEL) showed that the subtests FSM (\\(\\beta\\) = 0.79; t=2.33; p=0.02), LSM (\\(\\beta\\) = 1.07; t=3.92; p&lt;0.001), and DEL (\\(\\beta\\) = 1.27; t=3.13; p=0.002) were significant predictors of the CTOPP-2 scores, but the subtests RHY (\\(\\beta\\) = 0.44; t=1.13; p&gt;0.10), and BLE (\\(\\beta\\) = 0.55; t=0.90; p&gt;0.10, were not. We then used a Likelihood Ratio test to determine the influence of these non-significant subtests in our CTOPP-2 prediction, by comparing the full model, as described above, to a model without BLE (4 ROAR-PA subtests: FSM, LSM, RHY, DEL); and a model with only the 3 significant subtests (FSM, LSM, DEL). We found no significant differences in model predictions between the full model and the 4 subtest model (\\(\\chi^2\\)=0.85; p&gt;0.10), nor the full model and the 3 subtest model (\\(\\chi^2\\)=2.05; p&gt;0.10), suggesting that the three subtests (FSM, LSM, DEL) are sufficient to obtain an accurate PA composite that approximates the CTOPP (\\(R^2\\)=0.57).\nThese findings are corroborated by interpreting the Pearson correlation coefficients between ROAR-PA and CTOPP-2. Although the highest correlation was reported by summing the scores on all 5 ROAR-PA subtests (\\(r\\)=0.76), a composite score based on 4 (FSM, LSM, DEL,RHY) or 3 ROAR-PA subtests (FSM, LSM, DEL) was equally correlated with CTOPP-2 (\\(r\\)=0.75). The 3-subtest composite and 4-subtest composite both achieved good reliability as well: Cronbach’s alpha of \\(\\alpha_{4subtests}\\)=0.84, CI95=[0.77; 0.88] and \\(\\alpha_{3subtests}\\)=0.78, CI95=[0.67; 0.84] respectively. As convergent validity greater than \\(r\\)=0.70 is recommended to reflect whether two measures capture a common construct, it can be concluded that all possible composite scores (5, 4, and even 3 subtests) suffice to capture PA skills.\nFurthermore, an item analysis comparing the correlations between the item responses of ROAR-PA for each of the 123 test items and CTOPP-2 scores showed that performance on items from the subtest LSM were especially highly correlated with overall ROAR-PA performance and CTOPP-2 performance (Figure 24.1). This suggests LSM items are most informative about overall PA abilities. Items from the BLE subtest were least informative: the correlation between most blending items and ROAR-PA total score and CTOPP-2 total score was close to zero.\n\n\n24.1.4 Ideal age range for ROAR-Phoneme\nAfter selecting 3 subtests that make an efficient and reliable ROAR-PA composite score, we collected ROAR-PA data for an additional group of 127 participants, including mostly older children, resulting in a total of 270 participants (Age: 3.87–14.92, \\(\\mu\\)=9.12, \\(\\sigma\\)=2.71; Sex: 125 F, 145 M) who completed ROAR-PA FSM, LSM, and DEL subtests. Of these participants, 266 were also administered the CTOPP-2 PA assessment. The Pearson correlation analysis with the CTOPP-2 for this extended group of participants resulted in an overall correlation between CTOPP-2 and ROAR-PA composite (3 subtests) of \\(r\\)=0.70 (as opposed to \\(r\\)=0.75 in the initial sample of participants). The correlation between the CTOPP-2 and the individual subtests also went down for FSM and LSM (Fig. 2. Left top). The decrease in correlation likely reflected ceiling effects in older participants (Figure 24.2).\nTo examine the effect of age on the correlation between ROAR-PA and CTOPP-2, we split our sample into 3 different age bins (3.87–6.99 years old (N=71), 7.00–9.99 years old (N=91), 10.00–14.92 years old (N=104)). We found a correlation coefficient between the composite scores of ROAR-PA and CTOPP-2 of \\(r\\)=0.79 (CI95=[0.68; 0.86], Cronbach’s \\(\\alpha\\)=0.88) for the youngest group, \\(r\\)=0.69 (CI95=[0.56; 0.78], Cronbach’s \\(\\alpha\\)=0.79) for the middle group, and \\(r\\)=0.31 (CI95=[0.13; 0.48], Cronbach’s \\(\\alpha\\)=0.65) for the oldest group of children. Further correlation and Rasch analyses provided an ideal age range of up to 9.50 years old for the ROAR-PA (Figure 24.2), leading to a Pearson correlation coefficient of \\(r\\)=0.80 (CI95=[0.73; 0.85], Cronbach’s \\(\\alpha\\) of 0.80) between the ROAR-PA composite and CTOPP-2, and an increase of the correlations for individual subtests (FSM, LSM, DEL) to the CTOPP-2. This indicates that the ROAR-PA in its current form is predictive of PA skills for children in pre-kindergarten through fourth grade (Figure 24.2) but has ceiling effects above fourth grade. Interestingly, the correlation analysis in our sample shows a similar effect for the CTOPP-2 scores, indicating that both PA tasks (ROAR-PA and CTOPP-2) are most suited for younger children.\n\n\n\n\n\n\nFigure 24.2: : Pearson correlation matrix between the 3 selected ROAR-PA subtests (% correct) and between the CTOPP-2 subtests (raw scores) and overall scores on both tasks for all children (age 3.87—14.92 years; N=266—top) and for a subset of children, based on the limited age-selection (age 3.87–9.50 years; N=145—bottom). Orange box: correlation coefficients between ROAR-PA and overall CTOPP-2; Black boxes: correlation boxes within subtests of ROAR-PA and subtests of CTOPP-2. : Pearson correlation plot between the ROAR-PA (% correct) and between the CTOPP-2 (raw scores), for all children (N=266) age 3.87—14.92 years. The red dotted oval points to the ceiling effect of the oldest children. : Test information functions for the ROAR-PA. The x-axis shows ability estimates based on the Rasch model. The upper x-axis shows the estimated CTOPP-2 raw score equivalent based on the linear relationship between ability estimates and CTOPP-2 scores. The pink lines indicate age equivalents for CTOPP-2 scores (based on the CTOPP-2 manual). Test information is high for participants scoring between 4.5 and 9.5 years age equivalent on the CTOPP-2.\n\n\n\n\n\n24.1.5 Factor structure of Phonological Awareness\nTo evaluate the dimensionality of the ROAR-PA assessment we used exploratory FA with oblique rotation. FA poses the question of whether there is evidence that all of these items are measuring the same underlying phonological processing ability, or whether the items of these subtests better represent separable (but correlated) dimensions of PA.\nOur results suggest a multi-dimensional framework. First, the scree plot (Figure 24.3) of the different items (N=74) on these three subtests (FSM, LSM, DEL) indicate three factors before the rate of decrease flattens. Second, the magnitudes of the loadings for the three-factor model are larger than the one-factor model. Finally, examining the factor loadings, the items from each of the three subtests cleanly separate into separate factors, with the exception of a single item: FSM_13.\n\n\n\n\n\n\nFigure 24.3: Parallel Analysis of Scree plots of 74 items on three subtests (FSM, LSM, DEL). Inspection of the scree plot suggests three factors before the amount of variation represented by the eigenvalues flattens out. The dotted red lines represent extracted eigenvalues from data sets that are randomly created; there are three factors with observed eigenvalues that are larger than those extracted from the simulated data.\n\n\n\n\n\n24.1.6 Item Response Theory analysis: Rasch model\nIn a second step we identified a subset of items from ROAR-PA to remove in order to both improve model fit and reduce the length of the assessment. Given the evidence for a multi-dimensional framework, we proceeded by calibrating a Rasch Model separately for each of the subtests (FSM, LSM and DEL). In this IRT analysis we included data for all participants between 3.87 and 9.50 years old. For each subtest we reviewed four criteria, compiled from both the factor analysis and Rasch Model item fit statistics, to determine the best subset of items: (1) Does the item load on the subtest factor with a relationship &gt; .30? (Tabachnick et al. 2012) (2) Does the item resemble a functional form when looking at empirical plots? ((Allen and Yen 2001) (3) Is the item flagged based on Rasch model fit statistics (Wright 1994)? (4) Finally, as we want items to be informative and not redundant, is the item located near two or more items based on difficulty distribution, to create a test length that seems appropriate for children’s attention spans (Figure 24.5)?\nAnalysis of the FSM subtest (Figure 24.5) suggested removing 6 of the 25 items. After removing these items, no major degradation or change in the key item statistics for this assessment was observed. Cronbach’s \\(\\alpha\\) remained high (\\(\\alpha_{FSMallitems}\\) = .90, CI95 = [.87 ; .93] & \\(\\alpha_{FSMadjusted}\\) = .89, CI95 = [.85 ; .92]), and the distributions of the proportion-correct values and the point-biserial correlations for all items remained similar. The correlation (Fig. 2, left bottom) between FSM total scores and CTOPP-2 stayed about the same (\\(r_{FSMallitems}\\) = .69, \\(r_{FSMadjusted}\\) = .67). Analysis of the LSM subtest (Fig 4., left bottom) also suggested removing 6 out of 25 items. Similar to FSM, Cronbach’s \\(\\alpha\\) of LSM remained high (\\(\\alpha_{LSMallitems}\\) = .92, CI95 = [.90 ; .94] & \\(\\alpha_{LSMadjusted}\\) = .92, CI95 = [.90 ; .93]), the distributions of the proportion-correct values, the point-biserial correlations, and the correlation between the total scores and the CTOPP-2 remained similar (\\(r_{LSMallitems}\\) = .70, \\(r_{LSMadjusted}\\) = .70). Analysis of the DEL subtest (Figure 24.5) indicates removal of 5/24 items. Again, Cronbach’s \\(\\alpha\\) remained high (\\(\\alpha_{DELallitems}\\) = .86, CI95 = [.79 ; .89] & \\(\\alpha_{DELadjusted}\\) = .85, CI95 = [.78 ; .89]), the distributions of the proportion-correct values, the point-biserial correlations, and the correlation between the total scores and the CTOPP-2 remained similar (\\(r_{DELallitems}\\) = .65, \\(r_{DELadjusted}\\) = .63).\n\n\n\n\n\n\nFigure 24.4: Item analysis: Rasch models with .333 fixed guess rate, including students between 3.87 and 9.50 years old, that were not excluded based on clicking or foil patterns for the three remaining subtests (FSM, LSM and DEL). : Deleted items based on a minimum of meeting 2/4 criteria. & : Representation of correlation with CTOPP-2, Cronbach’s \\(\\alpha\\), Distributions of %-correct values and the point-biserial correlations for all items per subtest and for the subtests with items deleted based on the four criteria.\n\n\n\nThis Rasch item analysis suggests that every subtest of this ROAR-PA task has a good (DEL) to excellent (FSM, LSM) internal consistency, based on Cronbach’s \\(\\alpha\\), with a strong correlation of every subtest (r &gt; .65) to the overall CTOPP-2 scores. Item analysis based on meeting at least 2/4 suggested criteria, results in 19 items per subtest, and an overall task of 57 items + 2 practice items per subtest.\nROAR-Phoneme items were designed to span different theoretical levels of difficulty (e.g., Stanovich 2017; Treiman and Zukowski 1991). For the DEL subtest, difficulty levels were based on manipulation of (1) words and syllables (item 1-8), (2) onset and rimes (item 9-16), or (3) phonemes in the middle of the word (item 17-24). For FSM and LSM we can not follow these levels, as the task itself focuses on the first or last phoneme(s) of the word. We tried to create difficulty levels by manipulating single phonemes (level 1: item 1-16) or a single phoneme in a phoneme cluster (level 2: item 17-25). Surprisingly, based on the Rasch Model item-person maps for the three subtests (Fig. 5), we only found that the subtest DEL approximately follows the expected difficulty pattern. This analysis also showed that for FSM most items are closer to the lower-range of ability. For LSM and DEL, most items are close to the mid-range of ability.\n\n\n\n\n\n\nFigure 24.5: Item-person maps (or “Wright Maps”) for the three subtests. Per subtest, the distribution of the ability of the students is plotted on the left-hand side; higher ability is closer to the top of the map. The distribution of the item difficulty is plotted on the right-hand side. The deleted items from the item analysis are grayed out.",
    "crumbs": [
      "Construct Validity: Evidence that ROAR subtests reliably measure the intended constructs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Phonological Awareness (ROAR-Phoneme) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-pa.html#correlations-between-roar-phoneme-and-roar-word",
    "href": "concurrent-validity-pa.html#correlations-between-roar-phoneme-and-roar-word",
    "title": "24  Phonological Awareness (ROAR-Phoneme) Concurrent Validity",
    "section": "24.2 Correlations between ROAR-Phoneme and ROAR-Word",
    "text": "24.2 Correlations between ROAR-Phoneme and ROAR-Word\nPA is assessed at the beginning of reading instruction because of the relationship between PA and decoding skills. Students who struggle with PA tend to also struggle to learn decoding skills. There have been hundreds of studies documenting the relationship between PA and reading skills early in elementary school and the expected correlation ranges from r=0.3 to r=0.5 depending on the details of the measure and the sample (Scarborough 1998; Swanson et al. 2003). ?tbl-pa-word-cor-table shows the correlation between ROAR-Phoneme and ROAR-Word for kindergarten through 12th grade. The correlation is right in the expected range providing additional evidence for the validity of ROAR-Phoneme as a measure of PA skills.\n\n\n\n\n\nGrade\nCorrelation between ROAR-Phoneme and ROAR-Word\nN\n\n\n\n\nKindergarten\n0.44\n254\n\n\n1\n0.53\n3369\n\n\n2\n0.50\n2014\n\n\n3\n0.41\n956\n\n\n4\n0.42\n588\n\n\n5\n0.38\n400\n\n\n6\n0.19\n1342\n\n\n7\n0.27\n927\n\n\n8\n0.23\n1442\n\n\n9\n0.32\n1948\n\n\n10\n0.25\n1756\n\n\n11\n0.33\n1325\n\n\n12\n0.42\n1053",
    "crumbs": [
      "Construct Validity: Evidence that ROAR subtests reliably measure the intended constructs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Phonological Awareness (ROAR-Phoneme) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-palabra.html",
    "href": "concurrent-validity-palabra.html",
    "title": "25  Spanish Single Word Recognition (ROAR-Palabra) Concurrent Validity",
    "section": "",
    "text": "25.1 Convergent validity with oral measures of single word reading",
    "crumbs": [
      "Construct Validity: ROAR-Español",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Spanish Single Word Recognition (ROAR-Palabra) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-palabra.html#sec-convergent-validity-palabra",
    "href": "concurrent-validity-palabra.html#sec-convergent-validity-palabra",
    "title": "25  Spanish Single Word Recognition (ROAR-Palabra) Concurrent Validity",
    "section": "",
    "text": "25.1.1 Woodock Muñoz\n\n\n\nTable 25.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.1: Correlations between ROAR-Palabra raw scores (theta) and Woodcock-Muñoz Basic Reading Skills raw scores.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.2: Correlations between ROAR-Palabra raw scores and Woodcock-Muñoz Letter-word Identification raw scores.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.3: Correlations between ROAR-Palabra Raw Scores and Woodcock-Muñoz Word Attack raw scores.\n\n\n\n\n\n\n\n\n25.1.2 Growth Over Time\nAnother source of validation is examining growth trajectories of ROAR-Palabra scores over time. Figure 25.4 shows how ROAR-Palabra score steadily increase in each grade.\n\n\n\n\n\n\n\n\nFigure 25.4: ROAR-Palabra ability estimates (theta) by grade",
    "crumbs": [
      "Construct Validity: ROAR-Español",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Spanish Single Word Recognition (ROAR-Palabra) Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "concurrent-validity-frase.html",
    "href": "concurrent-validity-frase.html",
    "title": "26  ROAR-Frase Concurrent Validity",
    "section": "",
    "text": "26.1 Convergent validity with [Insert name of WM measure]",
    "crumbs": [
      "Construct Validity: ROAR-Español",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ROAR-Frase Concurrent Validity</span>"
    ]
  },
  {
    "objectID": "validity-dyslexia-screening.html",
    "href": "validity-dyslexia-screening.html",
    "title": "27  Validity: Dyslexia Screening and Sub-typing",
    "section": "",
    "text": "27.1 Dyslexia screening based on foundational reading skills: Criterion validity\nTo assess sensitivity and specificity of ROAR Foundational Reading Skills (see Section 9.1) as an indicator of dyslexia risk, we ran two studies of criterion validity—one with a reading assessment that is among the most commonly used in schools, and one with the most widely-used measure in dyslexia research:\nCriterion validity",
    "crumbs": [
      "Criterion Validity: Evidence for ROAR as a dyslexia screener",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Validity: Dyslexia Screening and Sub-typing</span>"
    ]
  },
  {
    "objectID": "validity-dyslexia-screening.html#dyslexia-screening-based-on-foundational-reading-skills-criterion-validity",
    "href": "validity-dyslexia-screening.html#dyslexia-screening-based-on-foundational-reading-skills-criterion-validity",
    "title": "27  Validity: Dyslexia Screening and Sub-typing",
    "section": "",
    "text": "A study in collaboration with two, large and diverse California school districts that uses FAST™ earlyReading and FAST™ CBMreading risk categories as the criterion measures. FAST™ earlyReading and FAST™ CBMreading are individually administered screeners that classify students into three different risk levels for reading difficulties: “Low Risk”, “Some Risk”, and “High Risk”. For kindergarten we calculate prediction accuracy, sensitivity and specificity of ROAR Foundational Reading Skills relative to FAST™ earlyReading. For first grade, we calculate prediction accuracy, sensitivity and specificity of ROAR Foundational Reading Skills relative to FAST™ earlyReading and FAST™ CBMreading. For second grade we calculate prediction accuracy, sensitivity and specificity of ROAR Foundational Reading Skills relative to FAST™ CBMreading.\nA study with participants recruited from around the United States that uses the Woodcock Johnson Basic Reading Skills Composite Index (WJ BRS) as the criterion measure. WJ BRS is the most widely used measure in dyslexia research for identifying characteristics of dyslexia and is one of the most widely used measures in special education and clinical practice for diagnosing dyslexia. For this study of criterion validity, we use a threshold of the 25th percentile based on national norms to define students at risk or with indications of dyslexia and we calculate prediction accuracy, sensitivity and specificity of ROAR Foundational Reading Skills relative to this criterion.\n\n\n\n27.1.1 Criterion Validity Study 1: FastBridge\n\n27.1.1.1 Sample demographics\nThis study was carried out in collaboration with two California school districts. Demographics of the sample are provided in Table 22.4.\nTable 27.1 and Table 27.2 show the distribution of students in the sample across FAST™ earlyReading and FAST™ CBMreading risk categories. Note that FAST™ CBMreading categories of “College Pathway” and “Exceeding Expectations” have been included in the category “Low Risk” for the sake of this analysis.\n\n\n\n\nTable 27.1: Distributions of FAST™ earlyReading risk categories\n\n\n\n\n\n\nGrade\nEarly Reading Risk Level\nN\nProportion of Risk Level\n\n\n\n\nKindergarten\nHigh Risk\n36\n35.6%\n\n\nKindergarten\nSome Risk\n22\n21.8%\n\n\nKindergarten\nLow Risk\n43\n42.6%\n\n\n1\nHigh Risk\n222\n26%\n\n\n1\nSome Risk\n177\n20.8%\n\n\n1\nLow Risk\n454\n53.2%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 27.2: Distributions of FAST™ CBMreading risk categories\n\n\n\n\n\n\nGrade\nCBMreading Risk Level\nN\nProportion of Grade\n\n\n\n\n1\nHigh Risk\n201\n22.5%\n\n\n1\nSome Risk\n163\n18.3%\n\n\n1\nLow Risk\n528\n59.2%\n\n\n2\nHigh Risk\n187\n19.3%\n\n\n2\nSome Risk\n151\n15.6%\n\n\n2\nLow Risk\n633\n65.2%\n\n\n\n\n\n\n\n\n\n\n\n\n27.1.1.2 ROAR-Word\nSince dyslexia is identified based on persistent difficulties with word reading accuracy and fluency, word reading measures are generally the most efficient screeners though additional measures of Letter Sound Knowledge, Phonological Awareness, Rapid Automatized Naming and Visual Processing can also improve sensitivity/specificity, particularly for younger students at the early stages of learning to read. Thus, we begin by computing prediction accuracy, sensitivity and specificity for ROAR-Word. We then examine whether additional measures lead to more accurate predictions. Finally, we examine each additional measure in isolation.\nFigure 27.1 shows an ROC curve for kindergarten and 1st grade computed from a logistic regression model with ROAR-Word as a predictor of the FAST™ earlyReading “High Risk” category. Figure 27.2 shows and ROC curve for 1st and 2nd grades computed from a logistic regression model with ROAR-Word as a predictor of the FAST™ CBMreading “High Risk” category. All models in 1st and 2nd grade achieved exceptional accuracy with area under the curve (AUC) greater than 0.9 for both criterion measures. In kindergarten accuracy was lower, which is expected for a model that does not include other screening measures. Table 27.3 and Table 27.4 report sensitivity, specificity and accuracy by each demographic. Table 27.5 and Table 27.6 report sensitivity, specificity and accuracy by each demographic.\n\n\n\n\n\n\n\n\n\n\n\n(a) Kindergarten prediction of FAST™ earlyReading\n\n\n\n\n\n\n\n\n\n\n\n(b) 1st grade prediction of FAST™ earlyReading\n\n\n\n\n\n\n\nFigure 27.1: Prediction of FAST™ earlyReading risk categories based on a logistic regression model with ROAR-Word. Receiver Operating Characteristic (ROC) curves display sensitivity and specificity at different thresholds.\n\n\n\n\n\n\n\nTable 27.3: Kindergarten area under the curve, best sensitivity and specificity, and specificity when sensitivity is held closest to 0.9 for FastBridge Early Reading.\n\n\n\n\n\n\nDemographic Group\nGrade\nAUC\nBest Specificity\nBest Sensitivity\nSpecificity (Sensitivity at 0.9)\nSensitivity at 0.9\nN\n\n\n\n\nEnglish Learner\nKindergarten\n0.7142857\n0.7500000\n0.7619048\n0.2500000\n0.9047619\n25\n\n\nFemale\nKindergarten\n0.7701754\n0.7894737\n0.7666667\n0.5263158\n0.9333333\n49\n\n\nMale\nKindergarten\n0.7693452\n0.5833333\n0.9285714\n0.5416667\n0.9285714\n52\n\n\nWhite\nKindergarten\n0.7619048\n0.5925926\n0.9523810\n0.5925926\n0.9047619\n48\n\n\nHispanic Ethnicity\nKindergarten\n0.8072917\n0.6666667\n0.9687500\n0.6666667\n0.9062500\n38\n\n\nFree or Reduced Lunch\nKindergarten\n1.0000000\n1.0000000\n1.0000000\n1.0000000\n0.9200000\n27\n\n\nAll\nKindergarten\n0.7825235\n0.5681818\n0.9482759\n0.5681818\n0.9137931\n102\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 27.4: 1st grade area under the curve, best sensitivity and specificity, and specificity when sensitivity is held closest to 0.9 for FastBridge Early Reading.\n\n\n\n\n\n\nDemographic Group\nGrade\nAUC\nBest Specificity\nBest Sensitivity\nSpecificity (Sensitivity at 0.9)\nSensitivity at 0.9\nN\n\n\n\n\nEnglish Learner\n1\n0.8665179\n0.7500000\n0.8785714\n0.5714286\n0.9071429\n196\n\n\nFemale\n1\n0.9024567\n0.8090452\n0.8722222\n0.7487437\n0.9055556\n578\n\n\nMale\n1\n0.8899066\n0.6803069\n0.9348837\n0.6930946\n0.9023256\n606\n\n\nWhite\n1\n0.8441987\n0.6636364\n0.9279279\n0.6636364\n0.9009009\n441\n\n\nHispanic Ethnicity\n1\n0.8455505\n0.7750000\n0.7963801\n0.5833333\n0.9004525\n341\n\n\nBlack or African American\n1\n0.8194444\n0.6666667\n1.0000000\nNA\nNA\n20\n\n\nMultiracial\n1\n0.8390533\n0.7769231\n0.8461538\n0.5153846\n0.9230769\n143\n\n\nSPED\n1\n0.9466403\n1.0000000\n0.8260870\n0.7272727\n0.9130435\n57\n\n\nFree or Reduced Lunch\n1\n0.8842728\n0.9041096\n0.7213115\n0.6849315\n0.9016393\n256\n\n\nAll\n1\n0.8948850\n0.7698113\n0.8696742\n0.7207547\n0.9022556\n1194\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 27.5: 1st grade area under the curve, best sensitivity and specificity, and specificity when sensitivity is held closest to 0.9 for FastBridge CBM Reading.\n\n\n\n\n\n\nDemographic Group\nGrade\nAUC\nBest Specificity\nBest Sensitivity\nSpecificity (Sensitivity at 0.9)\nSensitivity at 0.9\nN\n\n\n\n\nEnglish Learner\n1\n0.8613520\n0.7321429\n0.8785714\n0.6428571\n0.9071429\n196\n\n\nFemale\n1\n0.9245473\n0.8096386\n0.9447853\n0.8216867\n0.9018405\n578\n\n\nMale\n1\n0.9207303\n0.8166259\n0.8934010\n0.7750611\n0.9035533\n606\n\n\nWhite\n1\n0.8937006\n0.7500000\n0.9381443\n0.7587209\n0.9072165\n441\n\n\nHispanic Ethnicity\n1\n0.8608712\n0.7539683\n0.8651163\n0.6746032\n0.9023256\n341\n\n\nAsian\n1\n0.9237395\n0.7285714\n1.0000000\n0.7285714\n0.9411765\n297\n\n\nMultiracial\n1\n0.8986711\n0.7984496\n1.0000000\n0.7984496\n0.9285714\n143\n\n\nSPED\n1\n0.9146341\n0.9375000\n0.9024390\n0.8125000\n0.9024390\n57\n\n\nFree or Reduced Lunch\n1\n0.8880505\n0.7532468\n0.8826816\n0.7012987\n0.9050279\n256\n\n\nAll\n1\n0.9212068\n0.7807229\n0.9368132\n0.8000000\n0.9010989\n1194\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 27.6: 2nd grade area under the curve, best sensitivity and specificity, and specificity when sensitivity is held closest to 0.9 for FastBridge CBM Reading.\n\n\n\n\n\n\nDemographic Group\nGrade\nAUC\nBest Specificity\nBest Sensitivity\nSpecificity (Sensitivity at 0.9)\nSensitivity at 0.9\nN\n\n\n\n\nEnglish Learner\n2\n0.8081314\n0.7297297\n0.8103448\n0.5270270\n0.9022989\n248\n\n\nFemale\n2\n0.9291882\n0.7743363\n0.9652778\n0.8119469\n0.9027778\n596\n\n\nMale\n2\n0.8991029\n0.8659091\n0.8000000\n0.7181818\n0.9052632\n630\n\n\nWhite\n2\n0.8852896\n0.7398374\n1.0000000\n0.7926829\n0.9062500\n278\n\n\nHispanic Ethnicity\n2\n0.8916598\n0.8717949\n0.8082192\n0.6752137\n0.9041096\n336\n\n\nAsian\n2\n0.8841069\n0.8354430\n0.8666667\n0.5443038\n0.9333333\n252\n\n\nMultiracial\n2\n0.8273299\n0.7735849\n0.9090909\n0.3836478\n0.9090909\n170\n\n\nSPED\n2\n0.9068323\n0.7826087\n0.9047619\n0.6521739\n0.9047619\n65\n\n\nFree or Reduced Lunch\n2\n0.9117904\n0.8953488\n0.8225806\n0.7558140\n0.9032258\n272\n\n\nAll\n2\n0.9116782\n0.8400000\n0.8550296\n0.7777778\n0.9023669\n1238\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 1st grade prediction of FAST™ CBMreading\n\n\n\n\n\n\n\n\n\n\n\n(b) 2nd grade prediction of FAST™ CBMreading\n\n\n\n\n\n\n\nFigure 27.2: Prediction of FAST™ CBMreading risk categories based on a logistic regression model with ROAR-Word. Receiver Operating Characteristic (ROC) curves display sensitivity and specificity at different thresholds.\n\n\n\n\n\n\n27.1.1.3 ROAR Foundational Reading Skills Composite\nWe next examine model accuracy based on a logistic regression model with all three ROAR measures of foundational reading skills: ROAR-Phoneme, ROAR-Letter and ROAR-Word. Because model accuracy was already near perfect for 1st and 2nd grade we would not expect a large improvement. However in kindergarten, when foundational reading skills are still being established, we expect measures of Phonological Awareness and Letter Sound knowledge to improve prediction accuracy. Figure 27.3 shows an ROC curve for the full model with all the ROAR measures (Phoneme, Letter, and Word) compared to models with each individual measure in kindergarten. ROAR-Letter and ROAR-Phoneme both achieved exceptional accuracy and the full model performed marginally better. Figure 27.4 shows ROC curves for the four models in 1st grade. In 1st grade ROAR-Word is the best single predictor and the full model (ROAR-Letter, ROAR-Phoneme, and ROAR-Word) performs marginally better.\n\n\n\n\n\n\n\n\nFigure 27.3: Prediction of FAST™ earlyReading risk categories in kindergarten based on a logistic regression model with ROAR-Letter, ROAR-Phoneme, and ROAR-Word. Receiver Operating Characteristic (ROC) curves display sensitivity and specificity at different thresholds. Full Model refers to the logistic regression with all three predictors and models of individual ROAR measures are shown for comparison.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 27.4: Prediction of FAST™ earlyReading risk categories in 1st grade based on a logistic regression model with ROAR-Letter, ROAR-Phoneme, and ROAR-Word. Receiver Operating Characteristic (ROC) curves display sensitivity and specificity at different thresholds. Full Model refers to the logistic regression with all three predictors and models of individual ROAR measures are shown for comparison.\n\n\n\n\n\n\n\n\n27.1.2 Criterion Validity Study 2: Woodcock Johnson Basic Reading Skills (WJ BRS)\n\n27.1.2.1 Sample demographics\nThis study included participants recruited from all around the United States for research studies in the Brain Development & Education Lab. Figure 22.3 shows the age distribution and Table 22.1 shows the demographics of the students that participated in this validation study.\nTable 27.7 shows the distribution of students in the sample across Woodcock Johnson Basic Reading Skills (BRS) risk categories. Note that the original risk categories for Woodcock Johnson BRS were not used, rather, we determined the three level risk categories. Low risk included students who were greater than the 50th percentile, some risk included students who were between the 25th and 50th percentiles, and high risk included students who were below the 25th percentile.\n\n\n\n\nTable 27.7: Distributions of Woodcock Johnson risk categories\n\n\n\n\n\n\nAge Range\nWJ Reading Risk\nN\nProportion of Risk Level\n\n\n\n\nK-2\nLow Risk\n203\n71.2%\n\n\nK-2\nSome Risk\n45\n15.8%\n\n\nK-2\nHigh Risk\n37\n13%\n\n\n3rd-5th\nLow Risk\n76\n40.6%\n\n\n3rd-5th\nSome Risk\n50\n26.7%\n\n\n3rd-5th\nHigh Risk\n61\n32.6%\n\n\n6th-8th\nLow Risk\n19\n35.2%\n\n\n6th-8th\nSome Risk\n12\n22.2%\n\n\n6th-8th\nHigh Risk\n23\n42.6%\n\n\n9th-12th\nLow Risk\n22\n32.4%\n\n\n9th-12th\nSome Risk\n22\n32.4%\n\n\n9th-12th\nHigh Risk\n24\n35.3%\n\n\n\n\n\n\n\n\n\n\n\n\n27.1.2.2 ROAR-Word\nFigure 27.5 shows an ROC curve for all grades (grouped by K-2, 3-5, 6-8, 9-12) computed from a logistic regression model with ROAR-Word as a predictor of the Woodcock Johnson Basic Reading Skills “Some Risk” category. Figure 27.6 shows an ROC curve for all grades (grouped by K-2, 3-5, 6-8, 9-12) computed from a logistic regression model with ROAR-Word as a predictor of the Woodcock Johnson Basic Reading Skills “High Risk” category. The model of grades K-2 achieved exceptional accuracy with area under the curve (AUC) equal to or greater than 0.9 for both criterion measures. For older grades accuracy was lower, and this reflects the psychometric properties of the criterion measure in older students. Most middle school and high school students are at the ceiling of the Woodcock Johnson Basic Reading Skills index (for example see Table 22.3 which shows the decline in reliability of WJ in older grades).\n\n\n\n\n\n\n\n\n\n\n\n(a) Kindergarten and 1st Grade prediction of Woodcock Johnson BRS ‘Some Risk’\n\n\n\n\n\n\n\n\n\n\n\n(b) 3rd-5th Grade prediction of Woodcock Johnson BRS ‘Some Risk’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 6th-8th Grade prediction of Woodcock Johnson BRS ‘Some Risk’\n\n\n\n\n\n\n\n\n\n\n\n(d) 9th-12th Grade prediction of Woodcock Johnson BRS ‘Some Risk’\n\n\n\n\n\n\n\nFigure 27.5: Prediction of Woodcock Johnson Basic Reading Skills ‘Some Risk’ category based on a logistic regression model with ROAR-Word. Receiver Operating Characteristic (ROC) curves display sensitivity and specificity at different thresholds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Kindergarten and 1st Grade prediction of Woodcock Johnson BRS ‘High Risk’\n\n\n\n\n\n\n\n\n\n\n\n(b) 3rd-5th Grade prediction of Woodcock Johnson BRS ‘High Risk’\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 6th-8th Grade prediction of Woodcock Johnson BRS ‘High Risk’\n\n\n\n\n\n\n\n\n\n\n\n(d) 9th-12th Grade prediction of Woodcock Johnson BRS ‘High Risk’\n\n\n\n\n\n\n\nFigure 27.6: Prediction of Woodcock Johnson Basic Reading Skills ‘High Risk’ category based on a logistic regression model with ROAR-Word. Receiver Operating Characteristic (ROC) curves display sensitivity and specificity at different thresholds.",
    "crumbs": [
      "Criterion Validity: Evidence for ROAR as a dyslexia screener",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Validity: Dyslexia Screening and Sub-typing</span>"
    ]
  },
  {
    "objectID": "predictive-validity.html",
    "href": "predictive-validity.html",
    "title": "28  Predictive Validity",
    "section": "",
    "text": "28.1 Background: Published studies\nPredictive validity of ROAR Foundational Reading Skills (see Section 9.1 for additional information on ROAR Foundational Reading Skills) was first reported by (Gijbels et al. 2024). Gijbels et al. (2024) examined the classification accuracy of ROAR Foundational Reading Skills administered in 1st grade for classifying students who were deemed “at risk” for reading difficulties based on the Fountas and Pinnell (F&P) Benchmark Assessment 8 months later in the fall of 2nd grade. This study included N=130 1st grade students from a public school in California. Students completed ROAR Foundational Reading Skills measures in their classroom and F&P Benchmark Assessments were administered by their classroom teachers. A Generalized Additive Model (GAM) (S. Wood and Wood 2015; S. N. Wood 2017) based on ROAR-Phoneme achieved an AUC=0.70, ROAR-Word achieved and AUC=0.83, and a GAM with ROAR-Phoneme and ROAR-Word achieved an AUC=0.84. The prediction accuracy of ROAR-Phoneme and ROAR-Word for reading skills assessed the following school year with individually-admininstered assessments demonstrated the promise of ROAR as a quick and automated screener.",
    "crumbs": [
      "Predictive Validity: Longitudinal evidence that ROAR predicts future reading development and dyslexia risk",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Predictive Validity</span>"
    ]
  },
  {
    "objectID": "predictive-validity.html#longitudinal-studies-of-grades-1-3",
    "href": "predictive-validity.html#longitudinal-studies-of-grades-1-3",
    "title": "28  Predictive Validity",
    "section": "28.2 Longitudinal studies of grades 1-3",
    "text": "28.2 Longitudinal studies of grades 1-3\nWe ran 2 additional studies to assess the predictive validity of ROAR Foundational Reading Skills\n\n2 year longitudinal study of predictive validity: In a large California school district, all the 1st grade classrooms were administered ROAR Foundational Reading Skills measures three times per year and were followed longitudinally for 2 years. In the fall of 3rd grade, each student was individually administered the Woodcock Johnson Basic Reading Skills (WJ BRS) Composite Index. Based on this criterion measure, we assessed sensitivity and specificity of ROAR at each timepoint for predicting students who were classified as struggling readers with indications of dyslexia. Additionally we report prediction accuracy based on BRS as a continuous measure.\nFall to spring prediction in 1st, 2nd, and 3rd grade: In a second study we assessed predictive validity of ROAR Foundational Reading Skills measures admininstered in the Fall and Winter for predicting individually administered FAST™ earlyReading and FAST™ CBMreading in the Spring (for concurrent validity of ROAR Spring assessment see Chapter 27).\n\nFor each study we report Area Under the Curve (AUC), Sensitivity, and Specificity as measures of classification accuracy and Pearson’s \\(\\rho\\) as a measure of prediction accuracy for continuous criterion measures.\n\n28.2.1 Study 1: 2 year longitudinal study with Woodcock Johnsons Basic Reading Skills (BRS) as the criterion\nWe implemented our ROAR measures beginning in the first grade. As shown in Table 28.1, ROAR-Word admininstered in first grade consistently predicts third-grade WJ-BRS outcomes.The correlation between ROAR-Word and WJ-BRS strengthens over time. Table 28.2 demonstrates that ROAR measures can predict reading fluency as early as the first grade, with ROAR-Sentence being the most relevant predictor.\n\n\n\n\nTable 28.1: Predictive validity between ROAR measures and WJ BRS\n\n\n\n\n\n\nROAR Measure\nROAR Administration\nN\nCorrelation\n\n\n\n\nROAR Word\nFall 2021\n120\n0.685\n\n\nROAR Word\nSpring 2022\n120\n0.632\n\n\nROAR Word\nFall 2022\n125\n0.629\n\n\nROAR Word\nSpring 2023\n141\n0.738\n\n\nROAR Word\nFall 2023\n165\n0.744\n\n\nROAR Phoneme\nSpring 2022\n74\n0.408\n\n\nROAR Phoneme\nFall 2022\n117\n0.539\n\n\nROAR Phoneme\nSpring 2023\n133\n0.557\n\n\nROAR Phoneme\nFall 2023\n166\n0.526\n\n\nROAR Sentence\nSpring 2023\n125\n0.715\n\n\nROAR Sentence\nFall 2023\n164\n0.697\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 28.2: Predictive validity between ROAR measures and WJ Sentence Fluency\n\n\n\n\n\n\nROAR Measure\nROAR Administration\nN\nCorrelation\n\n\n\n\nROAR-Word\nFall 2021\n120\n0.703\n\n\nROAR-Word\nSpring 2022\n120\n0.733\n\n\nROAR-Word\nFall 2022\n125\n0.672\n\n\nROAR-Word\nSpring 2023\n141\n0.713\n\n\nROAR-Word\nFall 2023\n165\n0.737\n\n\nROAR-Phoneme\nSpring 2022\n74\n0.336\n\n\nROAR-Phoneme\nFall 2022\n117\n0.486\n\n\nROAR-Phoneme\nSpring 2023\n133\n0.574\n\n\nROAR-Phoneme\nFall 2023\n166\n0.513\n\n\nROAR-Sentence\nSpring 2023\n125\n0.817\n\n\nROAR-Sentence\nFall 2023\n164\n0.831\n\n\n\n\n\n\n\n\n\n\nBased on the WJ-BRS, 32 out of 170 students were identified as high-risk or at-risk struggling readers (scoring below the 50th percentile of the WJ-BRS norms). We treated this classification as the true score. Next, we examined the prediction accuracy of a logistic regression model using ROAR measures taken in the previous year. Figure 28.1 provides further evidence supporting the high sensitivity and specificity of ROAR-Word in predicting dyslexia classification with a lead time of two years.\n\n\n\n\n\n\n\n\nFigure 28.1: Prediction of Woodcock Johnsons Basic Reading Skills (BRS) risk categories based on a logistic regression model with ROAR measures in previous timepoints.\n\n\n\n\n\n\n\n28.2.2 Study 2: Fall to Spring prediction of FAST™ earlyReading and FAST™ CBMreading\nTable 28.3 demonstrates that ROAR-Word in the Fall, among ROAR measures, is the strongest predictor of FAST™ CBMreading performance in the Spring for 1st graders. For 2nd and 3rd graders, both ROAR-Word and ROAR-Sentence are strong predictors of FAST™ CBMreading outcomes. Additionally, Table 28.4 provides further evidence that ROAR-Word in the Fall is a robust predictor of FAST™ earlyReading performance in the Spring.\n\n\n\n\nTable 28.3: Predictive validity between ROAR measures and FAST™ CBMreading\n\n\n\n\n\n\nGrade\nROAR Measure\nROAR Administration\nN\nCorrelation\n\n\n\n\n1\nROAR-Word\nFall 2023\n313\n0.725\n\n\n1\nROAR-Word\n2024 Winter\n336\n0.782\n\n\n1\nROAR-Word\nSpring 2024\n306\n0.777\n\n\n1\nROAR-Phoneme\nFall 2023\n305\n0.589\n\n\n1\nROAR-Phoneme\n2024 Winter\n352\n0.647\n\n\n1\nROAR-Phoneme\nSpring 2024\n332\n0.604\n\n\n1\nROAR-Sentence\nFall 2023\n263\n0.647\n\n\n1\nROAR-Sentence\n2024 Winter\n345\n0.791\n\n\n1\nROAR-Sentence\nSpring 2024\n307\n0.796\n\n\n2\nROAR-Word\nFall 2023\n342\n0.748\n\n\n2\nROAR-Word\n2024 Winter\n338\n0.705\n\n\n2\nROAR-Word\nSpring 2024\n319\n0.678\n\n\n2\nROAR-Phoneme\nFall 2023\n350\n0.500\n\n\n2\nROAR-Phoneme\n2024 Winter\n150\n0.404\n\n\n2\nROAR-Sentence\nFall 2023\n333\n0.765\n\n\n2\nROAR-Sentence\n2024 Winter\n330\n0.784\n\n\n2\nROAR-Sentence\nSpring 2024\n322\n0.780\n\n\n3\nROAR-Word\nFall 2023\n192\n0.577\n\n\n3\nROAR-Word\n2024 Winter\n163\n0.583\n\n\n3\nROAR-Word\nSpring 2024\n150\n0.590\n\n\n3\nROAR-Phoneme\nFall 2023\n193\n0.363\n\n\n3\nROAR-Phoneme\n2024 Winter\n99\n0.399\n\n\n3\nROAR-Sentence\nFall 2023\n190\n0.587\n\n\n3\nROAR-Sentence\n2024 Winter\n163\n0.600\n\n\n3\nROAR-Sentence\nSpring 2024\n149\n0.594\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 28.4: Predictive validity between ROAR measures and FAST™ earlyReading\n\n\n\n\n\n\nGrade\nROAR Measure\nROAR Administration\nN\nCorrelation\n\n\n\n\n1\nROAR-Word\nFall 2023\n313\n0.725\n\n\n1\nROAR-Word\n2024 Winter\n335\n0.780\n\n\n1\nROAR-Word\nSpring 2024\n306\n0.777\n\n\n1\nROAR-Phoneme\nFall 2023\n305\n0.589\n\n\n1\nROAR-Phoneme\n2024 Winter\n351\n0.645\n\n\n1\nROAR-Phoneme\nSpring 2024\n331\n0.601\n\n\n1\nROAR-Sentence\nFall 2023\n263\n0.647\n\n\n1\nROAR-Sentence\n2024 Winter\n345\n0.791\n\n\n1\nROAR-Sentence\nSpring 2024\n307\n0.796\n\n\n\n\n\n\n\n\n\n\nWe examined the prediction accuracy of a logistic regression model using ROAR measures from Fall 2023 to predict the FAST™ classification (low risk vs. some risk and high risk) in Spring 2024. Figure 28.2 provides evidence supporting the high sensitivity and specificity of ROAR-Word in predicting dyslexia classification in both 1st and 2nd grades. Additionally, ROAR-Phoneme is more useful in 1st grade and ROAR-Sentence proves to be more useful in 2nd grade.\n\n\n\n\n\n\n\n\n\n\n\n(a) 1st grade prediction of FAST™ CBMreading\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) 2nd grade prediction of FAST™ CBMreading\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 1st grade prediction of FAST™ earlyReading\n\n\n\n\n\n\n\nFigure 28.2: Predictive validity between ROAR measures in the Fall and FAST™ measures in the Spring",
    "crumbs": [
      "Predictive Validity: Longitudinal evidence that ROAR predicts future reading development and dyslexia risk",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Predictive Validity</span>"
    ]
  },
  {
    "objectID": "predictions-RVP.html",
    "href": "predictions-RVP.html",
    "title": "29  Predictive validity of ROAR-RVP",
    "section": "",
    "text": "29.1 Background: Published studies\nThe correlation between rapid visual processing and reading outcomes were first reported in a large cross-sectional sample by (Ramamurthy, White, and Yeatman 2023). Reading challenges are not primarily an issue with visual processing, though there is strong evidence that visual processing differences contribute to and/or exacerbate reading difficulties. Figure 29.1 shows published findings from Ramamurthy, White, and Yeatman (2023) demonstrating the relationship between RVP and reading ability is participants ages 6-18.",
    "crumbs": [
      "Predictive Validity: Longitudinal evidence that ROAR predicts future reading development and dyslexia risk",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Predictive validity of ROAR-RVP</span>"
    ]
  },
  {
    "objectID": "predictions-RVP.html#sec-rvp-published",
    "href": "predictions-RVP.html#sec-rvp-published",
    "title": "29  Predictive validity of ROAR-RVP",
    "section": "",
    "text": "Figure 29.1: Differences in rapid visual processing predict reading ability across a broad aage range spanning kindergarten through adulthood. The x-axis show Woodcock Johnson Basic Reading Skills Standard Scores and the Y-axis shows age-standardized performance on the Rapid Visual Processing Task.",
    "crumbs": [
      "Predictive Validity: Longitudinal evidence that ROAR predicts future reading development and dyslexia risk",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Predictive validity of ROAR-RVP</span>"
    ]
  },
  {
    "objectID": "predictions-RVP.html#correlating-reading-outcomes-with-concurrent-rvp-measures",
    "href": "predictions-RVP.html#correlating-reading-outcomes-with-concurrent-rvp-measures",
    "title": "29  Predictive validity of ROAR-RVP",
    "section": "29.2 Correlating Reading Outcomes with Concurrent RVP Measures",
    "text": "29.2 Correlating Reading Outcomes with Concurrent RVP Measures\nIn the calibration sample of n=175 kindergarten and first grade children (see Section 19.2.2.4) we replicated the correlation strength reported in the cross-sectional sample in the calibration cohort of kindergarten and first grade children (Ramamurthy et al., n.d.). Figure 29.2 shows the correlation between task performance in the Rapid Visual Processing and various reading outcomes.\n\n\n\n\n\n\nFigure 29.2: ROAR-RVP correlates with standardized reading outcome measures. Panel a shows the correlation with KTEA standard scores and panel b shows the correlation with Woodcock Johnson Basic Reading Skills/ Both correlations replicate previously published effect sizes (e.g., Section 29.1).",
    "crumbs": [
      "Predictive Validity: Longitudinal evidence that ROAR predicts future reading development and dyslexia risk",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Predictive validity of ROAR-RVP</span>"
    ]
  },
  {
    "objectID": "predictions-RVP.html#winter-to-spring-predictions-rvp-measured-in-the-winter-predicts-end-of-year-reading-scores",
    "href": "predictions-RVP.html#winter-to-spring-predictions-rvp-measured-in-the-winter-predicts-end-of-year-reading-scores",
    "title": "29  Predictive validity of ROAR-RVP",
    "section": "29.3 Winter to spring predictions: RVP measured in the winter predicts end of year reading scores",
    "text": "29.3 Winter to spring predictions: RVP measured in the winter predicts end of year reading scores\nIn Winter 2023, 755 participants completed the RVP tasks and in Spring 2023 these students were administered Kaufman Test of Educational Achievement (KTEA) reading composite measure. The correlation strength between KTEA and task performance, in both the Letter (RVPL) [r = 0.42, p &lt; 2.2x10-16; CI: 0.359 - 0.477] and pseudo-letter (RVPS) [r = 0.37, p &lt; 2.2x10-16; CI: 0.307 - 0.431] versions were similar [∆r: 0.0489; z = 1.9166, p = 0.0553; computed using the cocor package in R (Diedenhofen and Musch 2015) that uses Hittner, May, and Silver’s (2003) modification of Dunn and Clark’s z (1969) using a back transformed average Fisher’s (1921) Z procedure]. Figure 29.3 shows the predictions of end of year reading scores. The correlation strengths were also comparable to our calibration data set and the previous cross-sectional study reported above in Figure 29.2.\n\n\n\n\n\n\nFigure 29.3: ROAR-RVP measured in the Winter predicts end of year reading assessments. The effect size is consistent with published studies (e.g., Section 29.1).\n\n\n\nA regression model with both letter and pseudo-letter scores predicted KTEA better than either measure on its own [Model A. Performance in the RVPL task explained 17.5% of variance in KTEA composite scores; Model B. performance in the RVPS task explained 13.7% of variance in KTEA composite scores; and Model C. with both RVPL and RVPS (KTEA ~ letters + pseudo-letters) as predictors explained 18.6% of variance in KTEA composite scores. Model C is significantly better than either models: Models A vs C: F(1,752) = 10.609; p =  0.001; and Models B vs C: F(1,752) = 46.373, p = 2.002x10-11].",
    "crumbs": [
      "Predictive Validity: Longitudinal evidence that ROAR predicts future reading development and dyslexia risk",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Predictive validity of ROAR-RVP</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allen, Mary J, and Wendy M Yen. 2001. Introduction to Measurement\nTheory. Waveland Press.\n\n\nAnthony, Jason L, and David J Francis. 2005. “Development of\nPhonological Awareness.” Curr. Dir. Psychol. Sci. 14\n(5): 255–59.\n\n\nAnthony, Jason L, and Christopher J Lonigan. 2004. “The Nature of\nPhonological Awareness: Converging Evidence from Four Studies of\nPreschool and Early Grade School Children.” Journal of\nEducational Psychology 96 (1): 43.\n\n\nAnthony, Jason L, Emily J Solari, Jeffrey M Williams, Kimberly D\nSchoger, Zhou Zhang, Lee Branum-Martin, and David J Francis. 2009.\n“Development of Bilingual Phonological Awareness in\nSpanish-Speaking English Language Learners: The Roles of Vocabulary,\nLetter Knowledge, and Prior Phonological Awareness.”\nScientific Studies of Reading 13 (6): 535–64.\n\n\nAnthony, Jason L, Jeffrey M Williams, Lillian K Durán, Sandra Laing\nGillam, Lan Liang, Rachel Aghara, Paul R Swank, Mike A Assel, and Susan\nH Landry. 2011. “Spanish Phonological Awareness: Dimensionality\nand Sequence of Development During the Preschool and Kindergarten\nYears.” Journal of Educational Psychology 103 (4): 857.\n\n\nAnthony, Jason L, Jeffrey M Williams, Renee McDonald, and David J\nFrancis. 2007. “Phonological Processing and Emergent Literacy in\nYounger and Older Preschool Children.” Annals of\nDyslexia 57: 113–37.\n\n\nBalota, David A, Melvin J Yap, and Michael J Cortese. 2006.\n“Visual Word Recognition.” In Handbook of\nPsycholinguistics, 285–375. Elsevier.\n\n\nBalota, David A, Melvin J Yap, Keith A Hutchison, Michael J Cortese,\nBrett Kessler, Bjorn Loftis, James H Neely, Douglas L Nelson, Greg B\nSimpson, and Rebecca Treiman. 2007. “The English Lexicon\nProject.” Behavior Research Methods 39: 445–59.\n\n\nBarrington, Elizabeth, Sadie Mae Sarkisian, Heidi M Feldman, and Jason D\nYeatman. 2023. “Rapid Online Assessment of Reading\n(ROAR): Evaluation of an Online Tool for Screening Reading\nSkills in a Developmental-Behavioral Pediatrics\nClinic.” J. Dev. Behav. Pediatr. 44 (9): e604–10.\n\n\nBergen, Elsje van, Aryan van der Leij, and Peter F de Jong. 2014.\n“The Intergenerational Multiple Deficit Model and the Case of\nDyslexia.” Frontiers in Human Neuroscience 8: 346.\n\n\nBhat, Kruttika G., Alexa Mogan, Ana Saavedra, Mia Fuentes-Jimenez,\nJulian M. Siebert, Wanjing Anya Ma, Carrie Townley-Flores, et al. 2024.\n“Shared and Unique Influences of Phonological\nProcessing on Reading and Math.”\n\n\nBlachman, Benita A. 2013. Foundations of Reading Acquisition and\nDyslexia: Implications for Early Intervention. Routledge.\n\n\nBlumenfeld, Henrike K, S C Bobb, and Viorica Marian. 2016. “The role of language proficiency, cognate status and word\nfrequency in the assessment of Spanish–English bilinguals’ verbal\nfluency.” International Journal of Speech-Language\nPathology 18 (2): 190–201.\n\n\nBoets, Bart, Jan Wouters, Astrid van Wieringen, Bert De Smedt, and Pol\nGhesquière. 2008. “Modelling Relations Between Sensory Processing,\nSpeech Perception, Orthographic and Phonological Ability, and Literacy\nAchievement.” Brain Lang. 106 (1): 29–40.\n\n\nBon, Wim H J van, Paula H Tooren, and Kees W J M van Eekelen. 2000.\n“Lexical Decision and Oral Reading by Poor and Normal\nReaders.” Eur. J. Psychol. Educ. 15 (3): 259–70.\n\n\nBosse, Marie Line, Marie Josèphe Tainturier, and Sylviane Valdois. 2007.\n“Developmental dyslexia: The visual attention\nspan deficit hypothesis.” Cognition 104 (2):\n198–230.\n\n\nBosse, Marie-Line, and Sylviane Valdois. 2009. “Influence of the\nVisual Attention Span on Child Reading Performance: A Cross-Sectional\nStudy.” J. Res. Read. 32 (2): 230–53.\n\n\nBradley L, and Bryant P E. 1983. “Categorizing sounds and learning to read – a causal\nconnection.” Nature 301 (3): 419–21.\n\n\nBrown, Megan C, Daragh E Sibley, Julie A Washington, Timothy T Rogers,\nJan R Edwards, Maryellen C MacDonald, and Mark S Seidenberg. 2015.\n“Impact of Dialect Use on a Basic Component of Learning to\nRead.” Front. Psychol. 6 (March): 196.\n\n\nCatts, Hugh W, Nicole Patton Terry, Christopher J Lonigan, Donald L\nCompton, Richard K Wagner, Laura M Steacy, Kelly Farquharson, and Yaacov\nPetscher. 2024. “Revisiting the Definition of Dyslexia.”\nAnn. Dyslexia, January.\n\n\nChrist, and Theodore Colleagues. 2018. Formative Assessment System\nfor Teachers™ Technical Manual. Author; FastBridge Learning.\n\n\nCompton, Donald L. 2021. “Focusing Our View of Dyslexia Through a\nMultifactorial Lens: A Commentary.” Learning Disability\nQuarterly 44 (3): 225–30.\n\n\nCompton, Donald L, John C DeFries, and Richard K Olson. 2001. “Are\nRAN-and Phonological Awareness-Deficits Additive in Children with\nReading Disabilities?” Dyslexia 7 (3): 125–49.\n\n\nCummings, Kelli D, Yonghan Park, and Holle A Bauer Schaper. 2013.\n“Form Effects on DIBELS Next Oral Reading Fluency\nProgress-Monitoring Passages.” Assessment for Effective\nIntervention 38 (2): 91–104.\n\n\nDenckla, Martha Bridge, and Laurie E Cutting. 1999. “History and\nSignificance of Rapid Automatized Naming.” Annals of\nDyslexia 49: 29–42.\n\n\nDenckla, Martha Bridge, and Rita G Rudel. 1976. “Rapid\n‘Automatized’naming (RAN): Dyslexia Differentiated from Other Learning\nDisabilities.” Neuropsychologia 14 (4): 471–79.\n\n\nDenton, Carolyn A, Amy E Barth, Jack M Fletcher, Jade Wexler, Sharon\nVaughn, Paul T Cirino, Melissa Romain, and David J Francis. 2011.\n“The Relations Among Oral and Silent Reading Fluency and\nComprehension in Middle School: Implications for Identification and\nInstruction of Students with Reading Difficulties.” Sci.\nStud. Read. 15 (2): 109–35.\n\n\nDiedenhofen, Birk, and Jochen Musch. 2015. “Cocor: A Comprehensive\nSolution for the Statistical Comparison of Correlations.”\nPLoS One 10 (3): e0121945.\n\n\nDomingue, Benjamin W, Heather J Hough, David Lang, and Jason Yeatman.\n2021. “Changing Patterns of Growth in Oral Reading Fluency During\nthe COVID-19 Pandemic. Working Paper.” Policy Analysis for\nCalifornia Education, PACE.\n\n\nEhri, Linnea C. 2005. “Learning to Read Words: Theory, Findings,\nand Issues.” Sci. Stud. Read. 9 (2): 167–88.\n\n\nElliott, Julian G, and Elena L Grigorenko. 2024b. “Dyslexia in the\nTwenty-First Century: A Commentary on the IDA Definition of\nDyslexia.” Ann. Dyslexia, June.\n\n\n———. 2024a. “Dyslexia in the Twenty-First Century: A Commentary on\nthe IDA Definition of Dyslexia.” Ann.\nDyslexia, June.\n\n\nFletcher, J M, G R Lyon, L S Fuchs, and M A Barnes. 2006. Learning disabilities: From identification to\nintervention. New York: Guilford PRess.\n\n\nFuchs, Lynn S, Douglas Fuchs, Michelle K Hosp, and Joseph R Jenkins.\n2001. “Oral Reading Fluency as an Indicator of Reading Competence:\nA Theoretical, Empirical, and Historical Analysis.” Sci.\nStud. Read. 5 (3): 239–56.\n\n\nGaab, Nadine, and Yaacov Petscher. 2022. “Screening for Early\nLiteracy Milestones and Reading Disabilities: The Why, When, Whom, How,\nand Where.” Perspectives on Language and Literacy 48\n(1): 11–18.\n\n\nGijbels, Liesbeth, Amy Burkhardt, Wanjing Anya Ma, and Jason D Yeatman.\n2024. “Rapid Online Assessment of Reading and Phonological\nAwareness (ROAR-PA).” Sci. Rep. 14 (1):\n1–16.\n\n\nGood, Roland H, Jerry Gruba, and Ruth A Kaminski. 2002. “Best\nPractices in Using Dynamic Indicators of Basic Early Literacy Skills\n(DIBELS) in an Outcomes-Driven Model.”\n\n\nGrosjean, François. 1989. “Neurolinguists,\nbeware! The bilingual is not two monolinguals in one\nperson.” Brain and Language 36 (1): 3–15. https://doi.org/10.1016/0093-934x(89)90048-5.\n\n\n———. 2008. Studying Bilinguals. Oxford University\nPress. Oxford University Press.\n\n\nHambleton, Ronald K, and Anil Kanjee. 1995. “Increasing the\nValidity of Cross-Cultural Assessments: Use of Improved Methods for Test\nAdaptations.” European Journal of Psychological\nAssessment 11 (3): 147–57.\n\n\nHelman, Lori A. 2004. “Building on the Sound System of Spanish:\nInsights from the Alphabetic Spellings of English-Language\nLearners.” The Reading Teacher 57 (5): 452–60.\n\n\nHoffman, Amy R, Jeanne E Jenkins, and S Kay Dunlap. 2009. “Using\nDIBELS: A Survey of Purposes and Practices.” Reading\nPsychology 30 (1): 1–16.\n\n\nHoover, Wesley A, and Philip B Gough. 1990. “The Simple View of\nReading.” Read. Writ. 2 (2): 127–60.\n\n\nHuang, Chen, Ningyu Liu, and Jing Zhao. 2021. “Different\nPredictive Roles of Phonological Awareness and Visual Attention Span for\nEarly Character Reading Fluency in Chinese.” J. Gen.\nPsychol. 148 (1): 45–66.\n\n\nHudson, Roxanne F, Paige C Pullen, Holly B Lane, and Joseph K Torgesen.\n2008. “The Complex Nature of Reading Fluency: A Multidimensional\nView.” Reading & Writing Quarterly 25 (1): 4–32.\n\n\nJiménez González, JE, and Maria del Rosario Ortiz Gonzalez. 1994.\n“Phonological Awareness in Learning Literacy.”\nIntellectica 18 (1): 155–81.\n\n\nJohnson, Evelyn S, Juli L Pool, and Deborah R Carter. 2011.\n“Validity Evidence for the Test of Silent Reading Efficiency and\nComprehension (TOSREC).” Assess. Eff.\nInterv. 37 (1): 50–57.\n\n\nKang, Eun Young, and Mikyung Shin. 2019. “The Contributions of\nReading Fluency and Decoding to Reading Comprehension for Struggling\nReaders in Fourth Grade.” Read. Writ. Q. 35 (3): 179–92.\n\n\nKatz, Leonard, Larry Brancazio, Julia Irwin, Stephen Katz, James\nMagnuson, and D H Whalen. 2012. “What lexical\ndecision and naming tell us about reading.” Read.\nWrit. 25 (6): 1259–82.\n\n\nKeuleers, Emmanuel, and Marc Brysbaert. 2010. “Wuggy: A multilingual pseudoword\ngenerator.” Behavior Research Methods 42 (3):\n627–33. https://doi.org/10.3758/brm.42.3.627.\n\n\nKeuleers, Emmanuel, Paula Lacey, Kathleen Rastle, and Marc Brysbaert.\n2012. “The British Lexicon Project: Lexical Decision Data for\n28,730 Monosyllabic and Disyllabic English Words.” Behav.\nRes. Methods 44 (1): 287–304.\n\n\nKim, Young-Suk Grace, Chea Hyeong Park, and Richard K Wagner. 2014.\n“Is Oral/Text Reading Fluency a ‘Bridge’ to Reading\nComprehension?” Read. Writ. 27 (1): 79–99.\n\n\nKim, Young-Suk, Richard K Wagner, and Danielle Lopez. 2012.\n“Developmental Relations Between Reading Fluency and Reading\nComprehension: A Longitudinal Study from Grade 1 to Grade 2.”\nJ. Exp. Child Psychol. 113 (1): 93–111.\n\n\nKremin, Lena V, Maria M Arredondo, Lucy Shih-Ju Hsu, Teresa Satterfield,\nand Ioulia Kovelman. 2019. “The Effects of Spanish Heritage\nLanguage Literacy on English Reading for Spanish–English Bilingual\nChildren in the US.” International Journal of Bilingual\nEducation and Bilingualism 22 (2): 192–206.\n\n\nLi, Hsin-Hung, Nina M Hanning, and Marisa Carrasco. 2021. “To Look\nor Not to Look: Dissociating Presaccadic and Covert Spatial\nAttention.” Trends Neurosci. 44 (8): 669–86.\n\n\nLinden, Wim J van der. 2000. Computerized Adaptive Testing: Theory\nand Practice. Edited by Cees A W Glas. Dordrecht, Netherlands:\nKluwer Academic.\n\n\nLindsey, Kim A, Franklin R Manis, and Caroline E Bailey. 2003.\n“Prediction of First-Grade Reading in Spanish-Speaking\nEnglish-Language Learners.” Journal of Educational\nPsychology 95 (3): 482.\n\n\nLobier, Muriel, and Sylviane Valdois. 2015. “Visual attention deficits in developmental dyslexia\ncannot be ascribed solely to poor reading experience.”\nNat. Rev. Neurosci. 16 (4): 1.\n\n\nLobier, Muriel, Rachel Zoubrinetzky, and Sylviane Valdois. 2012.\n“The visual attention span deficit in\ndyslexia is visual and not verbal.” Cortex 48\n(6): 768–73.\n\n\nLovett, Maureen W, Jan C Frijters, Maryanne Wolf, Karen A Steinbach,\nRose A Sevcik, and Robin D Morris. 2017. “Early Intervention for\nChildren at Risk for Reading Disabilities: The Impact of Grade at\nIntervention and Individual Differences on Intervention\nOutcomes.” Journal of Educational Psychology 109 (7):\n889.\n\n\nLyon, G Reid, Sally E Shaywitz, and Bennett A Shaywitz. 2003a. “A\nDefinition of Dyslexia.” Annals of Dyslexia 53: 1–14.\n\n\n———. 2003b. “A Definition of Dyslexia.” Ann.\nDyslexia 53 (1): 1–14.\n\n\nMa, Wanjing A, Adam Richie-Halford, Amy Burkhardt, Klint Kanopka,\nClementine Chou, Benjamin Domingue, and Jason D Yeatman. 2023.\n“ROAR-CAT: Rapid Online Assessment of Reading Ability\nwith Computerized Adaptive Testing.”\n\n\nManis, Franklin R, Kim A Lindsey, and Caroline E Bailey. 2004.\n“Development of Reading in Grades k–2 in Spanish–Speaking\nEnglish–Language Learners.” Learning Disabilities Research\n& Practice 19 (4): 214–24.\n\n\nMı́guez-Álvarez, Carla, Miguel Cuevas-Alonso, and Ángeles Saavedra. 2022.\n“Relationships Between Phonological Awareness and Reading in\nSpanish: A Meta-Analysis.” Language Learning 72 (1):\n113–57.\n\n\nMoyle, Maura Jones, John Heilmann, and S Sue Berman. 2013.\n“Assessment of Early Developing Phonological Awareness Skills: A\nComparison of the Preschool Individual Growth and Development Indicators\nand the Phonological Awareness and Literacy Screening–PreK.”\nEarly Educ. Dev. 24 (5): 668–86.\n\n\nNittrouer, Susan, Michael Studdert-Kennedy, and Richard S McGowan. 1989.\n“The Emergence of Phonetic Segments: Evidence from the Spectral\nStructure of Fricative-Vowel Syllables Spoken by Children and\nAdults.” J. Speech Lang. Hear. Res. 32 (1): 120–32.\n\n\nO’Brien, Gabrielle, and Jason D Yeatman. 2021. “Bridging Sensory\nand Language Theories of Dyslexia: Toward a Multifactorial\nModel.” Dev. Sci. 24 (3): e13039.\n\n\nPennington, Bruce F. 2006. “From Single to Multiple Deficit Models\nof Developmental Disorders.” Cognition 101 (2): 385–413.\n\n\nPennington, Rob, and Robert E Pennington. 2011. Find the Upside of\nthe down Times: How to Turn Your Worst Experience. Resource\nInternational.\n\n\nPerfetti, Charles A. 1985. “Reading Ability” 282.\n\n\nPhilip Chalmers, R. 2012. “Mirt: A Multidimensional Item Response\nTheory Package for the R Environment.” J. Stat.\nSoftw. 48 (May): 1–29.\n\n\nPikulski, J J, and D J Chard. 2005. “Fluency: Bridge Between\nDecoding and Reading Comprehension.” Read. Teach.\n\n\nPrice, Katherine W, Elizabeth B Meisinger, Max M Louwerse, and Sidney\nD’Mello. 2016. “The Contributions of Oral and Silent Reading\nFluency to Reading Comprehension.” Read. Psychol. 37\n(2): 167–201.\n\n\nRamamurthy, Mahalakshmi, Klint Kanopka, Adam Richie-Halford, Benjamin W\nDomingue, Francesca Pei, Phaedra Bell, Lucy Yan, Andrea Hartsough, Maria\nLuisa Gorno-Tempini, and Jason D Yeatman. n.d. “Design and\nValidation of a Rapid Visual Processing Measure for Screening Reading\nDifficulties in Early Childhood.”\n\n\nRamamurthy, Mahalakshmi, Klint Kanopka, Adam Richie-Halford, Benjamin\nDomingue, Francesca Pei, Phaedra Bell, Lucy Yan, Andrea Hartsough, Maria\nL Gorno-Tempini, and Jason D Yeatman. 2024. “Design and Validation\nof a Rapid Visual Processing Measure for Screening Reading Difficulties\nin Early Childhood,” February.\n\n\nRamamurthy, Mahalakshmi, Alex L White, and Jason D Yeatman. 2023a.\n“Children with Dyslexia Show No Deficit in Exogenous Spatial\nAttention but Show Differences in Visual Encoding.” Dev.\nSci., November, e13458.\n\n\nRamamurthy, Mahalakshmi, Alex White, and Jason D Yeatman. 2023b.\n“Children with Dyslexia Show No Deficit in Exogenous Spatial\nAttention but Show Differences in Visual Encoding.”\n\n\nRatcliff, Roger, Gail McKoon, and Pablo Gomez. 2004. “A Diffusion Model Account of the Lexical Decision\nTask.” Psychol. Rev. 111 (1): 159–82.\n\n\nRosario Basterra, Marı́a del, Elise Trumbull, and Guillermo\nSolano-Flores. 2011. Cultural Validity in Assessment. Routledge\nNew York, NY.\n\n\nSabatini, John, Jonathan Weeks, Tenaha O’Reilly, Kelly Bruce, Jonathan\nSteinberg, and Szu-Fu Chao. 2019. “SARA Reading\nComponents Tests, RISE Forms: Technical Adequacy and Test\nDesign, 3rd Edition.” ETS Res. Rep. Ser.\n2019 (1): 1–30.\n\n\nSaksida, Amanda, Stéphanie Iannuzzi, Caroline Bogliotti, Yves Chaix,\nJean François Démonet, Laure Bricout, Catherine Billrd, et al. 2016.\n“Phonological skills, visual attention span,\nand visual stress in developmental dyslexia.” Dev.\nPsychol. 52 (10): 1503–16.\n\n\nSamuels, S Jay. 2007. “The DIBELS Tests: Is Speed of Barking at\nPrint What We Mean by Reading Fluency?”\n\n\nScarborough, Hollis S. 1998. “Predicting the Future Achievement of\nSecond Graders with Reading Disabilities: Contributions of Phonemic\nAwareness, Verbal Memory, Rapid Naming, and IQ.” Annals of\nDyslexia 48: 115–36.\n\n\nSchrank, F A, K S McGrew, N Mather, B J Wendling, and E M LaForte. 2014.\n“Woodcock-Johnson IV Tests of\nAchievement.” Riverside Publishing Company.\n\n\nSchrank, Fredrick A, Kevin S McGrew, Mary L Ruef, and Criselda G\nAlvarado. 2005. “Baterı́a III\nWoodcock-Muñoz.” Rolling Meadows: Riverside\nPublishing.\n\n\nSeidenberg, M S, and J L McClelland. 1989. “A\ndistributed, developmental model of word recognition and\nnaming.” Psychol. Rev. 96 (4): 523–68.\n\n\nSilverman, Rebecca D, Deborah L Speece, Jeffrey R Harring, and Kristen D\nRitchey. 2013. “Fluency Has a Role in the Simple View of\nReading.” Sci. Stud. Read. 17 (2): 108–33.\n\n\nSnowling, Maggie, and Charles Hulme. 2024. “Do We Really Need a\nNew Definition of Dyslexia? A Commentary.” Ann.\nDyslexia, March.\n\n\nSnowling, Margaret. 1998. “Dyslexia as a Phonological Deficit:\nEvidence and Implications.” Child Psychology and Psychiatry\nReview 3 (1): 4–11.\n\n\nSnowling, Margaret J, Charles Hulme, and Kate Nation. 2020.\n“Defining and Understanding Dyslexia: Past, Present and\nFuture.” Oxford Review of Education 46 (4): 501–13.\n\n\nSolano-Flores, Guillermo, Eduardo Backhoff, and Luis Ángel\nContreras-Niño. 2009. “Theory of Test\nTranslation Error.” International Journal of\nTesting 9 (2): 78–91. https://doi.org/10.1080/15305050902880835.\n\n\nSperling, George. 1960. “The Information Available in Brief Visual\nPresentations.” Psychological Monographs: General and\nApplied 74 (11): 1.\n\n\n———. 1983. “Why We Need Iconic Memory.” Behav. Brain\nSci. 6 (1): 37–39.\n\n\nStanovich, Keith E. 1998. “Refining the Phonological Core Deficit\nModel.” Child Psychology and Psychiatry Review 3 (1):\n17–21.\n\n\n———. 2017. “Speculations on the Causes and Consequences of\nIndividual Differences in Early Reading Acquisition.” In\nReading Acquisition, 1st Edition, 307–42. Routledge.\n\n\nSwanson, H Lee, Guy Trainin, Denise M Necoechea, and Donald D Hammill.\n2003. “Rapid Naming, Phonological Awareness, and Reading: A\nMeta-Analysis of the Correlation Evidence.” Review of\nEducational Research 73 (4): 407–40.\n\n\nTabachnick, BG, LS Fidell, BG Tabachnick, and LS Fidell. 2012.\n“Chapter 13 Principal Components and Factor Analysis.”\nUsing Multivariate Statistics 6: 612–80.\n\n\nTaylor, Ellie K, Gavkhar Abdurokhmonova, and Rachel R Romeo. 2023.\n“Socioeconomic Status and Reading Development: Moving from\n‘Deficit’ to ‘Adaptation’ in Neurobiological\nModels of Experience-Dependent Learning.” Mind\nBrain Educ. 17 (4): 324–33.\n\n\nTorgesen, Joseph K. 1998. “Catch Them Before They Fall.”\nAmerican Educator 22: 32–41.\n\n\n———. 2004. “Preventing Early Reading Failure.” American\nEducator 28 (3): 6–9.\n\n\nTorgesen, Joseph K, Richard Wagner, and Carl Rashotte. 2011. TOWRE 2: Test of word reading efficiency.\nPearson Clinical Assessment.\n\n\nTran, Jasmine E, Jason D Yeatman, Amy Burkhardt, Wanjing A Ma, Jamie\nMitchell, Maya Yablonski, Liesbeth Gijbels, Carrie Townley-Flores, and\nAdam Richie-Halford. 2023. “Development and Validation of a Rapid\nOnline Sentence Reading Efficiency Assessment.”\n\n\nTreiman, Rebecca, and Andrea Zukowski. 1991. “Levels of\nPhonological Awareness.” Phonological Processes in Literacy:\nA Tribute to Isabelle Y. Liberman.\n\n\nTunmer, William E, Michael L Herriman, and Andrew R Nesdale. 1988.\n“Metalinguistic Abilities and Beginning Reading.” Read.\nRes. Q. 23 (2): 134.\n\n\nValdois, Sylviane, Carole Peyrin, Delphine Lassus-Sangosse, Marie\nLallier, Jean-François Démonet, and Sonia Kandel. 2014. “Dyslexia\nin a French–Spanish Bilingual Girl: Behavioural and Neural\nModulations Following a Visual Attention Span Intervention.”\nCortex 53 (April): 120–45.\n\n\nValdois, Sylviane, Caroline Reilhac, Emilie Ginestet, and Marie Line\nBosse. 2020. “Varieties of Cognitive Profiles in Poor Readers:\nEvidence for a VAS-Impaired Subtype.” J. Learn.\nDisabil., September, 22219420961332.\n\n\nVan Bon, Wim H J, Lotje T M Hoevenaars, and Joyce J Jongeneelen. 2004.\n“Using Pencil‐and‐paper Lexical‐decision Tests to Assess Word\nDecoding Skill: Aspects of Validity and Reliability.” J. Res.\nRead. 27 (1): 58–68.\n\n\nVellutino, Frank R, Jack M Fletcher, Margaret J Snowling, and Donna M\nScanlon. 2004. “Specific Reading Disability (Dyslexia): What Have\nWe Learned in the Past Four Decades?” J. Child Psychol.\nPsychiatry 45 (1): 2–40.\n\n\nWagner, R K, J K Torgesen, and C A Rashotte. 1999. Comprehensive test of phonological processes\n(CTOPP). Austin, TX: Pro-Ed.\n\n\nWagner, R. K., Torgesen, J. K., Rashotte, C. A., & Pearson, N. A.\n2010. Test of Silent Reading Efficiency and Comprehension. Pro\nEd.\n\n\nWagner, Richard K. 2011. “Relations Among Oral Reading Fluency,\nSilent Reading Fluency, and Reading Comprehension: A Latent Variable\nStudy of First-Grade Readers.” Sci. Stud. Read. 15 (4):\n338–62.\n\n\nWagner, Richard K, and Joseph K Torgesen. 1987. “The Nature of\nPhonological Processing and Its Causal Role in the Acquisition of\nReading Skills.” Psychological Bulletin 101 (2): 192.\n\n\nWang, Zuowei, John Sabatini, Tenaha O’Reilly, and Jonathan Weeks. 2019.\n“Decoding and Reading Comprehension: A Test of the Decoding\nThreshold Hypothesis.” J. Educ. Psychol. 111 (3):\n387–401.\n\n\nWashington, J A, and M S Seidenberg. 2021. “Teaching Reading to\nAfrican American Children: When Home and School Language Differ.”\nAmerican Educator.\n\n\nWei, Li. 2008. “Research perspectives on\nbilingualism and multilingualism.” In, edited by Li Wei\nand Melissa G Moyer, 1–17. The Blackwell Guide to Research Methods in\nBilingualism and Multilingualism. Blackwell Publishing.\n\n\nWentworth, Laura, Paula Arce-Trigatti, Carrie Conaway, and Samantha\nShewchuk. 2023. Brokering in Education Research-Practice\nPartnerships: A Guide for Education Professionals and Researchers.\nTaylor & Francis.\n\n\nWentworth, L, R Khanna, M Nayfack, and D Schwartz. 2021. “Closing\nthe Research-Practice Gap in Education.” Stanford Social\nInnovation Review 19 (2): 57–58.\n\n\nWolf, Maryanne, and Patricia Greig Bowers. 1999. “The\nDouble-Deficit Hypothesis for the Developmental Dyslexias.”\nJournal of Educational Psychology 91 (3): 415.\n\n\nWolf, Maryanne, Alyssa Goldberg O’rourke, Calvin Gidney, Maureen Lovett,\nPaul Cirino, and Robin Morris. 2002. “The Second Deficit: An\nInvestigation of the Independence of Phonological and Naming-Speed\nDeficits in Developmental Dyslexia.” Reading and Writing\n15: 43–72.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction\nwith R, Second Edition. CRC Press.\n\n\nWood, Simon, and Maintainer Simon Wood. 2015. “Package\n‘Mgcv’.” R Package Version 1 (29): 729.\n\n\nWright, Benjamin D. 1994. “Reasonable Mean-Square Fit\nValues.” Rasch Meas Transac 8: 370.\n\n\nWu, Margaret, and Richard J Adams. 2013. “Properties of Rasch\nResidual Fit Statistics.” J. Appl. Meas. 14 (4): 339–55.\n\n\nYeatman, Jason D. 2022. “The Neurobiology of Literacy.”\nThe Science of Reading: A Handbook, 533–55.\n\n\nYeatman, Jason D, Kenny An Tang, Patrick M Donnelly, Maya Yablonski,\nMahalakshmi Ramamurthy, Iliana I Karipidis, Sendy Caffarra, et al. 2021.\n“Rapid Online Assessment of Reading Ability.” Sci.\nRep. 11 (1): 6396.\n\n\nYeatman, Jason D, and Alex L White. 2021. “Reading: The Confluence\nof Vision and Language.” Annual Review of Vision Science\n7 (1): 487–517.\n\n\nZhao, Jing, Hanlong Liu, Jiaxiao Li, Haixia Sun, Zhanhong Liu, Jing Gao,\nYuan Liu, and Chen Huang. 2019. “Improving Sentence Reading\nPerformance in Chinese Children with Developmental Dyslexia by Training\nBased on Visual Attention Span.” Sci. Rep. 9 (1): 18964.\n\n\nZoubrinetzky, Rachel, Gregory Collet, Marie-Ange Nguyen-Morel, Sylviane\nValdois, and Willy Serniclaes. 2019. “Remediation of Allophonic\nPerception and Visual Attention Span in Developmental Dyslexia: A Joint\nAssay.” Front. Psychol. 10 (July): 1502.\n\n\nZuk, Jennifer, Jade Dunstan, Elizabeth Norton, Xi Yu, Ola\nOzernov-Palchik, Yingying Wang, Tiffany P Hogan, John DE Gabrieli, and\nNadine Gaab. 2021. “Multifactorial Pathways Facilitate Resilience\nAmong Kindergarteners at Risk for Dyslexia: A Longitudinal Behavioral\nand Neuroimaging Study.” Developmental Science 24 (1):\ne12983.\n\n\nZutell, Jerry, and Virginia Allen. 1988. “The English Spelling\nStrategies of Spanish-Speaking Bilingual Children.” TESol\nQuarterly 22 (2): 333–40.",
    "crumbs": [
      "References"
    ]
  }
]